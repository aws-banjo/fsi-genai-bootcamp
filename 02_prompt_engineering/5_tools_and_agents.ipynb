{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Retrieving Data Automatically from APIs\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "Throughout this workshop so far, we have been working with unstructured text retrieval via semantic similarity search. However, another important type of retrieval which customers can take advantage of with Amazon Bedrock is **structured data retrieval** from APIs. Structured data retrieval is extremely useful for augmenting LLM applications with up to date information which can be retrieved in a repeatable manner, but the outputs are always changing. An example of a question you might ask an LLM which uses this type of retrieval might be \"How long will it take for my Amazon.com order containing socks to arrive?\". In this notebook, we will show how to integrate an LLM with a backend API service which has the ability to answer a user's question through RAG.\n",
    "\n",
    "Specifically, we will be building a tool which is able to tell you the weather based on natural language. This is a fairly trivial example, but it does a good job of showing how multiple API tools can be used by an LLM to retrieve dynamic data to augment a prompt. Here is a visual of the architecture we will be building today.\n",
    "\n",
    "![api](./images/api.png)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cced6",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup `boto3` Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85063bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "import xmltodict\n",
    "from rich import print as rprint\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock\n",
    "from utils.prompt_utils import extract_docstring_info, construct_format_tool_for_claude_prompt\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f3e58",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the API Tools\n",
    "\n",
    "The first thing we need to do for our LLM is define the tools it has access to. In this case we will be defining local Python functions, but it is important to note that these could be any type of application service. Examples of what these tools might be on AWS include...\n",
    "\n",
    "* An AWS Lambda function\n",
    "* An Amazon RDS database connection\n",
    "* An Amazon DynamnoDB table\n",
    "  \n",
    "More generic examples include...\n",
    "\n",
    "* REST APIs\n",
    "* Data warehouses, data lakes, and databases\n",
    "* Computation engines\n",
    "\n",
    "In this case, we define two tools which reach external APIs below with two python functions\n",
    "1. the ability to retrieve the latitude and longitude of a place given a natural language input\n",
    "2. the ability to retrieve the weather given an input latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb0dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_weather(latitude: str, longitude: str):\n",
    "    \"\"\"\n",
    "    Fetches and returns the current weather for a given latitude and longitude from the Open-Meteo API.\n",
    "\n",
    "    Args:\n",
    "        latitude (str): The latitude of the location to fetch the weather for.\n",
    "        longitude (str): The longitude of the location to fetch the weather for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the current weather data for the given location.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "def get_lat_long(place: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fetches and returns the latitude and longitude of a given place from the Nominatim OpenStreetMap API.\n",
    "\n",
    "    Args:\n",
    "        place (str): The name of the place to fetch the latitude and longitude for.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the latitude and longitude of the given place, or None if the place was not found.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {'q': place, 'format': 'json', 'limit': 1}\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, params=params, headers=headers).json()\n",
    "    if response:\n",
    "        lat = response[0][\"lat\"]\n",
    "        lon = response[0][\"lon\"]\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def call_function(tool_name, parameters):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calls a function with the given name and parameters.\n",
    "\n",
    "    This function uses the `globals()` function to find a function with the given name in the global scope, and then calls it with the given parameters.\n",
    "\n",
    "    Args:\n",
    "        tool_name (str): The name of the function to call.\n",
    "        parameters (dict): A dictionary of parameters to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        The output of the called function.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    func = globals()[tool_name]\n",
    "    output = func(**parameters)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2d1c0",
   "metadata": {},
   "source": [
    "We also define a function called `call_function` which is used to abstract the tool name. You can see an example of determining the weather in Las Vegas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92319d0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "place = 'Las Vegas'\n",
    "lat_long_response = call_function('get_lat_long', {'place' : place})\n",
    "weather_response = call_function('get_weather', lat_long_response)\n",
    "print(f'Weather in {place} is...')\n",
    "weather_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18203223",
   "metadata": {},
   "source": [
    "As you might expect, we have to describe our tools to our LLM, so it knows how to use them. The strings below describe the python functions for lat/long and weather to Claude in an XML friendly format which we have seen previously in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e95846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import BedrockChat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_parameter = {\"temperature\": 0.0, \"max_tokens\": 2000}\n",
    "llm = BedrockChat(model_id=model_id, client=boto3_bedrock, model_kwargs=model_parameter, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd637c5",
   "metadata": {},
   "source": [
    "---\n",
    "## Define Prompts to Orchestrate our LLM Using Tools\n",
    "\n",
    "Now that the tools are defined both programmatically and as a string, we can start orchestrating the flow which will answer user questions. The first step to this is creating a prompt which defines the rules of operation for Claude. In the prompt below, we provide explicit direction on how Claude should use tools to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0021532",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_calling_prompt = \"\"\"\n",
    "\n",
    "Your job is to formulate a solution to a given <user-request> based on the instructions and tools below.\n",
    "\n",
    "Use these Instructions: \n",
    "1. In this environment you have access to a set of tools and functions you can use to answer the question.\n",
    "2. You can call the functions by using the <function_calls> format below.\n",
    "3. Only invoke one function at a time and wait for the results before invoking another function.\n",
    "4. The results of the function will be in xml tag <function_results>. \n",
    "5. Only use the information in the <function_results> to answer the question.\n",
    "6. Only make function calls if the answer is not already in the <function_results>.\n",
    "7. If you need to use the output of a function as an input to another function, you can do so.\n",
    "8. Put any intermediate reasoning steps into the <thoughts></thoughts> tags.\n",
    "9. Once you truly know the answer to the <user-request>, place the answer in <answer></answer> tags. Make sure to answer in a full sentence which is friendly.\n",
    "\n",
    "Here are the tools you can use:\n",
    "{tools_prompt}\n",
    "\n",
    "You may call them like this:\n",
    "<function_calls>\n",
    "<invoke>\n",
    "<tool_name>$TOOL_NAME</tool_name>\n",
    "<parameters>\n",
    "<$PARAMETER_NAME>$PARAMETER_VALUE</$PARAMETER_NAME>\n",
    "...\n",
    "</parameters>\n",
    "</invoke>\n",
    "</function_calls>\n",
    "\n",
    "Results from prior function calls are shown here. Do not repeat the same function call if you already have the results here.\n",
    "<function_results>\n",
    "{function_results}\n",
    "</function_results>\n",
    "\n",
    "<user-request>\n",
    "{user_request}\n",
    "</user-request>\n",
    "\"\"\"\n",
    "\n",
    "tools = [get_lat_long, get_weather]\n",
    "tool_prompt = \"\"\n",
    "for tool in tools:\n",
    "    docstring_info = extract_docstring_info(tool.__doc__)\n",
    "    tool_prompt += construct_format_tool_for_claude_prompt(tool.__name__, docstring_info['description'], docstring_info['params'])\n",
    "\n",
    "\n",
    "function_calling_prompt = ChatPromptTemplate.from_template(function_calling_prompt, partial_variables={\"tools_prompt\": tool_prompt})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331b2f1",
   "metadata": {},
   "source": [
    "When the model determines that it needs to invoke a tool, it will generate an xml output that specifies the tool to use and the input parameters to the tool. Since we can't use xml strings to invoke our tools, we convert the xml output to a dictionary and then use the `call_function` function to invoke the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5bd5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_dict(model_output: str):\n",
    "\n",
    "    \n",
    "    func_call_idx = model_output.find(\"<function_calls>\")\n",
    "    \n",
    "    invoke_xml = model_output[func_call_idx + len(\"<function_calls>\"):]\n",
    "    \n",
    "    invoke_dict = xmltodict.parse(invoke_xml)\n",
    "\n",
    "    return {\n",
    "        \"tool_name\": invoke_dict[\"invoke\"][\"tool_name\"],\n",
    "        \"parameters\": invoke_dict[\"invoke\"][\"parameters\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd51cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_chain = (\n",
    "    function_calling_prompt\n",
    "    | llm.bind(stop=[\"</function_calls>\", \"</answer>\"])\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd2f10",
   "metadata": {},
   "source": [
    "---\n",
    "## Executing the RAG Workflow\n",
    "\n",
    "Armed with our prompt and structured tools, we can now write an orchestration function which will iteratively step through the logical tasks to answer a user question. In the cell below we use the `invoke_model` function to generate a response with Claude and the `single_retriever_step` function to iteratively call tools when the LLM tells us we need to. The general flow works like this...\n",
    "\n",
    "1. The user enters an input to the application\n",
    "2. The user input is merged with the original prompt and sent to the LLM to determine the next step\n",
    "3. If the LLM knows the answer, it will answer and we are done. If not, go to next step 4.\n",
    "4. The LLM will determine which tool to use to answer the question.\n",
    "5. We will use the tool as directed by the LLM and retrieve the results.\n",
    "6. We provide the results back into the original prompt as more context.\n",
    "7. We ask the LLM the next step or if knows the answer.\n",
    "8. Return to step 3.\n",
    "\n",
    "We can orchestrate the entire workflow through a simple while loop where we continuously invoke the LLM until it has enough information to provide the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db941a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the weather in Las Vegas?\"\n",
    "# query = \"Is it currently warmer in Las Vegas or New York?\" # try to see what happens when you ask a more complex question\n",
    "\n",
    "intermediate_function_results = \"\"\n",
    "max_iter = 10\n",
    "\n",
    "while True:\n",
    "    output = func_chain.invoke(\n",
    "        {\n",
    "            \"user_request\": query,\n",
    "            \"function_results\": intermediate_function_results,\n",
    "        }\n",
    "    )\n",
    "    rprint(f\"[green]{output}[/green]\")\n",
    "\n",
    "    if (\"<answer>\" in output) | (max_iter == 0):\n",
    "        answer_idx = output.find(\"<answer>\")\n",
    "        output = output[answer_idx:].replace(\"<answer>\", \"\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "\n",
    "        calling_params = xml_to_dict(output)\n",
    "\n",
    "        func_call_results = call_function(**calling_params)\n",
    "        rprint(f'[bold cyan]Calling function {calling_params[\"tool_name\"]} with parameters {calling_params[\"parameters\"]} [/bold cyan]')\n",
    "        rprint(f'[bold cyan]Function call results: {func_call_results} [/bold cyan]')\n",
    "\n",
    "        parameters_xml = \"\\n\".join(\n",
    "            [f\"<{k}>{v}</{k}>\" for k, v in calling_params[\"parameters\"].items()]\n",
    "        )\n",
    "\n",
    "        intermediate_results = (\n",
    "            \"<result>\\n<tool_name>\"\n",
    "            + calling_params[\"tool_name\"]\n",
    "            + \"</tool_name>\\n\"\n",
    "            + \"<parameters>\\n\"\n",
    "            + parameters_xml\n",
    "            + \"\\n</parameters>\\n<output>\"\n",
    "            + json.dumps(func_call_results)\n",
    "            + \"\\n</output>\\n</result>\\n\"\n",
    "        )\n",
    "\n",
    "        intermediate_function_results += intermediate_results\n",
    "        max_iter -= 1\n",
    "\n",
    "rprint(f\"[#f77f00]==========FINAL OUTPUT==========\\n{output}\\n==============================[/#f77f00]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b217d",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now that you have used a few different retrieval systems, lets move on to the next notebook where you can apply the skills you've learned so far!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
