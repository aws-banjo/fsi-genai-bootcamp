{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Mistral 7b with Amazon SageMaker\n",
    "In this notebook we'll explore how to fine-tune a [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) model with Amazon SageMaker. We'll use the [Hugging Face](https://huggingface.co/) library to download the model and tokenizer, and we'll use the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/) to fine-tune the model on a sample dataset. The goal of this notebook is to cover several key aspects of fine-tuning LLMs including:\n",
    "- Preparing the data for fine-tuning\n",
    "- Obtaining the base model and tokenizer\n",
    "- Configuring a SageMaker training job\n",
    "- Utilizing QLoRA for parameter efficient fine-tuning (PEFT)\n",
    "- Applying supervised fine-tuning methods to train a model\n",
    "- Improving / Aligning the model's outputs with human preferences using Direct Preference Optimization (DPO)\n",
    "\n",
    "We will utilize the [fine-tuning recipes](https://github.com/huggingface/alignment-handbook) provided by Hugging Face that was used to fine-tune the Mistral-7B model to create the [Zephyr-7B-Beta](HuggingFaceH4/zephyr-7b-beta) model.\n",
    "\n",
    "The recipes utilize the [Transformer Reinforcement Learning (TRL)](https://github.com/huggingface/trl) for both supervised fine-tuning and preference alignment and is easy to adapt to other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker\n",
    "%pip install -Uq datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import time\n",
    "from pathlib import Path\n",
    "from utils import download_model\n",
    "\n",
    "boto3_session=boto3.session.Session()\n",
    "\n",
    "smr = boto3_session.client(\"sagemaker-runtime\") # sagemaker runtime client for invoking the endpoint\n",
    "sm = boto3_session.client(\"sagemaker\") \n",
    "s3_rsr = boto3_session.resource(\"s3\")\n",
    "role = sagemaker.get_execution_role()  \n",
    "\n",
    "sess = sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model\n",
    "First, we'll download the model and tokenizer from the Hugging Face model hub and upload them to our own S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = \"Mistral-7B\"\n",
    "if not Path(local_model_path).exists():\n",
    "    !aws s3 cp --recursive s3://jumpstart-cache-prod-{region}/huggingface-llm/huggingface-llm-mistral-7b/artifacts/inference/v1.0.0/ {local_model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists on the S3 bucket\n",
      "If you want to upload a new model, please delete the existing model from the S3 bucket with the following command: \n",
      " !aws s3 rm --recursive s3://sagemaker-us-east-1-152804913371/Mistral-7B\n"
     ]
    }
   ],
   "source": [
    "# check if the model has already been uploaded to the S3 bucket. If not, upload it.\n",
    "model_prefix = local_model_path\n",
    "\n",
    "if list(s3_rsr.Bucket(bucket).objects.filter(Prefix=model_prefix)) :\n",
    "    print(\"Model already exists on the S3 bucket\")\n",
    "    print(f\"If you want to upload a new model, please delete the existing model from the S3 bucket with the following command: \\n !aws s3 rm --recursive s3://{bucket}/{model_prefix}\")\n",
    "    s3_model_location = f\"s3://{bucket}/{model_prefix}\"\n",
    "else:\n",
    "    s3_model_location = sess.upload_data(path=local_model_path.as_posix(), bucket=bucket, key_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data and upload to S3\n",
    "Next we need to prepare the data for fine-tuning. We can use a sample of the data that was used to train the Zephy-7B-Beta model or we can bring our own data. \n",
    "\n",
    "If bringing our own data, we need to convert it into a json-lines format that is supported by the TRL trainers. \n",
    "- For Supervised Fine-tuning each record should contain a `messages` field. This field should contain a list of dictionaries that correspond to a conversation between a `user` and an AI `assistant`. The schema is `{\"role\": \"{role}\", \"content\": {content}}` where role is either `user`, `assistant`, or `system` and content is the text of the message. For more information see the recipe documentation [here](https://github.com/huggingface/alignment-handbook/tree/main/scripts) or an example dataset [here](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k) \n",
    "- For Direct Preference Optimization we need to provide a dataset that contains `chosen` and `rejected` responses as based on human preference. The schema for this dataset contains `chosen` and `rejected` fields that contain the conversation messages in the same format as the supervised fine-tuning dataset. For more information see the recipe documentation [here](https://github.com/huggingface/alignment-handbook/tree/main/scripts) or an example dataset [here](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n",
    "\n",
    "The tuning recipe will automatically convert the `messages` into a chat prompt that will be used to fine-tune the model. You can see what the default template looks like by visiting the [Zephyr-7B-Beta](HuggingFaceH4/zephyr-7b-beta) model card. The messages will be converted into a chat prompt that separates the system, user, and assistant messages with the following tokens: `<|system|>`, `<|user|>`, and `<|assistant|>` respectively along with an EOS token `</s>` at the end of each message. The template can be adjusted in the tuning script however it is important to keep in mind that the same template should be used during inference and should be well documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1080/1080 [00:00<00:00, 5341.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 120/120 [00:00<00:00, 1213.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1080/1080 [00:00<00:00, 6764.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 120/120 [00:00<00:00, 1331.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "USE_EXAMPLE_DATA = True # set to False to use your own data\n",
    "NUM_SAMPLES = 1200 # number of samples to use from the example data\n",
    "\n",
    "if USE_EXAMPLE_DATA:\n",
    "    sft_dataset = datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\")['train_sft'].select(range(NUM_SAMPLES))\n",
    "    dpo_dataset = datasets.load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")['train_prefs'].select(range(NUM_SAMPLES))\n",
    "    \n",
    "# adjust these values if bringing your own data\n",
    "# In the example here, a jsonl file is stored in /data/dpo and /data/sft that contains data in the format described above\n",
    "else:\n",
    "    dpo_dataset_path =\"./data/dpo\"\n",
    "    sft_dataset_path =\"./data/sft\"\n",
    "    try:\n",
    "        sft_dataset = datasets.load_dataset(sft_dataset_path)[\"train\"]\n",
    "        dpo_dataset = datasets.load_dataset(dpo_dataset_path)[\"train\"]\n",
    "    except Exception as e:\n",
    "        print(\"Please make sure that the data is present in the data folder. If not, please prepare the data first\")\n",
    "        raise Exception(e)\n",
    "\n",
    "sft_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).save_to_disk('fine-tuning-data/sft_split')\n",
    "dpo_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).save_to_disk('fine-tuning-data/dpo_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data file to s3://sagemaker-us-east-1-152804913371/fine-tuning-mistral/data\n"
     ]
    }
   ],
   "source": [
    "# upload the data to S3\n",
    "s3_data = sess.upload_data(path=\"fine-tuning-data\", bucket=bucket, key_prefix=\"fine-tuning-mistral/data\")\n",
    "\n",
    "print(f\"Uploaded training data file to {s3_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure SageMaker Training Job for Supervised Fine-tuning\n",
    "Now that the data is ready, we can configure the first SageMaker training job which will perform supervised fine-tuning. The code from the Hugging Face recipe [repo](https://github.com/huggingface/alignment-handbook/tree/main) is cloned into the `src` directory. The `src` directory also contains a requirements.txt file that will install the recipe module and [Flash Attention](https://github.com/Dao-AILab/flash-attention) to speed up the training.\n",
    "\n",
    "The repo contains two scripts, `alignment-handbook/scripts/run_sft.py` for supervised fine-tuning and `alignment-handbook/scripts/run_dpo.py` for direct preference optimization. Both scripts take a positional argument for the path of the recipe file like this `python run_{task}.py config_full.yaml`. The recipe file contains all of the hyperparameters for the training job. The recipe file for the supervised fine-tuning job is located at `src/config_sft_lora.yaml`. Several example recipe files are available within the repo for full and parameter efficient fine-tuning. We will utilize the parameter efficient fine-tuning recipe for this example.\n",
    "\n",
    "A few changes are required to the recipe file to run the job on SageMaker. First, we need to change the `model_name_or_path` to `/opt/ml/input/data/model`. This is the directory to which our base model will be copied to from S3. Next, we need to change the `dataset_mixer` directories to `/opt/ml/input/data/train` which is where our training data will be copied to from S3. Finally, we need to change the `output_dir` for the `trainer` to `/opt/ml/model` so that the model is saved to the `/opt/ml/model` directory which is the default directory for SageMaker models. The contents of the `/opt/ml/model` will be copied to S3 once the job finishes.  Optionally, we can set the `logging_dir` to `/opt/ml/output/tensorboard` to utilize [SageMaker Managed TensorBoard](https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html) for monitoring the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "import time\n",
    "\n",
    "str_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "tb_output_config = TensorBoardOutputConfig(s3_output_path=f\"s3://{bucket}/fine-tuning-mistral/tensorboard/{str_time}\",\n",
    "    container_local_output_path=\"/opt/ml/output/tensorboard\")\n",
    "\n",
    "job_name = f\"mistral7b-sft\"\n",
    "\n",
    "# the default script takes the yaml file as a positional argument\n",
    "# Since sagemaker only supports passing named arguments as hyperparameters, as small change was made to the fine tuning scripts\n",
    "\n",
    "hyperparameters = {\n",
    "    \"recipe\": \"config_sft_lora.yaml\",  # supervised fine-tuning with QLoRA recipe\n",
    "}\n",
    "\n",
    "sft_estimator = PyTorch(\n",
    "    base_job_name=job_name,\n",
    "    source_dir = \"src\",                                  # directory containing the fine-tuning scripts\n",
    "    entry_point=\"alignment-handbook/scripts/run_sft.py\", # fine-tuning script that will be run\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    instance_count=2,                                    # number of instances to use for training \n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g5.2xlarge\", \n",
    "    framework_version=\"2.1.0\",                          # PyTorch version\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    max_run=60*60*24*2,\n",
    "    keep_alive_period_in_seconds=3600,                    # after job is done keep the training cluster alive for 1 hour to accept other jobs\n",
    "    tensorboard_output_config=tb_output_config,\n",
    "    environment = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \n",
    "                    \"LIBRARY_PATH\": \"/opt/conda/lib/\",\n",
    "                    \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                    \"NCCL_P2P_LEVEL\": \"NVL\"},\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}}, # enable distributed training with torch.distributed \n",
    "    disable_output_compression = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: mistral7b-sft-2024-05-30-18-35-20-560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 18:35:21 Starting - Starting the training job...\n",
      "2024-05-30 18:35:25 Downloading - Downloading input data....................................\n",
      "2024-05-30 18:41:52 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-05-30 18:41:53,141 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-05-30 18:41:53,159 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-30 18:41:53,171 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-05-30 18:41:53,173 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2024-05-30 18:41:53,173 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-05-30 18:41:54,770 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Obtaining file:///opt/ml/code/alignment-handbook (from -r requirements.txt (line 2))\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting flash-attn==2.3.6 (from -r requirements.txt (line 1))\n",
      "Downloading flash_attn-2.3.6.tar.gz (2.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 73.0 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-05-30 18:41:53,021 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-05-30 18:41:53,039 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-30 18:41:53,050 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-05-30 18:41:53,054 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2024-05-30 18:41:53,054 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-05-30 18:41:54,705 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Obtaining file:///opt/ml/code/alignment-handbook (from -r requirements.txt (line 2))\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting flash-attn==2.3.6 (from -r requirements.txt (line 1))\n",
      "Downloading flash_attn-2.3.6.tar.gz (2.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 46.4 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (1.11.1.1)\n",
      "Collecting accelerate>=0.29.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes>=0.43.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting evaluate==0.4.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting datasets>=2.18.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deepspeed==0.12.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading deepspeed-0.12.2.tar.gz (1.2 MB)\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (1.11.1.1)\n",
      "Collecting accelerate>=0.29.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes>=0.43.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting evaluate==0.4.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting datasets>=2.18.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deepspeed==0.12.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading deepspeed-0.12.2.tar.gz (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 68.7 MB/s eta 0:00:00\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 78.4 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting hf_transfer>=0.1.4 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting peft>=0.9.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf<=3.20.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting hf_transfer>=0.1.4 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting peft>=0.9.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf<=3.20.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting safetensors>=0.3.3 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece>=0.1.99 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.12.0)\n",
      "Collecting tensorboard (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (4.66.4)\n",
      "Collecting transformers>=4.39.3 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting trl>=0.8.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hjson (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (5.9.8)\n",
      "Collecting py-cpuinfo (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.6.3)\n",
      "Collecting pynvml (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.32.2)\n",
      "Collecting xxhash (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Collecting responses<0.19 (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.29.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting safetensors>=0.3.3 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece>=0.1.99 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.12.0)\n",
      "Collecting tensorboard (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (4.66.4)\n",
      "Collecting transformers>=4.39.3 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting trl>=0.8.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hjson (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (5.9.8)\n",
      "Collecting py-cpuinfo (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.6.3)\n",
      "Collecting pynvml (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.32.2)\n",
      "Collecting xxhash (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Collecting responses<0.19 (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.29.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting aiohttp (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (3.2.1)\n",
      "Collecting aiohttp (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (3.2.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 6.3 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (68.1.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.0.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (68.1.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.0.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.16.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.1.2)\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 14.1 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 38.3 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.16.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.1.2)\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 13.2 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 38.0 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 17.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 52.5 MB/s eta 0:00:00\n",
      "Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 93.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 44.5 MB/s eta 0:00:00\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 37.5 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 70.9 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 72.5 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 72.5 MB/s eta 0:00:00\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 99.6 MB/s eta 0:00:00\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 35.1 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 101.6 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 23.9 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 82.4 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 101.7 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 22.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 68.0 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 95.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 89.6 MB/s eta 0:00:00\n",
      "Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.4/102.4 kB 18.9 MB/s eta 0:00:00\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 26.2 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 29.0 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 20.5 MB/s eta 0:00:00\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 37.3 MB/s eta 0:00:00\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 17.2 MB/s eta 0:00:00\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 52.2 MB/s eta 0:00:00\n",
      "Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 67.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 30.8 MB/s eta 0:00:00\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 31.9 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 48.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 49.1 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 55.9 MB/s eta 0:00:00\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 89.0 MB/s eta 0:00:00\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 28.1 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 69.8 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 21.7 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 64.3 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 67.3 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 17.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: flash-attn, deepspeed\n",
      "Building wheel for flash-attn (setup.py): started\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 49.2 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 61.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 48.6 MB/s eta 0:00:00\n",
      "Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.4/102.4 kB 13.2 MB/s eta 0:00:00\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 8.7 MB/s eta 0:00:00\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 7.4 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 21.6 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 25.0 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 15.5 MB/s eta 0:00:00\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 32.0 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: flash-attn, deepspeed\n",
      "Building wheel for flash-attn (setup.py): started\n",
      "Building wheel for flash-attn (setup.py): finished with status 'done'\n",
      "Created wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56477261 sha256=652ad256d0891cb2c6d7183f96f7f56ff61cdeee24388381abb35e7a0f2eeca1\n",
      "Stored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\n",
      "Building wheel for deepspeed (setup.py): started\n",
      "Building wheel for flash-attn (setup.py): finished with status 'done'\n",
      "Created wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56477261 sha256=652ad256d0891cb2c6d7183f96f7f56ff61cdeee24388381abb35e7a0f2eeca1\n",
      "Stored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\n",
      "Building wheel for deepspeed (setup.py): started\n",
      "Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "Created wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265667 sha256=34d526a6f9bbed788289e9c1ebfdf3437b73be9334a6321dba245c8983a58e59\n",
      "Stored in directory: /root/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\n",
      "Successfully built flash-attn deepspeed\n",
      "Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "Created wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265667 sha256=66eab1112ca5d47d48ce0d9568771bd9bc6dd634433cf07a52de5332969338cc\n",
      "Stored in directory: /root/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\n",
      "Successfully built flash-attn deepspeed\n",
      "Installing collected packages: sentencepiece, py-cpuinfo, hjson, xxhash, tensorboard-data-server, shtab, safetensors, regex, pynvml, pyarrow-hotfix, protobuf, multidict, markdown, hf_transfer, grpcio, frozenlist, docstring-parser, async-timeout, absl-py, yarl, tensorboard, responses, huggingface-hub, aiosignal, tyro, tokenizers, flash-attn, deepspeed, bitsandbytes, aiohttp, accelerate, transformers, peft, datasets, trl, evaluate, alignment-handbook\n",
      "Attempting uninstall: protobuf\n",
      "Found existing installation: protobuf 3.20.3\n",
      "Uninstalling protobuf-3.20.3:\n",
      "Successfully uninstalled protobuf-3.20.3\n",
      "Attempting uninstall: flash-attn\n",
      "Found existing installation: flash-attn 2.0.4\n",
      "Uninstalling flash-attn-2.0.4:\n",
      "Successfully uninstalled flash-attn-2.0.4\n",
      "Installing collected packages: sentencepiece, py-cpuinfo, hjson, xxhash, tensorboard-data-server, shtab, safetensors, regex, pynvml, pyarrow-hotfix, protobuf, multidict, markdown, hf_transfer, grpcio, frozenlist, docstring-parser, async-timeout, absl-py, yarl, tensorboard, responses, huggingface-hub, aiosignal, tyro, tokenizers, flash-attn, deepspeed, bitsandbytes, aiohttp, accelerate, transformers, peft, datasets, trl, evaluate, alignment-handbook\n",
      "Attempting uninstall: protobuf\n",
      "Found existing installation: protobuf 3.20.3\n",
      "Uninstalling protobuf-3.20.3:\n",
      "Successfully uninstalled protobuf-3.20.3\n",
      "Attempting uninstall: flash-attn\n",
      "Found existing installation: flash-attn 2.0.4\n",
      "Uninstalling flash-attn-2.0.4:\n",
      "Successfully uninstalled flash-attn-2.0.4\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.22.0\n",
      "Uninstalling accelerate-0.22.0:\n",
      "Successfully uninstalled accelerate-0.22.0\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.22.0\n",
      "Uninstalling accelerate-0.22.0:\n",
      "Successfully uninstalled accelerate-0.22.0\n",
      "Running setup.py develop for alignment-handbook\n",
      "Running setup.py develop for alignment-handbook\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.3.6 which is incompatible.\n",
      "Successfully installed absl-py-2.1.0 accelerate-0.30.1 aiohttp-3.9.5 aiosignal-1.3.1 alignment-handbook-0.4.0.dev0 async-timeout-4.0.3 bitsandbytes-0.43.1 datasets-2.19.1 deepspeed-0.12.2 docstring-parser-0.16 evaluate-0.4.0 flash-attn-2.3.6 frozenlist-1.4.1 grpcio-1.64.0 hf_transfer-0.1.6 hjson-3.1.0 huggingface-hub-0.23.2 markdown-3.6 multidict-6.0.5 peft-0.11.1 protobuf-3.20.2 py-cpuinfo-9.0.0 pyarrow-hotfix-0.6 pynvml-11.5.0 regex-2024.5.15 responses-0.18.0 safetensors-0.4.3 sentencepiece-0.2.0 shtab-1.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.19.1 transformers-4.41.2 trl-0.8.6 tyro-0.8.4 xxhash-3.4.1 yarl-1.9.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "2024-05-30 18:42:32,852 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-30 18:42:32,852 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-30 18:42:32,889 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.3.6 which is incompatible.\n",
      "Successfully installed absl-py-2.1.0 accelerate-0.30.1 aiohttp-3.9.5 aiosignal-1.3.1 alignment-handbook-0.4.0.dev0 async-timeout-4.0.3 bitsandbytes-0.43.1 datasets-2.19.1 deepspeed-0.12.2 docstring-parser-0.16 evaluate-0.4.0 flash-attn-2.3.6 frozenlist-1.4.1 grpcio-1.64.0 hf_transfer-0.1.6 hjson-3.1.0 huggingface-hub-0.23.2 markdown-3.6 multidict-6.0.5 peft-0.11.1 protobuf-3.20.2 py-cpuinfo-9.0.0 pyarrow-hotfix-0.6 pynvml-11.5.0 regex-2024.5.15 responses-0.18.0 safetensors-0.4.3 sentencepiece-0.2.0 shtab-1.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.19.1 transformers-4.41.2 trl-0.8.6 tyro-0.8.4 xxhash-3.4.1 yarl-1.9.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "2024-05-30 18:42:32,919 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-30 18:42:32,932 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2024-05-30 18:42:32,950 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-30 18:42:32,963 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"recipe\": \"config_sft_lora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"mistral7b-sft-2024-05-30-18-35-20-560\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-152804913371/mistral7b-sft-2024-05-30-18-35-20-560/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"alignment-handbook/scripts/run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"alignment-handbook/scripts/run_sft.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"recipe\":\"config_sft_lora.yaml\"}\n",
      "SM_USER_ENTRY_POINT=alignment-handbook/scripts/run_sft.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"model\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=alignment-handbook/scripts/run_sft\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-152804913371/mistral7b-sft-2024-05-30-18-35-20-560/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"recipe\":\"config_sft_lora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"mistral7b-sft-2024-05-30-18-35-20-560\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-152804913371/mistral7b-sft-2024-05-30-18-35-20-560/source/sourcedir.tar.gz\",\"module_name\":\"alignment-handbook/scripts/run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"alignment-handbook/scripts/run_sft.py\"}\n",
      "SM_USER_ARGS=[\"--recipe\",\"config_sft_lora.yaml\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_MODEL=/opt/ml/input/data/model\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_RECIPE=config_sft_lora.yaml\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-1 --master_port 7777 --node_rank 0 alignment-handbook/scripts/run_sft.py --recipe config_sft_lora.yaml\n",
      "2024-05-30 18:42:33,285 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-30 18:42:33,285 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-30 18:42:33,320 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-30 18:42:33,349 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-30 18:42:33,362 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2024-05-30 18:42:33,381 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-30 18:42:33,393 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"recipe\": \"config_sft_lora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"mistral7b-sft-2024-05-30-18-35-20-560\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-152804913371/mistral7b-sft-2024-05-30-18-35-20-560/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"alignment-handbook/scripts/run_sft\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"alignment-handbook/scripts/run_sft.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"recipe\":\"config_sft_lora.yaml\"}\n",
      "SM_USER_ENTRY_POINT=alignment-handbook/scripts/run_sft.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"model\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-2\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=alignment-handbook/scripts/run_sft\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-152804913371/mistral7b-sft-2024-05-30-18-35-20-560/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"recipe\":\"config_sft_lora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"mistral7b-sft-2024-05-30-18-35-20-560\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-152804913371/mistral7b-sft-2024-05-30-18-35-20-560/source/sourcedir.tar.gz\",\"module_name\":\"alignment-handbook/scripts/run_sft\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"alignment-handbook/scripts/run_sft.py\"}\n",
      "SM_USER_ARGS=[\"--recipe\",\"config_sft_lora.yaml\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_MODEL=/opt/ml/input/data/model\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_RECIPE=config_sft_lora.yaml\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-1 --master_port 7777 --node_rank 1 alignment-handbook/scripts/run_sft.py --recipe config_sft_lora.yaml\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "Downloading readme:   0%|          | 0.00/6.52k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 6.52k/6.52k [00:00<00:00, 47.0MB/s]\n",
      "Downloading readme:   0%|          | 0.00/6.52k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 6.52k/6.52k [00:00<00:00, 44.1MB/s]\n",
      "Downloading data:   0%|          | 0.00/83.9k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 83.9k/83.9k [00:00<00:00, 1.82MB/s]\n",
      "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]\n",
      "Generating test split: 100%|██████████| 164/164 [00:00<00:00, 32735.25 examples/s]\n",
      "Downloading data:   0%|          | 0.00/83.9k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 83.9k/83.9k [00:00<00:00, 1.62MB/s]\n",
      "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]\n",
      "Generating test split: 100%|██████████| 164/164 [00:00<00:00, 31888.45 examples/s]\n",
      "[2024-05-30 18:42:46,662] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "2024-05-30 18:42:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n",
      "2024-05-30 18:42:47 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='/opt/ml/input/data/model', model_revision='main', model_code_revision=None, torch_dtype='auto', tokenizer_name_or_path=None, trust_remote_code=False, use_flash_attention_2=True, use_peft=True, lora_r=64, lora_alpha=16, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False, bnb_4bit_quant_storage='uint8')\n",
      "2024-05-30 18:42:47 - INFO - __main__ - Data parameters DataArguments(chat_template=\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", dataset_mixer={'/opt/ml/input/data/train': 1.0}, text_column='text', dataset_splits=['train', 'test'], dataset_configs=None, preprocessing_num_workers=2, truncation_side=None, auto_insert_empty_system_msg=True)\n",
      "2024-05-30 18:42:47 - INFO - __main__ - Training/evaluation parameters SFTConfig(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset_kwargs=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=zephyr-7b-sft-lora,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/opt/ml/output/tensorboard,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_seq_length=2048,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_bnb_8bit,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/opt/ml/model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/opt/ml/model,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Caching indices mapping at /opt/ml/input/data/train/train/cache-8aceb323340ea995.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Caching indices mapping at /opt/ml/input/data/train/train/cache-8aceb323340ea995.arrow\n",
      "Caching indices mapping at /opt/ml/input/data/train/test/cache-366cca8fb0c19703.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Caching indices mapping at /opt/ml/input/data/train/test/cache-366cca8fb0c19703.arrow\n",
      "2024-05-30 18:42:47 - INFO - __main__ - Training on the following datasets and their proportions: ['train : 1080', 'test : 120']\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,072 >> loading file tokenizer_config.json\n",
      "2024-05-30 18:42:47 - INFO - __main__ - *** Load pretrained model ***\n",
      "Process #0 will write at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00000_of_00002.arrow\n",
      "Process #1 will write at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00001_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Process #0 will write at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00000_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Process #1 will write at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00001_of_00002.arrow\n",
      "Spawning 2 processes\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Spawning 2 processes\n",
      "Applying chat template (num_proc=2):   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Caching processed dataset at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00000_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00000_of_00002.arrow\n",
      "Caching processed dataset at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00001_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00001_of_00002.arrow\n",
      "Applying chat template (num_proc=2):   8%|▊         | 90/1080 [00:00<00:02, 493.43 examples/s]\n",
      "Applying chat template (num_proc=2):  74%|███████▍  | 797/1080 [00:00<00:00, 2937.11 examples/s]\n",
      "Applying chat template (num_proc=2): 100%|██████████| 1080/1080 [00:00<00:00, 2463.23 examples/s]\n",
      "Concatenating 2 shards\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n",
      "Process #0 will write at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00000_of_00002.arrow\n",
      "Process #1 will write at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00001_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Process #0 will write at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00000_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Process #1 will write at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00001_of_00002.arrow\n",
      "Spawning 2 processes\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Spawning 2 processes\n",
      "Applying chat template (num_proc=2):   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Caching processed dataset at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00000_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00000_of_00002.arrow\n",
      "Applying chat template (num_proc=2):  50%|█████     | 60/120 [00:00<00:00, 364.84 examples/s]\n",
      "[2024-05-30 18:42:47,358] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "2024-05-30 18:42:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n",
      "2024-05-30 18:42:47 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='/opt/ml/input/data/model', model_revision='main', model_code_revision=None, torch_dtype='auto', tokenizer_name_or_path=None, trust_remote_code=False, use_flash_attention_2=True, use_peft=True, lora_r=64, lora_alpha=16, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False, bnb_4bit_quant_storage='uint8')\n",
      "2024-05-30 18:42:47 - INFO - __main__ - Data parameters DataArguments(chat_template=\"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", dataset_mixer={'/opt/ml/input/data/train': 1.0}, text_column='text', dataset_splits=['train', 'test'], dataset_configs=None, preprocessing_num_workers=2, truncation_side=None, auto_insert_empty_system_msg=True)\n",
      "2024-05-30 18:42:47 - INFO - __main__ - Training/evaluation parameters SFTConfig(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset_kwargs=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=zephyr-7b-sft-lora,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/opt/ml/output/tensorboard,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_seq_length=2048,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_bnb_8bit,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/opt/ml/model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/opt/ml/model,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Caching indices mapping at /opt/ml/input/data/train/train/cache-8aceb323340ea995.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Caching indices mapping at /opt/ml/input/data/train/train/cache-8aceb323340ea995.arrow\n",
      "Caching indices mapping at /opt/ml/input/data/train/test/cache-366cca8fb0c19703.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Caching indices mapping at /opt/ml/input/data/train/test/cache-366cca8fb0c19703.arrow\n",
      "2024-05-30 18:42:47 - INFO - __main__ - Training on the following datasets and their proportions: ['train : 1080', 'test : 120']\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,803 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,803 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,804 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,804 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,804 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,804 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,804 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,804 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,804 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-30 18:42:47,804 >> loading file tokenizer_config.json\n",
      "2024-05-30 18:42:47 - INFO - __main__ - *** Load pretrained model ***\n",
      "Process #0 will write at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00000_of_00002.arrow\n",
      "Process #1 will write at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00001_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Process #0 will write at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00000_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Process #1 will write at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00001_of_00002.arrow\n",
      "Spawning 2 processes\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Spawning 2 processes\n",
      "Applying chat template (num_proc=2):   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Caching processed dataset at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00001_of_00002.arrow\n",
      "2024-05-30 18:42:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00001_of_00002.arrow\n",
      "Applying chat template (num_proc=2): 100%|██████████| 120/120 [00:00<00:00, 452.44 examples/s]\n",
      "Concatenating 2 shards\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n",
      "Filter:   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Caching processed dataset at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00000_of_00002.arrow\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00000_of_00002.arrow\n",
      "Caching processed dataset at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00001_of_00002.arrow\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/train/cache-452f7ee24e8922e5_00001_of_00002.arrow\n",
      "Applying chat template (num_proc=2):   8%|▊         | 82/1080 [00:00<00:02, 430.60 examples/s]\n",
      "Applying chat template (num_proc=2):  72%|███████▏  | 779/1080 [00:00<00:00, 2820.22 examples/s]\n",
      "Applying chat template (num_proc=2): 100%|██████████| 1080/1080 [00:00<00:00, 2402.60 examples/s]\n",
      "Concatenating 2 shards\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n",
      "Process #0 will write at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00000_of_00002.arrow\n",
      "Process #1 will write at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00001_of_00002.arrow\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Process #0 will write at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00000_of_00002.arrow\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Process #1 will write at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00001_of_00002.arrow\n",
      "Spawning 2 processes\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Spawning 2 processes\n",
      "Applying chat template (num_proc=2):   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Caching processed dataset at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00000_of_00002.arrow\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00000_of_00002.arrow\n",
      "Applying chat template (num_proc=2):  50%|█████     | 60/120 [00:00<00:00, 375.69 examples/s]\n",
      "Caching processed dataset at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00001_of_00002.arrow\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/test/cache-fe5fde978b39a86c_00001_of_00002.arrow\n",
      "Applying chat template (num_proc=2): 100%|██████████| 120/120 [00:00<00:00, 466.43 examples/s]\n",
      "Concatenating 2 shards\n",
      "2024-05-30 18:42:48 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n",
      "Filter:   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Caching processed dataset at /opt/ml/input/data/train/train/cache-04f5526dd78417e1.arrow\n",
      "2024-05-30 18:42:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/train/cache-04f5526dd78417e1.arrow\n",
      "Filter: 100%|██████████| 1080/1080 [00:02<00:00, 539.45 examples/s]\n",
      "Filter: 100%|██████████| 1080/1080 [00:02<00:00, 539.12 examples/s]\n",
      "Filter:   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Caching processed dataset at /opt/ml/input/data/train/test/cache-07b2d443dc829453.arrow\n",
      "2024-05-30 18:42:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/test/cache-07b2d443dc829453.arrow\n",
      "Filter: 100%|██████████| 120/120 [00:00<00:00, 535.41 examples/s]\n",
      "Filter: 100%|██████████| 120/120 [00:00<00:00, 532.93 examples/s]\n",
      "2024-05-30 18:42:50 - INFO - __main__ - Decontaminated 0 (0.00%) samples from the training set.\n",
      "2024-05-30 18:42:50 - INFO - __main__ - Sample 228 of the processed training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Write a comprehensive proposal outlining the objectives, strategies, and expected outcomes for implementing a new employee wellness program that will promote physical, mental, and emotional health in the workplace. The proposal should include specific interventions, timelines, budget estimates, and metrics for measuring the success of the program. Additionally, it should address any potential barriers and offer solutions to overcome them. The proposal should be presented in a professional and persuasive style, utilizing evidence-based practices and research to support its recommendations.</s>\n",
      "<|assistant|>\n",
      "Executive Summary:\n",
      "The proposal aims to introduce a wellness program in our company that promotes the physical, mental, and emotional well-being of employees. Research has shown that such programs lead to increased job satisfaction, employee engagement, and productivity, and reduce healthcare costs. After conducting a needs assessment and analyzing data, we have identified three focus areas: physical activity, mental health, and nutrition. Based on the evidence from research and best practices, we have developed a comprehensive program that includes various interventions and strategies. The program will be implemented in three phases over one year, with a budget of $250,000. The outcome measures include the reduction of absenteeism, healthcare costs, and stress levels, and the improvement of employee engagement, morale, and productivity. The proposal also addresses potential barriers and suggests solutions to overcome them.\n",
      "Objectives:\n",
      "The objectives of the employee wellness program are:\n",
      "• To promote the physical, mental, and emotional well-being of employees\n",
      "• To reduce absenteeism and presenteeism due to illness or stress\n",
      "• To improve employee engagement, morale, and productivity\n",
      "• To reduce healthcare costs and insurance premiums\n",
      "• To enhance the company's reputation as an employer of choice\n",
      "• To create a culture of health and wellness in the workplace\n",
      "Strategies and Interventions:\n",
      "Physical Activity:\n",
      "The physical activity component of the wellness program aims to encourage employees to engage in regular exercise and increase their physical activity levels. The following interventions will be implemented:\n",
      "• Provide on-site fitness classes, such as yoga, pilates, and Zumba, at least twice a week\n",
      "• Offer standing desks, treadmill workstations, and other ergonomic equipment to promote movement and reduce sedentary behavior\n",
      "• Organize walking challenges, such as step counting competitions, with prizes for the winners\n",
      "• Provide healthy snacks in the break room, such as fresh fruit, granola bars, and nuts, to fuel physical activity\n",
      "Mental Health:\n",
      "The mental health component of the wellness program aims to promote psychological well-being, reduce stress levels, and increase resilience. The following interventions will be implemented:\n",
      "• Provide access to counseling services, either in-person or via teletherapy, for employees who need support\n",
      "• Organize stress management workshops, such as meditation, mindfulness, and deep breathing exercises\n",
      "• Offer mental health first aid training to managers and supervisors to enable them to identify and support employees who may be struggling\n",
      "• Provide resources and information on mental health, such as self-help books, apps, and websites\n",
      "Nutrition:\n",
      "The nutrition component of the wellness program aims to promote healthy eating habits and reduce unhealthy behaviors, such as junk food consumption and overeating. The following interventions will be implemented:\n",
      "• Offer healthy food options in the cafeteria, such as salads, vegetable-based dishes, and lean protein sources\n",
      "• Promote healthy eating through educational workshops and seminars on topics such as meal planning, portion control, and mindful eating\n",
      "• Provide healthy snack options in vending machines, such as fruit, trail mix, and low-fat popcorn\n",
      "• Offer nutrition counseling and coaching services for employees who want to make dietary improvements\n",
      "Timelines:\n",
      "The employee wellness program will be implemented in three phases over one year:\n",
      "Phase 1: Needs Assessment and Planning (Month 1-2)\n",
      "• Conduct an employee survey to gather data on health behaviors, preferences, and needs\n",
      "• Analyze data and identify focus areas and target populations\n",
      "• Develop the program components, interventions, and strategies\n",
      "• Establish a wellness committee to oversee the program\n",
      "Phase 2: Implementation (Month 3-9)\n",
      "• Roll out the program interventions and strategies in the three focus areas\n",
      "• Offer ongoing education and support to employees on wellness topics\n",
      "• Evaluate the program's effectiveness and make adjustments as needed\n",
      "Phase 3: Maintenance and Evaluation (Month 10-12)\n",
      "• Continue to offer the program interventions and monitor results\n",
      "• Collect and analyze program data to evaluate outcomes and make recommendations for future improvements\n",
      "• Celebrate successes and recognize employee achievements\n",
      "Budget Estimates:\n",
      "The total budget for the employee wellness program is $250,000. The breakdown of expenses is as follows:\n",
      "• Fitness classes and equipment: $80,000\n",
      "• Counseling services and mental health first aid training: $60,000\n",
      "• Nutrition counseling and healthy food options: $40,000\n",
      "• Program materials and resources: $20,000\n",
      "• Program evaluation and data analysis: $30,000\n",
      "• Wellness committee and staff time: $20,000\n",
      "Metrics for Measuring Success:\n",
      "The outcome measures for the program include:\n",
      "• Reduction of absenteeism and presenteeism due to illness or stress\n",
      "• Reduction of healthcare costs and insurance premiums\n",
      "• Improvement in employee engagement, morale, and productivity\n",
      "• Improvement in health behaviors, such as physical activity and healthy eating\n",
      "• Reduction of stress levels\n",
      "Potential Barriers and Solutions:\n",
      "Potential barriers to the success of the program include:\n",
      "• Lack of employee participation and motivation\n",
      "• Resistance from managers and supervisors\n",
      "• Budget constraints\n",
      "The following solutions are recommended to overcome these barriers:\n",
      "• Use incentives, such as prizes and rewards,</s>\n",
      "<|user|>\n",
      "Thanks for generating such a comprehensive proposal for the employee wellness program. Could you please provide some more information on how the program will be evaluated and how we can measure its success?</s>\n",
      "<|assistant|>\n",
      "Sure! Evaluation and metrics are critical to determining if the employee wellness program is successful and aligns with its objectives. Here are some ways to measure the success of the program:\n",
      "1. Absenteeism and Presenteeism: The program's success can be measured by tracking the number of days of absenteeism and presenteeism due to stress or illness. Keep a record of the data for each quarter and compare it to previous years to get an accurate picture of the program's effectiveness.\n",
      "2. Health Behaviors: The program can be evaluated by monitoring changes in health behaviors such as physical activity levels, healthy eating, and stress management strategies. This can be done by conducting surveys and using activity trackers to monitor employee activity levels.\n",
      "3. Healthcare Costs and Insurance premiums: The healthcare costs and insurance premiums can be analyzed to compare the ROI of implementing the employee wellness program with those of other years, to see if the program results in reduced healthcare costs.\n",
      "4. Employee Engagement and Productivity: Employee satisfaction surveys can be conducted to evaluate their experiences of the program, and productivity can be measured by tracking the number of positive workdays.\n",
      "5. Compliance with Participation: The number of consumer engagement in the program should be also monitored to track adherence and compliance with the program.\n",
      "6. Specific Program Results: Specific program measures can be developed to evaluate each component of the wellness program. For instance, the number of employees participating in a stress management workshop can be evaluated, similarly, the percentage of employees who joined fitness classes every week, employees who made dietary changes, and other changes in wellness behavior can be tracked.\n",
      "Overall, the program's outcomes will be measured using several metrics to determine how successful it is in terms of reaching the objectives defined. An adequate evaluation plan is critical when implementing the program to ensure that interventions and strategies are relevant and effective.</s>\n",
      "2024-05-30 18:42:50 - INFO - __main__ - Sample 51 of the processed training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Write a 5 to 7-page memoir about the role parental pressure played in your eating disorder. Please include specific examples of times when you felt pressured to meet certain expectations related to your weight, appearance, or diet. Additionally, consider exploring the emotional impact of this pressure on your relationship with your parents and how it may have affected other areas of your life. Use a conversational tone and feel free to incorporate personal anecdotes and reflections as appropriate.</s>\n",
      "<|assistant|>\n",
      "As I sit down to write this memoir about my eating disorder, I can't help but feel a sense of vulnerability. Talking about my struggles with food and my body is not easy, especially when it comes to the role that my parents played in the development of my disorder. However, I know that sharing my story can help others who may be going through similar struggles.\n",
      "Growing up, my parents placed a lot of importance on being thin and physically attractive. My mom was always dieting, counting calories and obsessing over her weight. She would make comments about her own body, saying things like \"I have to lose 10 pounds before my reunion\" or \"I can't believe how big my thighs look in these jeans.\" Her constant fixation on her appearance made me feel as though it was the most important thing in the world.\n",
      "When I was around 12, I started to gain weight. My mom would make comments about my body, saying things like \"you're getting a little chubby\" or \"maybe you should skip dessert tonight.\" My dad, too, would make comments about my weight, though he was less direct than my mom. He would suggest that I take up a sport or activity to \"stay in shape,\" or that I should \"watch what I eat\" to avoid gaining more weight.\n",
      "Their comments affected me deeply. I started to feel as though my body was always under scrutiny, as though I wasn't good enough just the way I was. I started to feel ashamed of myself, and my confidence plummeted. Food became my refuge, my way of coping with the stress and anxiety that came with feeling like I was constantly failing.\n",
      "At first, I would sneak treats and hide them in my room. I would hoard candy and chips, eating them when no one was around. But soon, my eating spiraled out of control. I would binge eat entire containers of ice cream, eat whole pizza pies by myself, or consume entire bags of chips in one sitting. I felt out of control, as though my hunger was a monster that I couldn't tame. I hated myself for how much I was eating and how little control I had over it.\n",
      "My parents started to notice that I was gaining weight rapidly, and they became more vocal about it. My mom started to make more pointed comments about my size, saying things like \"are you sure you need that second helping?\" or \"maybe you should start watching your weight.\" My dad would comment on my eating habits, asking why I was always going back for more food or questioning why I wasn't losing weight despite my \"healthy\" diet.\n",
      "Their comments made me feel even more ashamed and out of control. I started to feel like my body was the source of all my problems, that my weight was what defined me. I became more and more isolated, preferring to spend time alone rather than with my family. I stopped participating in activities I used to enjoy, like sports or music, because I felt like I didn't deserve to enjoy them since I was overweight.\n",
      "As I got older, my eating disorder morphed into different forms. Sometimes I would restrict my eating, trying to be as skinny as possible. Other times I would purge, making myself throw up after meals or taking laxatives to get rid of the food I had eaten. My parents started to notice that something was wrong, but they didn't understand what was happening.\n",
      "When they asked me about my eating habits, I would lie and say that everything was fine. I didn't know how to explain to them that their comments and pressure had caused me to spiral into this pattern of self-destructive behavior. I didn't know how to ask for help, or how to tell them that I needed them to stop commenting on my body and my eating habits.\n",
      "It took many years and a lot of therapy to undo the damage that my parents' pressure and comments had caused. But even now, years later, I still struggle with feeling like my body isn't good enough. I still have moments where I feel like I need to restrict my eating or purge after a meal. But I know now that these feelings aren't my fault and that I deserve to be happy and healthy no matter what my body looks like.\n",
      "Looking back, I wish my parents had known how much their comments hurt me, how they caused me to develop an unhealthy relationship with food and my body. I wish they had known that their love and acceptance was more important than any number on a scale or the size of my clothing. But I also know that they were doing the best they could with the tools they had, and that they loved me to the best of their ability.\n",
      "Now, I am working on healing and finding ways to love and accept my body. It's not easy, but it's worth it. I hope that by sharing my story, I can help others who may be struggling with similar issues. Remember, you are more than your body, and you deserve to be happy and healthy no matter what.</s>\n",
      "<|user|>\n",
      "It was very insightful and relatable. Could you add more information about how your eating disorder affected your mental health and daily life?</s>\n",
      "<|assistant|>\n",
      "Sure, I'd be happy to add more about how my eating disorder affected my mental health and daily life. As my eating disorder progressed, it began to take a toll on my mental health. I became increasingly anxious and depressed, feeling as though I was always on edge or in a low mood. I started to feel ashamed of myself, almost as though I was a failure because I couldn't control my eating habits. These negative emotions began to consume me, making it difficult to focus on anything else. I found myself obsessing about my weight and my body image, constantly comparing myself to others and feeling like I didn't measure up. This, in turn, made it harder to interact with people or participate in activities that I used to enjoy. For example, I started to become more withdrawn, spending more time alone and avoiding social situations. I didn't want to go out to eat with friends or go to parties because I was afraid of being judged or being tempted by food. This sense of isolation only made my mental health worse, as I became more and more stuck in my own negative thoughts. My daily life was also affected in more practical ways. My eating disorder made it difficult for me to maintain a healthy diet or exercise routine. I was either overeating or undereating, never finding a balance that worked for me. This also meant that I struggled with maintaining a healthy weight, which only perpetuated the negative feelings I had about my body. Additionally, my eating disorder affected my ability to concentrate and be productive. When I was in the midst of a binge or purge episode, I couldn't focus on anything else. I would procrastinate on schoolwork or sit at my desk for hours, unable to get anything done. This, again, made me feel like a failure and only added to my stress and anxiety. Overall, my eating disorder was a pervasive presence in my life, affecting everything from my mental health to my daily routines. It took a lot of time and effort to overcome, but I'm grateful to be in a better place now.</s>\n",
      "2024-05-30 18:42:50 - INFO - __main__ - Sample 563 of the processed training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Explain the cultural significance of the symbolism behind the leaning Tower of Pisa in Italian art history, and provide examples of how it has been represented in paintings, literature and other media throughout the centuries.</s>\n",
      "<|assistant|>\n",
      "The Leaning Tower of Pisa has been a symbol of Italian art and culture for centuries. The significance of this leaning tower lies in its unique architectural design, which has come to represent the ingenuity, innovation, and creative spirit of the Italian people.\n",
      "In Italian art history, the tower has been a prominent subject in paintings and other forms of media, often serving as a metaphor for the human condition. Artists have used the tower to convey a range of themes, including the fragility of human existence, the passing of time, and the struggles that people face in life.\n",
      "One of the most famous examples of the Leaning Tower in art is the painting by Giovanni Paolo Panini, \"Interior of a Picture Gallery with the Collection of Cardinal Silvio Valenti Gonzaga.\" This painting depicts the tower in the background, leaning dramatically over the cityscape. The painting is significant because it demonstrates the Leaning Tower's association with the past and the idea of Italy as a nation with a rich cultural heritage.\n",
      "In literature, the Leaning Tower of Pisa has been referenced countless times over the years. In Dante Alighieri's famous work, \"The Divine Comedy,\" the tower is mentioned as a symbol of sin and evil, and as a warning against the dangers of pride and arrogance.\n",
      "Overall, the cultural significance of the Leaning Tower of Pisa lies in its metaphorical power as a symbol of human struggle and achievement. It has been represented in countless works of art and literature throughout the centuries, serving as a reminder of the importance of innovation, creativity, and perseverance in the face of adversity.</s>\n",
      "<|user|>\n",
      "That's a really interesting explanation! Could you give me some more examples of how the Leaning Tower of Pisa has been represented in literature besides Dante's \"The Divine Comedy\"? And also, are there any other famous paintings or artworks that feature the Tower?</s>\n",
      "<|assistant|>\n",
      "Yes, of course! The Leaning Tower of Pisa has been referenced in literature beyond Dante's \"The Divine Comedy.\" For example, in William Shakespeare's play \"Twelfth Night,\" the character Sir Andrew Aguecheek compares his dancing to the leaning tower in order to mock himself:\n",
      "\"I can see no rhyme in the reason why I should dance \n",
      "barefoot to the tower like a peasant.\"\n",
      "Additionally, the tower has been described in works by other authors such as Giuseppe Giusti and Giovanni Boccaccio.\n",
      "In terms of paintings and artworks, the Tower has been featured in many works throughout the centuries. One of the most famous is the \"View of Pisa\" by Joachim Patinir, which was painted around 1520. The painting shows the Tower prominently, leaning in the background over the city, and it is believed to be among the first depictions of the Tower in art.\n",
      "Another famous work is \"The Leaning Tower of Pisa\" by Eugène Delacroix. This painting depicts the Tower from a dramatic, low angle and captures the sense of precariousness and instability that is associated with the structure. \n",
      "In contemporary art, the Tower has also been used as a subject, such as in David Mach's sculpture \"Big Heids in Pisa\" which features giant matchstick figures leaning against the Tower.\n",
      "Overall, the Leaning Tower of Pisa has been a popular subject in literature and visual art throughout the years, and it continues to captivate and inspire people from all over the world.</s>\n",
      "<|user|>\n",
      "Could you also tell me some fun facts about the Leaning Tower of Pisa? It would be interesting to learn more about it beyond its cultural significance.</s>\n",
      "<|assistant|>\n",
      "Sure, here are some interesting facts about the Leaning Tower of Pisa:\n",
      "1. The Tower started leaning shortly after construction began in 1173. It is believed that the ground was not stable enough to support the weight of the structure and caused it to sink on one side. 2. The Tower is not alone. It is actually one of four buildings that make up the Piazza dei Miracoli (Square of Miracles) complex in Pisa, Italy. The others include the Cathedral, the Baptistery, and the Camposanto Monumentale. 3. The Tower is not perfectly straight, but rather, it has a slight curve. The architects corrected the angle of the Tower during construction to try to compensate for its leaning position, but this actually caused the Tower to have a bit of a curve in its shape. 4. The Tower stands at 56 meters (183.3 feet) tall and has a weight of approximately 14,500 tons. 5. The construction of the Tower took over 200 years to complete. It was originally supposed to be completed in just two decades, but delays and interruptions caused the project to stretch out over centuries. 6. The Tower was almost destroyed during World War II when the German army occupied the area. Luckily, an American Army Sergeant named Leon Weckstein recognized the cultural and historical significance of the building and made sure that Allied forces did not bomb it. 7. In 2008, the Tower was fitted with a set of 11 counterweights to help stabilize it and prevent further leaning. These weights were installed under the base of the Tower and have been successful in reducing the amount of leaning. 8. The Leaning Tower of Pisa has been featured in many pop culture references, such as in video games, movies, and television shows. Some examples include the video game \"Assassin's Creed II,\" the movie \"The Italian Job,\" and the TV show \"The Simpsons.\" I hope you found those facts interesting!</s>\n",
      "NCCL version 2.18.5+cuda12.1\n",
      "Caching processed dataset at /opt/ml/input/data/train/train/cache-04f5526dd78417e1.arrow\n",
      "2024-05-30 18:42:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/train/cache-04f5526dd78417e1.arrow\n",
      "Filter: 100%|██████████| 1080/1080 [00:02<00:00, 526.45 examples/s]\n",
      "Filter: 100%|██████████| 1080/1080 [00:02<00:00, 526.13 examples/s]\n",
      "Filter:   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "algo-1:201:291 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-1:201:291 [0] nccl_net_ofi_init:1239 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "Caching processed dataset at /opt/ml/input/data/train/test/cache-07b2d443dc829453.arrow\n",
      "2024-05-30 18:42:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /opt/ml/input/data/train/test/cache-07b2d443dc829453.arrow\n",
      "Filter: 100%|██████████| 120/120 [00:00<00:00, 532.04 examples/s]\n",
      "Filter: 100%|██████████| 120/120 [00:00<00:00, 529.31 examples/s]\n",
      "2024-05-30 18:42:51 - INFO - __main__ - Decontaminated 0 (0.00%) samples from the training set.\n",
      "2024-05-30 18:42:51 - INFO - __main__ - Sample 228 of the processed training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Write a comprehensive proposal outlining the objectives, strategies, and expected outcomes for implementing a new employee wellness program that will promote physical, mental, and emotional health in the workplace. The proposal should include specific interventions, timelines, budget estimates, and metrics for measuring the success of the program. Additionally, it should address any potential barriers and offer solutions to overcome them. The proposal should be presented in a professional and persuasive style, utilizing evidence-based practices and research to support its recommendations.</s>\n",
      "<|assistant|>\n",
      "Executive Summary:\n",
      "The proposal aims to introduce a wellness program in our company that promotes the physical, mental, and emotional well-being of employees. Research has shown that such programs lead to increased job satisfaction, employee engagement, and productivity, and reduce healthcare costs. After conducting a needs assessment and analyzing data, we have identified three focus areas: physical activity, mental health, and nutrition. Based on the evidence from research and best practices, we have developed a comprehensive program that includes various interventions and strategies. The program will be implemented in three phases over one year, with a budget of $250,000. The outcome measures include the reduction of absenteeism, healthcare costs, and stress levels, and the improvement of employee engagement, morale, and productivity. The proposal also addresses potential barriers and suggests solutions to overcome them.\n",
      "Objectives:\n",
      "The objectives of the employee wellness program are:\n",
      "• To promote the physical, mental, and emotional well-being of employees\n",
      "• To reduce absenteeism and presenteeism due to illness or stress\n",
      "• To improve employee engagement, morale, and productivity\n",
      "• To reduce healthcare costs and insurance premiums\n",
      "• To enhance the company's reputation as an employer of choice\n",
      "• To create a culture of health and wellness in the workplace\n",
      "Strategies and Interventions:\n",
      "Physical Activity:\n",
      "The physical activity component of the wellness program aims to encourage employees to engage in regular exercise and increase their physical activity levels. The following interventions will be implemented:\n",
      "• Provide on-site fitness classes, such as yoga, pilates, and Zumba, at least twice a week\n",
      "• Offer standing desks, treadmill workstations, and other ergonomic equipment to promote movement and reduce sedentary behavior\n",
      "• Organize walking challenges, such as step counting competitions, with prizes for the winners\n",
      "• Provide healthy snacks in the break room, such as fresh fruit, granola bars, and nuts, to fuel physical activity\n",
      "Mental Health:\n",
      "The mental health component of the wellness program aims to promote psychological well-being, reduce stress levels, and increase resilience. The following interventions will be implemented:\n",
      "• Provide access to counseling services, either in-person or via teletherapy, for employees who need support\n",
      "• Organize stress management workshops, such as meditation, mindfulness, and deep breathing exercises\n",
      "• Offer mental health first aid training to managers and supervisors to enable them to identify and support employees who may be struggling\n",
      "• Provide resources and information on mental health, such as self-help books, apps, and websites\n",
      "Nutrition:\n",
      "The nutrition component of the wellness program aims to promote healthy eating habits and reduce unhealthy behaviors, such as junk food consumption and overeating. The following interventions will be implemented:\n",
      "• Offer healthy food options in the cafeteria, such as salads, vegetable-based dishes, and lean protein sources\n",
      "• Promote healthy eating through educational workshops and seminars on topics such as meal planning, portion control, and mindful eating\n",
      "• Provide healthy snack options in vending machines, such as fruit, trail mix, and low-fat popcorn\n",
      "• Offer nutrition counseling and coaching services for employees who want to make dietary improvements\n",
      "Timelines:\n",
      "The employee wellness program will be implemented in three phases over one year:\n",
      "Phase 1: Needs Assessment and Planning (Month 1-2)\n",
      "• Conduct an employee survey to gather data on health behaviors, preferences, and needs\n",
      "• Analyze data and identify focus areas and target populations\n",
      "• Develop the program components, interventions, and strategies\n",
      "• Establish a wellness committee to oversee the program\n",
      "Phase 2: Implementation (Month 3-9)\n",
      "• Roll out the program interventions and strategies in the three focus areas\n",
      "• Offer ongoing education and support to employees on wellness topics\n",
      "• Evaluate the program's effectiveness and make adjustments as needed\n",
      "Phase 3: Maintenance and Evaluation (Month 10-12)\n",
      "• Continue to offer the program interventions and monitor results\n",
      "• Collect and analyze program data to evaluate outcomes and make recommendations for future improvements\n",
      "• Celebrate successes and recognize employee achievements\n",
      "Budget Estimates:\n",
      "The total budget for the employee wellness program is $250,000. The breakdown of expenses is as follows:\n",
      "• Fitness classes and equipment: $80,000\n",
      "• Counseling services and mental health first aid training: $60,000\n",
      "• Nutrition counseling and healthy food options: $40,000\n",
      "• Program materials and resources: $20,000\n",
      "• Program evaluation and data analysis: $30,000\n",
      "• Wellness committee and staff time: $20,000\n",
      "Metrics for Measuring Success:\n",
      "The outcome measures for the program include:\n",
      "• Reduction of absenteeism and presenteeism due to illness or stress\n",
      "• Reduction of healthcare costs and insurance premiums\n",
      "• Improvement in employee engagement, morale, and productivity\n",
      "• Improvement in health behaviors, such as physical activity and healthy eating\n",
      "• Reduction of stress levels\n",
      "Potential Barriers and Solutions:\n",
      "Potential barriers to the success of the program include:\n",
      "• Lack of employee participation and motivation\n",
      "• Resistance from managers and supervisors\n",
      "• Budget constraints\n",
      "The following solutions are recommended to overcome these barriers:\n",
      "• Use incentives, such as prizes and rewards,</s>\n",
      "<|user|>\n",
      "Thanks for generating such a comprehensive proposal for the employee wellness program. Could you please provide some more information on how the program will be evaluated and how we can measure its success?</s>\n",
      "<|assistant|>\n",
      "Sure! Evaluation and metrics are critical to determining if the employee wellness program is successful and aligns with its objectives. Here are some ways to measure the success of the program:\n",
      "1. Absenteeism and Presenteeism: The program's success can be measured by tracking the number of days of absenteeism and presenteeism due to stress or illness. Keep a record of the data for each quarter and compare it to previous years to get an accurate picture of the program's effectiveness.\n",
      "2. Health Behaviors: The program can be evaluated by monitoring changes in health behaviors such as physical activity levels, healthy eating, and stress management strategies. This can be done by conducting surveys and using activity trackers to monitor employee activity levels.\n",
      "3. Healthcare Costs and Insurance premiums: The healthcare costs and insurance premiums can be analyzed to compare the ROI of implementing the employee wellness program with those of other years, to see if the program results in reduced healthcare costs.\n",
      "4. Employee Engagement and Productivity: Employee satisfaction surveys can be conducted to evaluate their experiences of the program, and productivity can be measured by tracking the number of positive workdays.\n",
      "5. Compliance with Participation: The number of consumer engagement in the program should be also monitored to track adherence and compliance with the program.\n",
      "6. Specific Program Results: Specific program measures can be developed to evaluate each component of the wellness program. For instance, the number of employees participating in a stress management workshop can be evaluated, similarly, the percentage of employees who joined fitness classes every week, employees who made dietary changes, and other changes in wellness behavior can be tracked.\n",
      "Overall, the program's outcomes will be measured using several metrics to determine how successful it is in terms of reaching the objectives defined. An adequate evaluation plan is critical when implementing the program to ensure that interventions and strategies are relevant and effective.</s>\n",
      "2024-05-30 18:42:51 - INFO - __main__ - Sample 51 of the processed training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Write a 5 to 7-page memoir about the role parental pressure played in your eating disorder. Please include specific examples of times when you felt pressured to meet certain expectations related to your weight, appearance, or diet. Additionally, consider exploring the emotional impact of this pressure on your relationship with your parents and how it may have affected other areas of your life. Use a conversational tone and feel free to incorporate personal anecdotes and reflections as appropriate.</s>\n",
      "<|assistant|>\n",
      "As I sit down to write this memoir about my eating disorder, I can't help but feel a sense of vulnerability. Talking about my struggles with food and my body is not easy, especially when it comes to the role that my parents played in the development of my disorder. However, I know that sharing my story can help others who may be going through similar struggles.\n",
      "Growing up, my parents placed a lot of importance on being thin and physically attractive. My mom was always dieting, counting calories and obsessing over her weight. She would make comments about her own body, saying things like \"I have to lose 10 pounds before my reunion\" or \"I can't believe how big my thighs look in these jeans.\" Her constant fixation on her appearance made me feel as though it was the most important thing in the world.\n",
      "When I was around 12, I started to gain weight. My mom would make comments about my body, saying things like \"you're getting a little chubby\" or \"maybe you should skip dessert tonight.\" My dad, too, would make comments about my weight, though he was less direct than my mom. He would suggest that I take up a sport or activity to \"stay in shape,\" or that I should \"watch what I eat\" to avoid gaining more weight.\n",
      "Their comments affected me deeply. I started to feel as though my body was always under scrutiny, as though I wasn't good enough just the way I was. I started to feel ashamed of myself, and my confidence plummeted. Food became my refuge, my way of coping with the stress and anxiety that came with feeling like I was constantly failing.\n",
      "At first, I would sneak treats and hide them in my room. I would hoard candy and chips, eating them when no one was around. But soon, my eating spiraled out of control. I would binge eat entire containers of ice cream, eat whole pizza pies by myself, or consume entire bags of chips in one sitting. I felt out of control, as though my hunger was a monster that I couldn't tame. I hated myself for how much I was eating and how little control I had over it.\n",
      "My parents started to notice that I was gaining weight rapidly, and they became more vocal about it. My mom started to make more pointed comments about my size, saying things like \"are you sure you need that second helping?\" or \"maybe you should start watching your weight.\" My dad would comment on my eating habits, asking why I was always going back for more food or questioning why I wasn't losing weight despite my \"healthy\" diet.\n",
      "Their comments made me feel even more ashamed and out of control. I started to feel like my body was the source of all my problems, that my weight was what defined me. I became more and more isolated, preferring to spend time alone rather than with my family. I stopped participating in activities I used to enjoy, like sports or music, because I felt like I didn't deserve to enjoy them since I was overweight.\n",
      "As I got older, my eating disorder morphed into different forms. Sometimes I would restrict my eating, trying to be as skinny as possible. Other times I would purge, making myself throw up after meals or taking laxatives to get rid of the food I had eaten. My parents started to notice that something was wrong, but they didn't understand what was happening.\n",
      "When they asked me about my eating habits, I would lie and say that everything was fine. I didn't know how to explain to them that their comments and pressure had caused me to spiral into this pattern of self-destructive behavior. I didn't know how to ask for help, or how to tell them that I needed them to stop commenting on my body and my eating habits.\n",
      "It took many years and a lot of therapy to undo the damage that my parents' pressure and comments had caused. But even now, years later, I still struggle with feeling like my body isn't good enough. I still have moments where I feel like I need to restrict my eating or purge after a meal. But I know now that these feelings aren't my fault and that I deserve to be happy and healthy no matter what my body looks like.\n",
      "Looking back, I wish my parents had known how much their comments hurt me, how they caused me to develop an unhealthy relationship with food and my body. I wish they had known that their love and acceptance was more important than any number on a scale or the size of my clothing. But I also know that they were doing the best they could with the tools they had, and that they loved me to the best of their ability.\n",
      "Now, I am working on healing and finding ways to love and accept my body. It's not easy, but it's worth it. I hope that by sharing my story, I can help others who may be struggling with similar issues. Remember, you are more than your body, and you deserve to be happy and healthy no matter what.</s>\n",
      "<|user|>\n",
      "It was very insightful and relatable. Could you add more information about how your eating disorder affected your mental health and daily life?</s>\n",
      "<|assistant|>\n",
      "Sure, I'd be happy to add more about how my eating disorder affected my mental health and daily life. As my eating disorder progressed, it began to take a toll on my mental health. I became increasingly anxious and depressed, feeling as though I was always on edge or in a low mood. I started to feel ashamed of myself, almost as though I was a failure because I couldn't control my eating habits. These negative emotions began to consume me, making it difficult to focus on anything else. I found myself obsessing about my weight and my body image, constantly comparing myself to others and feeling like I didn't measure up. This, in turn, made it harder to interact with people or participate in activities that I used to enjoy. For example, I started to become more withdrawn, spending more time alone and avoiding social situations. I didn't want to go out to eat with friends or go to parties because I was afraid of being judged or being tempted by food. This sense of isolation only made my mental health worse, as I became more and more stuck in my own negative thoughts. My daily life was also affected in more practical ways. My eating disorder made it difficult for me to maintain a healthy diet or exercise routine. I was either overeating or undereating, never finding a balance that worked for me. This also meant that I struggled with maintaining a healthy weight, which only perpetuated the negative feelings I had about my body. Additionally, my eating disorder affected my ability to concentrate and be productive. When I was in the midst of a binge or purge episode, I couldn't focus on anything else. I would procrastinate on schoolwork or sit at my desk for hours, unable to get anything done. This, again, made me feel like a failure and only added to my stress and anxiety. Overall, my eating disorder was a pervasive presence in my life, affecting everything from my mental health to my daily routines. It took a lot of time and effort to overcome, but I'm grateful to be in a better place now.</s>\n",
      "2024-05-30 18:42:51 - INFO - __main__ - Sample 563 of the processed training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Explain the cultural significance of the symbolism behind the leaning Tower of Pisa in Italian art history, and provide examples of how it has been represented in paintings, literature and other media throughout the centuries.</s>\n",
      "<|assistant|>\n",
      "The Leaning Tower of Pisa has been a symbol of Italian art and culture for centuries. The significance of this leaning tower lies in its unique architectural design, which has come to represent the ingenuity, innovation, and creative spirit of the Italian people.\n",
      "In Italian art history, the tower has been a prominent subject in paintings and other forms of media, often serving as a metaphor for the human condition. Artists have used the tower to convey a range of themes, including the fragility of human existence, the passing of time, and the struggles that people face in life.\n",
      "One of the most famous examples of the Leaning Tower in art is the painting by Giovanni Paolo Panini, \"Interior of a Picture Gallery with the Collection of Cardinal Silvio Valenti Gonzaga.\" This painting depicts the tower in the background, leaning dramatically over the cityscape. The painting is significant because it demonstrates the Leaning Tower's association with the past and the idea of Italy as a nation with a rich cultural heritage.\n",
      "In literature, the Leaning Tower of Pisa has been referenced countless times over the years. In Dante Alighieri's famous work, \"The Divine Comedy,\" the tower is mentioned as a symbol of sin and evil, and as a warning against the dangers of pride and arrogance.\n",
      "Overall, the cultural significance of the Leaning Tower of Pisa lies in its metaphorical power as a symbol of human struggle and achievement. It has been represented in countless works of art and literature throughout the centuries, serving as a reminder of the importance of innovation, creativity, and perseverance in the face of adversity.</s>\n",
      "<|user|>\n",
      "That's a really interesting explanation! Could you give me some more examples of how the Leaning Tower of Pisa has been represented in literature besides Dante's \"The Divine Comedy\"? And also, are there any other famous paintings or artworks that feature the Tower?</s>\n",
      "<|assistant|>\n",
      "Yes, of course! The Leaning Tower of Pisa has been referenced in literature beyond Dante's \"The Divine Comedy.\" For example, in William Shakespeare's play \"Twelfth Night,\" the character Sir Andrew Aguecheek compares his dancing to the leaning tower in order to mock himself:\n",
      "\"I can see no rhyme in the reason why I should dance \n",
      "barefoot to the tower like a peasant.\"\n",
      "Additionally, the tower has been described in works by other authors such as Giuseppe Giusti and Giovanni Boccaccio.\n",
      "In terms of paintings and artworks, the Tower has been featured in many works throughout the centuries. One of the most famous is the \"View of Pisa\" by Joachim Patinir, which was painted around 1520. The painting shows the Tower prominently, leaning in the background over the city, and it is believed to be among the first depictions of the Tower in art.\n",
      "Another famous work is \"The Leaning Tower of Pisa\" by Eugène Delacroix. This painting depicts the Tower from a dramatic, low angle and captures the sense of precariousness and instability that is associated with the structure. \n",
      "In contemporary art, the Tower has also been used as a subject, such as in David Mach's sculpture \"Big Heids in Pisa\" which features giant matchstick figures leaning against the Tower.\n",
      "Overall, the Leaning Tower of Pisa has been a popular subject in literature and visual art throughout the years, and it continues to captivate and inspire people from all over the world.</s>\n",
      "<|user|>\n",
      "Could you also tell me some fun facts about the Leaning Tower of Pisa? It would be interesting to learn more about it beyond its cultural significance.</s>\n",
      "<|assistant|>\n",
      "Sure, here are some interesting facts about the Leaning Tower of Pisa:\n",
      "1. The Tower started leaning shortly after construction began in 1173. It is believed that the ground was not stable enough to support the weight of the structure and caused it to sink on one side. 2. The Tower is not alone. It is actually one of four buildings that make up the Piazza dei Miracoli (Square of Miracles) complex in Pisa, Italy. The others include the Cathedral, the Baptistery, and the Camposanto Monumentale. 3. The Tower is not perfectly straight, but rather, it has a slight curve. The architects corrected the angle of the Tower during construction to try to compensate for its leaning position, but this actually caused the Tower to have a bit of a curve in its shape. 4. The Tower stands at 56 meters (183.3 feet) tall and has a weight of approximately 14,500 tons. 5. The construction of the Tower took over 200 years to complete. It was originally supposed to be completed in just two decades, but delays and interruptions caused the project to stretch out over centuries. 6. The Tower was almost destroyed during World War II when the German army occupied the area. Luckily, an American Army Sergeant named Leon Weckstein recognized the cultural and historical significance of the building and made sure that Allied forces did not bomb it. 7. In 2008, the Tower was fitted with a set of 11 counterweights to help stabilize it and prevent further leaning. These weights were installed under the base of the Tower and have been successful in reducing the amount of leaning. 8. The Leaning Tower of Pisa has been featured in many pop culture references, such as in video games, movies, and television shows. Some examples include the video game \"Assassin's Creed II,\" the movie \"The Italian Job,\" and the TV show \"The Simpsons.\" I hope you found those facts interesting!</s>\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:731] 2024-05-30 18:42:52,660 >> loading configuration file /opt/ml/input/data/model/config.json\n",
      "[INFO|configuration_utils.py:731] 2024-05-30 18:42:52,660 >> loading configuration file /opt/ml/input/data/model/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-05-30 18:42:52,661 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "[INFO|configuration_utils.py:796] 2024-05-30 18:42:52,661 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "[INFO|modeling_utils.py:3471] 2024-05-30 18:42:52,666 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:3471] 2024-05-30 18:42:52,666 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:3570] 2024-05-30 18:42:52,666 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:3570] 2024-05-30 18:42:52,666 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:1519] 2024-05-30 18:42:52,667 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|modeling_utils.py:1519] 2024-05-30 18:42:52,667 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-30 18:42:52,667 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "[WARNING|logging.py:329] 2024-05-30 18:42:52,667 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "[INFO|configuration_utils.py:962] 2024-05-30 18:42:52,670 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "[INFO|configuration_utils.py:962] 2024-05-30 18:42:52,670 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "algo-2:197:289 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-2:197:289 [0] nccl_net_ofi_init:1239 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:731] 2024-05-30 18:42:52,661 >> loading configuration file /opt/ml/input/data/model/config.json\n",
      "[INFO|configuration_utils.py:731] 2024-05-30 18:42:52,661 >> loading configuration file /opt/ml/input/data/model/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-05-30 18:42:52,662 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "[INFO|configuration_utils.py:796] 2024-05-30 18:42:52,662 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "[INFO|modeling_utils.py:3471] 2024-05-30 18:42:52,667 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:3471] 2024-05-30 18:42:52,667 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:3570] 2024-05-30 18:42:52,667 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:3570] 2024-05-30 18:42:52,667 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:1519] 2024-05-30 18:42:52,667 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|modeling_utils.py:1519] 2024-05-30 18:42:52,667 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-30 18:42:52,667 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "[WARNING|logging.py:329] 2024-05-30 18:42:52,667 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "[INFO|configuration_utils.py:962] 2024-05-30 18:42:52,671 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "[INFO|configuration_utils.py:962] 2024-05-30 18:42:52,671 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.37s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.41s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.87s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.80s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-05-30 18:43:09,534 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "[INFO|modeling_utils.py:4288] 2024-05-30 18:43:09,534 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|modeling_utils.py:4280] 2024-05-30 18:43:09,534 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "[INFO|modeling_utils.py:4288] 2024-05-30 18:43:09,534 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:915] 2024-05-30 18:43:09,538 >> loading configuration file /opt/ml/input/data/model/generation_config.json\n",
      "[INFO|configuration_utils.py:915] 2024-05-30 18:43:09,538 >> loading configuration file /opt/ml/input/data/model/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-05-30 18:43:09,538 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "[INFO|configuration_utils.py:962] 2024-05-30 18:43:09,538 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[INFO|training_args.py:2052] 2024-05-30 18:43:09,722 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:2052] 2024-05-30 18:43:09,722 >> PyTorch: setting up devices\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.95s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.87s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-05-30 18:43:09,763 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "[INFO|modeling_utils.py:4288] 2024-05-30 18:43:09,763 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|modeling_utils.py:4280] 2024-05-30 18:43:09,763 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "[INFO|modeling_utils.py:4288] 2024-05-30 18:43:09,763 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:915] 2024-05-30 18:43:09,767 >> loading configuration file /opt/ml/input/data/model/generation_config.json\n",
      "[INFO|configuration_utils.py:915] 2024-05-30 18:43:09,767 >> loading configuration file /opt/ml/input/data/model/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-05-30 18:43:09,767 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "[INFO|configuration_utils.py:962] 2024-05-30 18:43:09,767 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[INFO|training_args.py:2052] 2024-05-30 18:43:09,947 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:2052] 2024-05-30 18:43:09,947 >> PyTorch: setting up devices\n",
      "Using custom data configuration default-c2737fd693eb441a\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "2024-05-30 18:43:10 - INFO - datasets.builder - Using custom data configuration default-c2737fd693eb441a\n",
      "2024-05-30 18:43:10 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "Generating dataset generator (/root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0)\n",
      "2024-05-30 18:43:10 - INFO - datasets.builder - Generating dataset generator (/root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0)\n",
      "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0...\n",
      "2024-05-30 18:43:10 - INFO - datasets.builder - Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0...\n",
      "Generating train split\n",
      "2024-05-30 18:43:10 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Using custom data configuration default-c2737fd693eb441a\n",
      "2024-05-30 18:43:10 - INFO - datasets.builder - Using custom data configuration default-c2737fd693eb441a\n",
      "2024-05-30 18:43:10 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "Generating dataset generator (/root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0)\n",
      "2024-05-30 18:43:10 - INFO - datasets.builder - Generating dataset generator (/root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0)\n",
      "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0...\n",
      "2024-05-30 18:43:10 - INFO - datasets.builder - Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0...\n",
      "Generating train split\n",
      "2024-05-30 18:43:10 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "[WARNING|tokenization_utils_base.py:3921] 2024-05-30 18:43:13,378 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2169 > 2048). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3921] 2024-05-30 18:43:13,378 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2169 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Generating train split: 1 examples [00:02,  2.95s/ examples]\n",
      "Generating train split: 303 examples [00:03, 139.93 examples/s]\n",
      "Generating train split: 650 examples [00:03, 346.21 examples/s]\n",
      "Generating train split: 719 examples [00:03, 219.74 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "2024-05-30 18:43:13 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0. Subsequent calls will reuse this data.\n",
      "2024-05-30 18:43:13 - INFO - datasets.builder - Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-98dd4ee92e213105\n",
      "2024-05-30 18:43:13 - INFO - datasets.builder - Using custom data configuration default-98dd4ee92e213105\n",
      "2024-05-30 18:43:13 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "Generating dataset generator (/root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0)\n",
      "2024-05-30 18:43:13 - INFO - datasets.builder - Generating dataset generator (/root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0)\n",
      "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0...\n",
      "2024-05-30 18:43:13 - INFO - datasets.builder - Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0...\n",
      "Generating train split\n",
      "2024-05-30 18:43:13 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "[WARNING|tokenization_utils_base.py:3921] 2024-05-30 18:43:13,775 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2169 > 2048). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3921] 2024-05-30 18:43:13,775 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2169 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Generating train split: 1 examples [00:02,  2.99s/ examples]\n",
      "Generating train split: 315 examples [00:03, 143.70 examples/s]\n",
      "Generating train split: 1 examples [00:00,  2.69 examples/s]\n",
      "Generating train split: 79 examples [00:00, 197.15 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "2024-05-30 18:43:14 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0. Subsequent calls will reuse this data.\n",
      "2024-05-30 18:43:14 - INFO - datasets.builder - Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0. Subsequent calls will reuse this data.\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:641] 2024-05-30 18:43:14,733 >> Using auto half precision backend\n",
      "[INFO|trainer.py:641] 2024-05-30 18:43:14,733 >> Using auto half precision backend\n",
      "2024-05-30 18:43:14 - INFO - __main__ - *** Train ***\n",
      "Generating train split: 675 examples [00:03, 355.28 examples/s]\n",
      "Generating train split: 719 examples [00:03, 217.77 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "2024-05-30 18:43:14 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0. Subsequent calls will reuse this data.\n",
      "2024-05-30 18:43:14 - INFO - datasets.builder - Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-c2737fd693eb441a/0.0.0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-98dd4ee92e213105\n",
      "2024-05-30 18:43:14 - INFO - datasets.builder - Using custom data configuration default-98dd4ee92e213105\n",
      "2024-05-30 18:43:14 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/generator\n",
      "Generating dataset generator (/root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0)\n",
      "2024-05-30 18:43:14 - INFO - datasets.builder - Generating dataset generator (/root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0)\n",
      "Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0...\n",
      "2024-05-30 18:43:14 - INFO - datasets.builder - Downloading and preparing dataset generator/default to /root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0...\n",
      "Generating train split\n",
      "2024-05-30 18:43:14 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 1 examples [00:00,  2.70 examples/s]\n",
      "Generating train split: 79 examples [00:00, 197.06 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "2024-05-30 18:43:14 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0. Subsequent calls will reuse this data.\n",
      "2024-05-30 18:43:14 - INFO - datasets.builder - Dataset generator downloaded and prepared to /root/.cache/huggingface/datasets/generator/default-98dd4ee92e213105/0.0.0. Subsequent calls will reuse this data.\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:641] 2024-05-30 18:43:14,748 >> Using auto half precision backend\n",
      "[INFO|trainer.py:641] 2024-05-30 18:43:14,748 >> Using auto half precision backend\n",
      "2024-05-30 18:43:14 - INFO - __main__ - *** Train ***\n",
      "[INFO|trainer.py:2078] 2024-05-30 18:43:15,308 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-30 18:43:15,308 >>   Num examples = 719\n",
      "[INFO|trainer.py:2078] 2024-05-30 18:43:15,308 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-30 18:43:15,308 >>   Num examples = 719\n",
      "[INFO|trainer.py:2080] 2024-05-30 18:43:15,308 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2080] 2024-05-30 18:43:15,308 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2081] 2024-05-30 18:43:15,308 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-05-30 18:43:15,308 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2085] 2024-05-30 18:43:15,308 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2081] 2024-05-30 18:43:15,308 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-05-30 18:43:15,308 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2085] 2024-05-30 18:43:15,308 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2086] 2024-05-30 18:43:15,308 >>   Total optimization steps = 45\n",
      "[INFO|trainer.py:2086] 2024-05-30 18:43:15,308 >>   Total optimization steps = 45\n",
      "[INFO|trainer.py:2087] 2024-05-30 18:43:15,313 >>   Number of trainable parameters = 54,525,952\n",
      "[INFO|trainer.py:2087] 2024-05-30 18:43:15,313 >>   Number of trainable parameters = 54,525,952\n",
      "[INFO|trainer.py:2078] 2024-05-30 18:43:15,321 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-30 18:43:15,321 >>   Num examples = 719\n",
      "[INFO|trainer.py:2078] 2024-05-30 18:43:15,321 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-30 18:43:15,321 >>   Num examples = 719\n",
      "[INFO|trainer.py:2080] 2024-05-30 18:43:15,321 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2080] 2024-05-30 18:43:15,321 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2081] 2024-05-30 18:43:15,321 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-05-30 18:43:15,322 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2085] 2024-05-30 18:43:15,322 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2086] 2024-05-30 18:43:15,322 >>   Total optimization steps = 45\n",
      "[INFO|trainer.py:2081] 2024-05-30 18:43:15,321 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-05-30 18:43:15,322 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2085] 2024-05-30 18:43:15,322 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2086] 2024-05-30 18:43:15,322 >>   Total optimization steps = 45\n",
      "[INFO|trainer.py:2087] 2024-05-30 18:43:15,328 >>   Number of trainable parameters = 54,525,952\n",
      "[INFO|trainer.py:2087] 2024-05-30 18:43:15,328 >>   Number of trainable parameters = 54,525,952\n",
      "0%|          | 0/45 [00:00<?, ?it/s]\n",
      "[WARNING|logging.py:329] 2024-05-30 18:43:21,833 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-30 18:43:21,833 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-30 18:43:21,890 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-30 18:43:21,890 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "2%|▏         | 1/45 [00:22<16:49, 22.95s/it]\n",
      "{'loss': 1.2045, 'grad_norm': 0.1435546875, 'learning_rate': 1.9975640502598243e-05, 'epoch': 0.02}\n",
      "2%|▏         | 1/45 [00:22<16:49, 22.95s/it]\n",
      "[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "4%|▍         | 2/45 [00:44<16:00, 22.35s/it]\n",
      "7%|▋         | 3/45 [01:06<15:33, 22.22s/it]\n",
      "9%|▉         | 4/45 [01:28<15:06, 22.11s/it]\n",
      "11%|█         | 5/45 [01:50<14:41, 22.05s/it]\n",
      "{'loss': 1.1121, 'grad_norm': 0.12890625, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.11}\n",
      "11%|█         | 5/45 [01:50<14:41, 22.05s/it]\n",
      "13%|█▎        | 6/45 [02:12<14:20, 22.06s/it]\n",
      "16%|█▌        | 7/45 [02:34<13:56, 22.02s/it]\n",
      "18%|█▊        | 8/45 [02:56<13:35, 22.04s/it]\n",
      "20%|██        | 9/45 [03:18<13:12, 22.01s/it]\n",
      "22%|██▏       | 10/45 [03:40<12:49, 21.98s/it]\n",
      "{'loss': 1.168, 'grad_norm': 0.12890625, 'learning_rate': 1.766044443118978e-05, 'epoch': 0.22}\n",
      "22%|██▏       | 10/45 [03:40<12:49, 21.98s/it]\n",
      "24%|██▍       | 11/45 [04:02<12:28, 22.01s/it]\n",
      "27%|██▋       | 12/45 [04:24<12:05, 21.99s/it]\n",
      "29%|██▉       | 13/45 [04:46<11:44, 22.02s/it]\n",
      "31%|███       | 14/45 [05:08<11:21, 21.99s/it]\n",
      "33%|███▎      | 15/45 [05:30<10:59, 21.97s/it]\n",
      "{'loss': 1.1264, 'grad_norm': 0.10693359375, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.33}\n",
      "33%|███▎      | 15/45 [05:30<10:59, 21.97s/it]\n",
      "36%|███▌      | 16/45 [05:52<10:38, 22.00s/it]\n",
      "38%|███▊      | 17/45 [06:14<10:15, 21.98s/it]\n",
      "40%|████      | 18/45 [06:36<09:54, 22.01s/it]\n",
      "42%|████▏     | 19/45 [06:58<09:31, 21.99s/it]\n",
      "44%|████▍     | 20/45 [07:20<09:09, 21.97s/it]\n",
      "{'loss': 1.1462, 'grad_norm': 0.099609375, 'learning_rate': 1.1736481776669307e-05, 'epoch': 0.44}\n",
      "44%|████▍     | 20/45 [07:20<09:09, 21.97s/it]\n",
      "47%|████▋     | 21/45 [07:42<08:48, 22.00s/it]\n",
      "49%|████▉     | 22/45 [08:04<08:25, 21.98s/it]\n",
      "51%|█████     | 23/45 [08:26<08:04, 22.01s/it]\n",
      "53%|█████▎    | 24/45 [08:48<07:41, 21.99s/it]\n",
      "56%|█████▌    | 25/45 [09:10<07:19, 21.97s/it]\n",
      "{'loss': 1.1036, 'grad_norm': 0.10009765625, 'learning_rate': 8.263518223330698e-06, 'epoch': 0.56}\n",
      "56%|█████▌    | 25/45 [09:10<07:19, 21.97s/it]\n",
      "58%|█████▊    | 26/45 [09:32<06:58, 22.00s/it]\n",
      "60%|██████    | 27/45 [09:54<06:35, 21.98s/it]\n",
      "62%|██████▏   | 28/45 [10:16<06:14, 22.01s/it]\n",
      "64%|██████▍   | 29/45 [10:38<05:51, 21.99s/it]\n",
      "67%|██████▋   | 30/45 [11:00<05:30, 22.02s/it]\n",
      "{'loss': 1.1518, 'grad_norm': 0.0986328125, 'learning_rate': 5.000000000000003e-06, 'epoch': 0.67}\n",
      "67%|██████▋   | 30/45 [11:00<05:30, 22.02s/it]\n",
      "69%|██████▉   | 31/45 [11:22<05:07, 22.00s/it]\n",
      "71%|███████   | 32/45 [11:44<04:45, 21.98s/it]\n",
      "73%|███████▎  | 33/45 [12:06<04:24, 22.01s/it]\n",
      "76%|███████▌  | 34/45 [12:28<04:01, 21.98s/it]\n",
      "78%|███████▊  | 35/45 [12:50<03:39, 21.97s/it]\n",
      "{'loss': 1.115, 'grad_norm': 0.0947265625, 'learning_rate': 2.339555568810221e-06, 'epoch': 0.78}\n",
      "78%|███████▊  | 35/45 [12:50<03:39, 21.97s/it]\n",
      "80%|████████  | 36/45 [13:12<03:18, 22.00s/it]\n",
      "82%|████████▏ | 37/45 [13:34<02:55, 21.98s/it]\n",
      "84%|████████▍ | 38/45 [13:56<02:34, 22.01s/it]\n",
      "87%|████████▋ | 39/45 [14:18<02:11, 21.99s/it]\n",
      "89%|████████▉ | 40/45 [14:40<01:49, 21.97s/it]\n",
      "{'loss': 1.0702, 'grad_norm': 0.1005859375, 'learning_rate': 6.030737921409169e-07, 'epoch': 0.89}\n",
      "89%|████████▉ | 40/45 [14:40<01:49, 21.97s/it]\n",
      "91%|█████████ | 41/45 [15:02<01:28, 22.01s/it]\n",
      "93%|█████████▎| 42/45 [15:24<01:05, 21.98s/it]\n",
      "96%|█████████▌| 43/45 [15:46<00:44, 22.01s/it]\n",
      "98%|█████████▊| 44/45 [16:08<00:21, 21.99s/it]\n",
      "100%|██████████| 45/45 [16:30<00:00, 22.02s/it]\n",
      "{'loss': 1.1236, 'grad_norm': 0.0927734375, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "100%|██████████| 45/45 [16:30<00:00, 22.02s/it]\n",
      "[INFO|trainer.py:3719] 2024-05-30 18:59:51,041 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3719] 2024-05-30 18:59:51,041 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-30 18:59:51,041 >>   Num examples = 79\n",
      "[INFO|trainer.py:3724] 2024-05-30 18:59:51,041 >>   Batch size = 4\n",
      "[INFO|trainer.py:3721] 2024-05-30 18:59:51,041 >>   Num examples = 79\n",
      "[INFO|trainer.py:3724] 2024-05-30 18:59:51,041 >>   Batch size = 4\n",
      "[INFO|trainer.py:3719] 2024-05-30 18:59:51,040 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-30 18:59:51,040 >>   Num examples = 79\n",
      "[INFO|trainer.py:3724] 2024-05-30 18:59:51,040 >>   Batch size = 4\n",
      "[INFO|trainer.py:3719] 2024-05-30 18:59:51,040 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-30 18:59:51,040 >>   Num examples = 79\n",
      "[INFO|trainer.py:3724] 2024-05-30 18:59:51,040 >>   Batch size = 4\n",
      "0%|          | 0/10 [00:00<?, ?it/s]#033[A\n",
      "20%|██        | 2/10 [00:02<00:11,  1.48s/it]#033[A\n",
      "30%|███       | 3/10 [00:05<00:14,  2.09s/it]#033[A\n",
      "40%|████      | 4/10 [00:08<00:14,  2.41s/it]#033[A\n",
      "50%|█████     | 5/10 [00:11<00:13,  2.60s/it]#033[A\n",
      "60%|██████    | 6/10 [00:14<00:10,  2.72s/it]#033[A\n",
      "70%|███████   | 7/10 [00:17<00:08,  2.79s/it]#033[A\n",
      "80%|████████  | 8/10 [00:20<00:05,  2.84s/it]#033[A\n",
      "90%|█████████ | 9/10 [00:23<00:02,  2.88s/it]#033[A\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.90s/it]#033[A\n",
      "#033[A\n",
      "{'eval_loss': 1.1381494998931885, 'eval_runtime': 29.5488, 'eval_samples_per_second': 2.674, 'eval_steps_per_second': 0.338, 'epoch': 1.0}\n",
      "100%|██████████| 45/45 [17:00<00:00, 22.02s/it]\n",
      "#015100%|██████████| 10/10 [00:26<00:00,  2.90s/it]#033[A\n",
      "#033[A\n",
      "[INFO|trainer.py:2329] 2024-05-30 19:00:20,590 >> \n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "[INFO|trainer.py:2329] 2024-05-30 19:00:20,590 >> \n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "{'train_runtime': 1025.2764, 'train_samples_per_second': 0.701, 'train_steps_per_second': 0.044, 'train_loss': 1.1261554612053766, 'epoch': 1.0}\n",
      "100%|██████████| 45/45 [17:00<00:00, 22.02s/it]\n",
      "100%|██████████| 45/45 [17:00<00:00, 22.67s/it]\n",
      "***** train metrics *****\n",
      "epoch                    =        1.0\n",
      "  total_flos               = 59039315GF\n",
      "  train_loss               =     1.1262\n",
      "  train_runtime            = 0:17:05.27\n",
      "  train_samples            =       1080\n",
      "  train_samples_per_second =      0.701\n",
      "  train_steps_per_second   =      0.044\n",
      "2024-05-30 19:00:20 - INFO - __main__ - *** Save model ***\n",
      "[INFO|trainer.py:3410] 2024-05-30 19:00:20,594 >> Saving model checkpoint to /opt/ml/model\n",
      "[INFO|trainer.py:3410] 2024-05-30 19:00:20,594 >> Saving model checkpoint to /opt/ml/model\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /opt/ml/input/data/model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-05-30 19:00:20,814 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-05-30 19:00:20,814 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-05-30 19:00:20,814 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-05-30 19:00:20,814 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "2024-05-30 19:00:20 - INFO - __main__ - Model saved to /opt/ml/model\n",
      "[INFO|modelcard.py:450] 2024-05-30 19:00:20,843 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'dataset': {'name': '/opt/ml/input/data/train', 'type': '/opt/ml/input/data/train', 'config': 'default', 'split': 'train', 'args': 'default'}}\n",
      "[INFO|modelcard.py:450] 2024-05-30 19:00:20,843 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'dataset': {'name': '/opt/ml/input/data/train', 'type': '/opt/ml/input/data/train', 'config': 'default', 'split': 'train', 'args': 'default'}}\n",
      "[INFO|configuration_utils.py:472] 2024-05-30 19:00:20,848 >> Configuration saved in /opt/ml/model/config.json\n",
      "[INFO|configuration_utils.py:472] 2024-05-30 19:00:20,848 >> Configuration saved in /opt/ml/model/config.json\n",
      "2024-05-30 19:00:20 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3719] 2024-05-30 19:00:20,851 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3719] 2024-05-30 19:00:20,851 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-30 19:00:20,851 >>   Num examples = 79\n",
      "[INFO|trainer.py:3724] 2024-05-30 19:00:20,851 >>   Batch size = 4\n",
      "[INFO|trainer.py:3721] 2024-05-30 19:00:20,851 >>   Num examples = 79\n",
      "[INFO|trainer.py:3724] 2024-05-30 19:00:20,851 >>   Batch size = 4\n",
      "[INFO|trainer.py:2329] 2024-05-30 19:00:20,589 >> \n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "[INFO|trainer.py:2329] 2024-05-30 19:00:20,589 >> \n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "2024-05-30 19:00:20 - INFO - __main__ - *** Save model ***\n",
      "2024-05-30 19:00:20 - INFO - __main__ - Model saved to /opt/ml/model\n",
      "2024-05-30 19:00:20 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3719] 2024-05-30 19:00:20,594 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3719] 2024-05-30 19:00:20,594 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-30 19:00:20,594 >>   Num examples = 79\n",
      "[INFO|trainer.py:3724] 2024-05-30 19:00:20,594 >>   Batch size = 4\n",
      "[INFO|trainer.py:3721] 2024-05-30 19:00:20,594 >>   Num examples = 79\n",
      "[INFO|trainer.py:3724] 2024-05-30 19:00:20,594 >>   Batch size = 4\n",
      "0%|          | 0/10 [00:00<?, ?it/s]\n",
      "20%|██        | 2/10 [00:02<00:11,  1.48s/it]\n",
      "30%|███       | 3/10 [00:05<00:14,  2.09s/it]\n",
      "40%|████      | 4/10 [00:08<00:14,  2.41s/it]\n",
      "50%|█████     | 5/10 [00:11<00:13,  2.60s/it]\n",
      "60%|██████    | 6/10 [00:14<00:10,  2.72s/it]\n",
      "70%|███████   | 7/10 [00:17<00:08,  2.79s/it]\n",
      "80%|████████  | 8/10 [00:20<00:05,  2.84s/it]\n",
      "90%|█████████ | 9/10 [00:23<00:02,  2.88s/it]\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.90s/it]\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.67s/it]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     1.1381\n",
      "  eval_runtime            = 0:00:29.54\n",
      "  eval_samples            =        120\n",
      "  eval_samples_per_second =      2.674\n",
      "  eval_steps_per_second   =      0.338\n",
      "2024-05-30 19:00:50 - INFO - __main__ - *** Training complete ***\n",
      "2024-05-30 19:00:50 - INFO - __main__ - *** Training complete ***\n",
      "2024-05-30 19:00:56,068 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-30 19:00:56,068 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-30 19:00:56,068 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "2024-05-30 19:00:56,060 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-30 19:00:56,060 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-30 19:00:56,060 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2024-05-30 19:01:15 Uploading - Uploading generated training model\n",
      "2024-05-30 19:01:15 Completed - Resource retained for reuse\n",
      "Training seconds: 3090\n",
      "Billable seconds: 3090\n"
     ]
    }
   ],
   "source": [
    "# Invoking the fit method on the estimator starts the training job\n",
    "# data will be copied into training cluster based on the dictionary keys specified here\n",
    "# The contents of the s3_model_location will be copied into the /opt/ml/input/data/model directory\n",
    "# The contents of the s3_data will be copied into the /opt/ml/input/data/train directory\n",
    "sft_estimator.fit({\"model\": s3_model_location, \"train\": f\"{s3_data}/sft_split\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure SageMaker Training Job for Direct Preference Optimization\n",
    "Once we have the fine-tuned model, we can further fine-tune it using Direct Preference Optimization to better align with our preferences and improve the model's outputs. The code process is similar to the supervised fine-tuning job. Except now we will use the `alignment-handbook/scripts/run_dpo.py` script and also provide our fine-tuned model as an additional input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "tb_output_config = TensorBoardOutputConfig(s3_output_path=f\"s3://{bucket}/fine-tuning-mistral/tensorboard/{str_time}\",\n",
    "    container_local_output_path=\"/opt/ml/output/tensorboard\")\n",
    "\n",
    "job_name = f\"mistral7b-dpo\"\n",
    "\n",
    "# hyperparameters = {\n",
    "#     \"recipe\": \"config_dpo_lora_fsdp.yaml\",\n",
    "#     \"torch_dtype\": \"bfloat16\",\n",
    "#     \"bnb_4bit_quant_storage\": \"bfloat16\"\n",
    "# }\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"recipe\": \"config_dpo_lora.yaml\",\n",
    "}\n",
    "\n",
    "dpo_estimator = PyTorch(\n",
    "    base_job_name=job_name,\n",
    "    source_dir = \"src\",\n",
    "    entry_point=\"alignment-handbook/scripts/run_dpo.py\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    instance_count=2, \n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g5.2xlarge\", \n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    max_run=60*60*24*2,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    tensorboard_output_config=tb_output_config,\n",
    "    environment = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \n",
    "                    \"LIBRARY_PATH\": \"/opt/conda/lib/\",\n",
    "                    \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                    \"NCCL_P2P_LEVEL\": \"NVL\"},\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    disable_output_compression = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the location of the fine tuned model from the sft_estimator\n",
    "# sft_model_location = sft_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]\n",
    "\n",
    "sft_model_location = \"s3://sagemaker-us-east-1-152804913371/mistral7b-sft-2024-05-30-18-35-20-560/output/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since LoRA was used for fine-tuning, or SFT model will only contain the LoRA adapter. Therefore we also provide the base model as another input into the `.fit` call below. The training script will automatically merge the base model with the LoRA adapter and then proceed with the DPO fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: mistral7b-dpo-2024-05-31-16-17-48-875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 16:17:49 Starting - Starting the training job\n",
      "2024-05-31 16:17:49 Pending - Training job waiting for capacity......\n",
      "2024-05-31 16:18:33 Pending - Preparing the instances for training...\n",
      "2024-05-31 16:19:06 Downloading - Downloading input data....................................\n",
      "2024-05-31 16:25:23 Downloading - Downloading the training image.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-05-31 16:25:26,281 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-05-31 16:25:26,298 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-31 16:25:26,310 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-05-31 16:25:26,317 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2024-05-31 16:25:26,317 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-05-31 16:25:28,600 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Obtaining file:///opt/ml/code/alignment-handbook (from -r requirements.txt (line 2))\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting flash-attn==2.3.6 (from -r requirements.txt (line 1))\n",
      "Downloading flash_attn-2.3.6.tar.gz (2.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 65.0 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-05-31 16:25:26,394 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-05-31 16:25:26,411 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-31 16:25:26,423 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-05-31 16:25:26,432 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2024-05-31 16:25:26,432 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-05-31 16:25:28,754 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Obtaining file:///opt/ml/code/alignment-handbook (from -r requirements.txt (line 2))\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting flash-attn==2.3.6 (from -r requirements.txt (line 1))\n",
      "Downloading flash_attn-2.3.6.tar.gz (2.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 54.0 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (1.11.1.1)\n",
      "Collecting accelerate>=0.29.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes>=0.43.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting evaluate==0.4.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.3.6->-r requirements.txt (line 1)) (1.11.1.1)\n",
      "Collecting accelerate>=0.29.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting bitsandbytes>=0.43.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting evaluate==0.4.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting datasets>=2.18.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deepspeed==0.12.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading deepspeed-0.12.2.tar.gz (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 68.2 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Collecting datasets>=2.18.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deepspeed==0.12.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading deepspeed-0.12.2.tar.gz (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 54.3 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting hf_transfer>=0.1.4 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting hf_transfer>=0.1.4 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting peft>=0.9.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting peft>=0.9.0 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf<=3.20.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting safetensors>=0.3.3 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece>=0.1.99 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.12.0)\n",
      "Collecting tensorboard (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (4.66.4)\n",
      "Collecting transformers>=4.39.3 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting trl>=0.8.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hjson (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (5.9.8)\n",
      "Collecting py-cpuinfo (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.6.3)\n",
      "Collecting pynvml (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.32.3)\n",
      "Collecting xxhash (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.2.0)\n",
      "\n",
      "2024-05-31 16:25:23 Training - Training image download completed. Training in progress.Collecting protobuf<=3.20.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting safetensors>=0.3.3 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece>=0.1.99 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.12.0)\n",
      "Collecting tensorboard (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (4.66.4)\n",
      "Collecting transformers>=4.39.3 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.8/43.8 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting trl>=0.8.2 (from alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hjson (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (5.9.8)\n",
      "Collecting py-cpuinfo (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.6.3)\n",
      "Collecting pynvml (from deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.32.3)\n",
      "Collecting xxhash (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Collecting responses<0.19 (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.29.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting responses<0.19 (from evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.29.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting aiohttp (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (3.2.1)\n",
      "Collecting aiohttp (from datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (3.2.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.39.3->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (68.1.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.0.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (68.1.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.0.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.1)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.18.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2))\n",
      "Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.16.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.1.2)\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 10.4 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 30.1 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed==0.12.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.16.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.3.6->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.8.2->alignment-handbook==0.4.0.dev0->-r requirements.txt (line 2)) (0.1.2)\n",
      "Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 14.2 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.6/302.6 kB 37.4 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 43.7 MB/s eta 0:00:00\n",
      "Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 87.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 37.9 MB/s eta 0:00:00\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 30.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 64.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 66.7 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 69.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 99.4 MB/s eta 0:00:00\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 23.8 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 98.4 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 19.6 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 78.4 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 98.8 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 13.5 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 56.0 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 14.5 MB/s eta 0:00:00\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 42.7 MB/s eta 0:00:00\n",
      "Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 74.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.7/401.7 kB 40.3 MB/s eta 0:00:00\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 30.0 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 60.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 58.0 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 63.2 MB/s eta 0:00:00\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 86.0 MB/s eta 0:00:00\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 29.6 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 84.4 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 19.5 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 62.3 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 86.6 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 13.8 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 51.7 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 82.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 96.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 87.2 MB/s eta 0:00:00\n",
      "Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.4/102.4 kB 17.8 MB/s eta 0:00:00\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 25.4 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 28.6 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 20.0 MB/s eta 0:00:00\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 35.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: flash-attn, deepspeed\n",
      "Building wheel for flash-attn (setup.py): started\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 83.6 MB/s eta 0:00:00\n",
      "Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.4/102.4 kB 15.2 MB/s eta 0:00:00\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 8.7 MB/s eta 0:00:00\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 26.5 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 29.9 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 19.7 MB/s eta 0:00:00\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 34.4 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: flash-attn, deepspeed\n",
      "Building wheel for flash-attn (setup.py): started\n",
      "Building wheel for flash-attn (setup.py): finished with status 'done'\n",
      "Building wheel for flash-attn (setup.py): finished with status 'done'\n",
      "Created wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56477261 sha256=652ad256d0891cb2c6d7183f96f7f56ff61cdeee24388381abb35e7a0f2eeca1\n",
      "Stored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\n",
      "Building wheel for deepspeed (setup.py): started\n",
      "Created wheel for flash-attn: filename=flash_attn-2.3.6-cp310-cp310-linux_x86_64.whl size=56477261 sha256=652ad256d0891cb2c6d7183f96f7f56ff61cdeee24388381abb35e7a0f2eeca1\n",
      "Stored in directory: /root/.cache/pip/wheels/24/5f/16/5044cdddb6dfb3331dfbffa28ab6096ec2900777af5cb0253a\n",
      "Building wheel for deepspeed (setup.py): started\n",
      "Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "Created wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265672 sha256=d190bd3461975d8a519c80f6fc3a07fb2f24fa380f048cfb9107a29593e8a408\n",
      "Stored in directory: /root/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\n",
      "Successfully built flash-attn deepspeed\n",
      "Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "Created wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265669 sha256=e07212172fff172a5bd65d93d6f152e777ea0bc8c57d37e093bb6e0596e76be9\n",
      "Stored in directory: /root/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\n",
      "Successfully built flash-attn deepspeed\n",
      "Installing collected packages: sentencepiece, py-cpuinfo, hjson, xxhash, tensorboard-data-server, shtab, safetensors, regex, pynvml, pyarrow-hotfix, protobuf, multidict, markdown, hf_transfer, grpcio, frozenlist, docstring-parser, async-timeout, absl-py, yarl, tensorboard, responses, huggingface-hub, aiosignal, tyro, tokenizers, flash-attn, deepspeed, bitsandbytes, aiohttp, accelerate, transformers, peft, datasets, trl, evaluate, alignment-handbook\n",
      "Installing collected packages: sentencepiece, py-cpuinfo, hjson, xxhash, tensorboard-data-server, shtab, safetensors, regex, pynvml, pyarrow-hotfix, protobuf, multidict, markdown, hf_transfer, grpcio, frozenlist, docstring-parser, async-timeout, absl-py, yarl, tensorboard, responses, huggingface-hub, aiosignal, tyro, tokenizers, flash-attn, deepspeed, bitsandbytes, aiohttp, accelerate, transformers, peft, datasets, trl, evaluate, alignment-handbook\n",
      "Attempting uninstall: protobuf\n",
      "Found existing installation: protobuf 3.20.3\n",
      "Uninstalling protobuf-3.20.3:\n",
      "Attempting uninstall: protobuf\n",
      "Found existing installation: protobuf 3.20.3\n",
      "Uninstalling protobuf-3.20.3:\n",
      "Successfully uninstalled protobuf-3.20.3\n",
      "Successfully uninstalled protobuf-3.20.3\n",
      "Attempting uninstall: flash-attn\n",
      "Found existing installation: flash-attn 2.0.4\n",
      "Attempting uninstall: flash-attn\n",
      "Found existing installation: flash-attn 2.0.4\n",
      "Uninstalling flash-attn-2.0.4:\n",
      "Uninstalling flash-attn-2.0.4:\n",
      "Successfully uninstalled flash-attn-2.0.4\n",
      "Successfully uninstalled flash-attn-2.0.4\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.22.0\n",
      "Uninstalling accelerate-0.22.0:\n",
      "Successfully uninstalled accelerate-0.22.0\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.22.0\n",
      "Uninstalling accelerate-0.22.0:\n",
      "Successfully uninstalled accelerate-0.22.0\n",
      "Running setup.py develop for alignment-handbook\n",
      "Running setup.py develop for alignment-handbook\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.3.6 which is incompatible.\n",
      "Successfully installed absl-py-2.1.0 accelerate-0.30.1 aiohttp-3.9.5 aiosignal-1.3.1 alignment-handbook-0.4.0.dev0 async-timeout-4.0.3 bitsandbytes-0.43.1 datasets-2.19.1 deepspeed-0.12.2 docstring-parser-0.16 evaluate-0.4.0 flash-attn-2.3.6 frozenlist-1.4.1 grpcio-1.64.0 hf_transfer-0.1.6 hjson-3.1.0 huggingface-hub-0.23.2 markdown-3.6 multidict-6.0.5 peft-0.11.1 protobuf-3.20.2 py-cpuinfo-9.0.0 pyarrow-hotfix-0.6 pynvml-11.5.0 regex-2024.5.15 responses-0.18.0 safetensors-0.4.3 sentencepiece-0.2.0 shtab-1.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.19.1 transformers-4.41.2 trl-0.8.6 tyro-0.8.4 xxhash-3.4.1 yarl-1.9.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.3.6 which is incompatible.\n",
      "Successfully installed absl-py-2.1.0 accelerate-0.30.1 aiohttp-3.9.5 aiosignal-1.3.1 alignment-handbook-0.4.0.dev0 async-timeout-4.0.3 bitsandbytes-0.43.1 datasets-2.19.1 deepspeed-0.12.2 docstring-parser-0.16 evaluate-0.4.0 flash-attn-2.3.6 frozenlist-1.4.1 grpcio-1.64.0 hf_transfer-0.1.6 hjson-3.1.0 huggingface-hub-0.23.2 markdown-3.6 multidict-6.0.5 peft-0.11.1 protobuf-3.20.2 py-cpuinfo-9.0.0 pyarrow-hotfix-0.6 pynvml-11.5.0 regex-2024.5.15 responses-0.18.0 safetensors-0.4.3 sentencepiece-0.2.0 shtab-1.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tokenizers-0.19.1 transformers-4.41.2 trl-0.8.6 tyro-0.8.4 xxhash-3.4.1 yarl-1.9.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "2024-05-31 16:26:07,969 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-31 16:26:07,969 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-31 16:26:08,005 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-31 16:26:08,035 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-31 16:26:08,049 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2024-05-31 16:26:08,067 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-31 16:26:08,080 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"sft_model\": \"/opt/ml/input/data/sft_model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"recipe\": \"config_dpo_lora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"sft_model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"mistral7b-dpo-2024-05-31-16-17-48-875\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-2\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-152804913371/mistral7b-dpo-2024-05-31-16-17-48-875/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"alignment-handbook/scripts/run_dpo\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"alignment-handbook/scripts/run_dpo.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"recipe\":\"config_dpo_lora.yaml\"}\n",
      "SM_USER_ENTRY_POINT=alignment-handbook/scripts/run_dpo.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"sft_model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"model\",\"sft_model\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-2\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=alignment-handbook/scripts/run_dpo\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-152804913371/mistral7b-dpo-2024-05-31-16-17-48-875/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"sft_model\":\"/opt/ml/input/data/sft_model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"recipe\":\"config_dpo_lora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"sft_model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"mistral7b-dpo-2024-05-31-16-17-48-875\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-152804913371/mistral7b-dpo-2024-05-31-16-17-48-875/source/sourcedir.tar.gz\",\"module_name\":\"alignment-handbook/scripts/run_dpo\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"alignment-handbook/scripts/run_dpo.py\"}\n",
      "SM_USER_ARGS=[\"--recipe\",\"config_dpo_lora.yaml\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_MODEL=/opt/ml/input/data/model\n",
      "SM_CHANNEL_SFT_MODEL=/opt/ml/input/data/sft_model\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_RECIPE=config_dpo_lora.yaml\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-2 --master_port 7777 --node_rank 0 alignment-handbook/scripts/run_dpo.py --recipe config_dpo_lora.yaml\n",
      "2024-05-31 16:26:08,174 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-31 16:26:08,174 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-31 16:26:08,213 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-31 16:26:08,244 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-31 16:26:08,257 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2024-05-31 16:26:08,275 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-31 16:26:08,287 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"sft_model\": \"/opt/ml/input/data/sft_model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"recipe\": \"config_dpo_lora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"sft_model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"mistral7b-dpo-2024-05-31-16-17-48-875\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-2\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-152804913371/mistral7b-dpo-2024-05-31-16-17-48-875/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"alignment-handbook/scripts/run_dpo\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"alignment-handbook/scripts/run_dpo.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"recipe\":\"config_dpo_lora.yaml\"}\n",
      "SM_USER_ENTRY_POINT=alignment-handbook/scripts/run_dpo.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"sft_model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"model\",\"sft_model\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=alignment-handbook/scripts/run_dpo\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-152804913371/mistral7b-dpo-2024-05-31-16-17-48-875/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"sft_model\":\"/opt/ml/input/data/sft_model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"recipe\":\"config_dpo_lora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"sft_model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"mistral7b-dpo-2024-05-31-16-17-48-875\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-152804913371/mistral7b-dpo-2024-05-31-16-17-48-875/source/sourcedir.tar.gz\",\"module_name\":\"alignment-handbook/scripts/run_dpo\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"alignment-handbook/scripts/run_dpo.py\"}\n",
      "SM_USER_ARGS=[\"--recipe\",\"config_dpo_lora.yaml\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_MODEL=/opt/ml/input/data/model\n",
      "SM_CHANNEL_SFT_MODEL=/opt/ml/input/data/sft_model\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_RECIPE=config_dpo_lora.yaml\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-2 --master_port 7777 --node_rank 1 alignment-handbook/scripts/run_dpo.py --recipe config_dpo_lora.yaml\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "Downloading readme:   0%|          | 0.00/6.52k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 6.52k/6.52k [00:00<00:00, 43.9MB/s]\n",
      "Downloading readme:   0%|          | 0.00/6.52k [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 6.52k/6.52k [00:00<00:00, 42.1MB/s]\n",
      "Downloading data:   0%|          | 0.00/83.9k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 83.9k/83.9k [00:00<00:00, 1.56MB/s]\n",
      "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]\n",
      "Generating test split: 100%|██████████| 164/164 [00:00<00:00, 40685.27 examples/s]\n",
      "Downloading data:   0%|          | 0.00/83.9k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 83.9k/83.9k [00:00<00:00, 2.16MB/s]\n",
      "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]\n",
      "Generating test split: 100%|██████████| 164/164 [00:00<00:00, 45076.40 examples/s]\n",
      "[2024-05-31 16:26:21,632] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "2024-05-31 16:26:22 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='/opt/ml/input/data/sft_model', model_revision='main', model_code_revision=None, torch_dtype='auto', tokenizer_name_or_path=None, trust_remote_code=False, use_flash_attention_2=True, use_peft=True, lora_r=32, lora_alpha=16, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False, bnb_4bit_quant_storage='uint8')\n",
      "2024-05-31 16:26:22 - INFO - __main__ - Data parameters DataArguments(chat_template=None, dataset_mixer={'/opt/ml/input/data/train': 1.0}, text_column='text', dataset_splits=['train', 'test'], dataset_configs=None, preprocessing_num_workers=2, truncation_side=None, auto_insert_empty_system_msg=True)\n",
      "2024-05-31 16:26:22 - INFO - __main__ - Training/evaluation parameters DPOConfig(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "beta=0.1,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=50,\n",
      "eval_strategy=epoch,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_model_revision=main,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-07,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/opt/ml/output/tensorboard,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "loss_type=sigmoid,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_length=1536,\n",
      "max_prompt_length=768,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=paged_adamw_32bit,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/opt/ml/model,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/opt/ml/model,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "2024-05-31 16:26:22 - INFO - __main__ - Training on the following splits: ['train : 1080', 'test : 120']\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,049 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,049 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,049 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,049 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,050 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,050 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,050 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,050 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,050 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,050 >> loading file tokenizer_config.json\n",
      "Formatting comparisons with prompt template (num_proc=2):   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2):   6%|▌         | 65/1080 [00:00<00:02, 348.65 examples/s]\n",
      "[2024-05-31 16:26:22,206] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "2024-05-31 16:26:22 - INFO - __main__ - Model parameters ModelArguments(base_model_revision=None, model_name_or_path='/opt/ml/input/data/sft_model', model_revision='main', model_code_revision=None, torch_dtype='auto', tokenizer_name_or_path=None, trust_remote_code=False, use_flash_attention_2=True, use_peft=True, lora_r=32, lora_alpha=16, lora_dropout=0.1, lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_modules_to_save=None, load_in_8bit=False, load_in_4bit=True, bnb_4bit_quant_type='nf4', use_bnb_nested_quant=False, bnb_4bit_quant_storage='uint8')\n",
      "2024-05-31 16:26:22 - INFO - __main__ - Data parameters DataArguments(chat_template=None, dataset_mixer={'/opt/ml/input/data/train': 1.0}, text_column='text', dataset_splits=['train', 'test'], dataset_configs=None, preprocessing_num_workers=2, truncation_side=None, auto_insert_empty_system_msg=True)\n",
      "2024-05-31 16:26:22 - INFO - __main__ - Training/evaluation parameters DPOConfig(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "beta=0.1,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=50,\n",
      "eval_strategy=epoch,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_model_revision=main,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-07,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/opt/ml/output/tensorboard,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "loss_type=sigmoid,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_length=1536,\n",
      "max_prompt_length=768,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=paged_adamw_32bit,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/opt/ml/model,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/opt/ml/model,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "2024-05-31 16:26:22 - INFO - __main__ - Training on the following splits: ['train : 1080', 'test : 120']\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-05-31 16:26:22,613 >> loading file tokenizer_config.json\n",
      "Formatting comparisons with prompt template (num_proc=2):   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2):  53%|█████▎    | 577/1080 [00:00<00:00, 2102.70 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2): 100%|█████████▉| 1077/1080 [00:00<00:00, 3111.07 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2): 100%|██████████| 1080/1080 [00:00<00:00, 2152.38 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2):   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2):  50%|█████     | 60/120 [00:00<00:00, 349.85 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2): 100%|██████████| 120/120 [00:00<00:00, 439.44 examples/s]\n",
      "Decontaminating HumanEval samples:   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2):   5%|▌         | 58/1080 [00:00<00:03, 297.89 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2):  54%|█████▍    | 585/1080 [00:00<00:00, 2121.03 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2): 100%|██████████| 1080/1080 [00:00<00:00, 3027.62 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2): 100%|██████████| 1080/1080 [00:00<00:00, 2143.98 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2):   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2):  50%|█████     | 60/120 [00:00<00:00, 353.79 examples/s]\n",
      "Formatting comparisons with prompt template (num_proc=2): 100%|██████████| 120/120 [00:00<00:00, 442.26 examples/s]\n",
      "Decontaminating HumanEval samples:   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Decontaminating HumanEval samples: 100%|██████████| 1080/1080 [00:01<00:00, 789.71 examples/s]\n",
      "Decontaminating HumanEval samples: 100%|██████████| 1080/1080 [00:01<00:00, 788.98 examples/s]\n",
      "Decontaminating HumanEval samples:   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Decontaminating HumanEval samples: 100%|██████████| 120/120 [00:00<00:00, 784.73 examples/s]\n",
      "Decontaminating HumanEval samples: 100%|██████████| 120/120 [00:00<00:00, 778.96 examples/s]\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Decontaminated 0 (0.00%) samples from the training set.\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Prompt sample 228 of the raw training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "This pure white bouquet of Casa Blanca lilies makes an elegant statement as a perfect floral tribute, centerpiece or decorative arrangement. These lovely lilies also leave a soft fragrance in the air. Show loved ones that you care with fresh flowers from Texis Flower Shop.\n",
      "Vibrant tulips spring forth from a uniquely innovative garden of daisies, mini carnations and solidago arranged in a playful bird's nest basket. Based on the passage above, Can you provide more information about Casa Blanca lilies, such as their origin and symbolism?</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Chosen sample 228 of the raw training set:\n",
      "<|assistant|>\n",
      "Casa Blanca lilies, also known as Lilium 'Casa Blanca', are a popular variety of white lilies that originated in the United States. The hybrid was first developed by Dr. Cecil P. Henson in 1968 and introduced to the market by the Armstrong Nursery in California. The plant's exceptional qualities, such as its large, fragrant white flowers and strong stems, quickly made it a favorite among gardeners and florists.\n",
      "Casa Blanca lilies are known for their large, pure white, trumpet-shaped flowers, which can grow up to 8 inches in diameter. The flowers are often described as having a creamy white or ivory color and sometimes feature subtle yellow or pinkish streaks near the base of the petals. The lily's stamens are a rich yellow, adding a warm contrast to the pure white petals.\n",
      "In terms of symbolism, white lilies in general are often associated with purity, innocence, and sympathy. They are commonly used in funeral arrangements and memorial services to represent the departed's spiritual journey or to express sympathy to the bereaved. However, the specific symbolism of Casa Blanca lilies may vary depending on the culture or personal interpretation.</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Rejected sample 228 of the raw training set:\n",
      "<|assistant|>\n",
      "Casa Blanca lilies, also known as Lilium 'Casa Blanca' or 'White Lily', are a hybrid variety of the Oriental Lily group. They originate from the cold regions of Japan, where they were first cultivated in the late 20th century.\n",
      "Casa Blanca lilies are highly prized for their large, trumpet-shaped flowers, which are pure white, showy, and emit a sweet, captivating fragrance. They have a strong presence in wedding ceremonies, as well as other arrangements, for their elegance and simplicity.\n",
      "In terms of symbolism, white lilies, including Casa Blanca lilies, have a rich and deep meaning in various cultures. Traditionally, they are associated with purity, innocence, and spiritual enlightenment. In Christianity, the white lily is often considered a symbol of the Virgin Mary. It signifies her purity and chastity. In Greek mythology, the lily is associated with the goddess Hera, who created the lily when drops of her breast milk fell to the ground.\n",
      "Additionally, lilies are often included in funeral flower arrangements to symbolize the rebirth and renewal of the deceased's soul and to represent restored innocence after death. Casa Blanca lilies, with their pure white color and intense fragrance, evoke a sense of peace, love, and fond reminiscence, making them an ideal choice for various occasions and expressions of hope, sympathy, and adoration.</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Prompt sample 51 of the raw training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Rearrange the list of words to form a coherent sentence.\n",
      "film the interested viewer oldest</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Chosen sample 51 of the raw training set:\n",
      "<|assistant|>\n",
      "The oldest viewer is interested in film.</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Rejected sample 51 of the raw training set:\n",
      "<|assistant|>\n",
      "Oldest film the interested viewer.</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Prompt sample 563 of the raw training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "import tkinter as tk\n",
      "from tkinter import ttk\n",
      "class App:\n",
      " def \\_\\_init\\_\\_(self, master):\n",
      " self.master = master\n",
      " master.title(\"GUI Application\")\n",
      " \n",
      " # set the minimum window size\n",
      " master.minsize(width=400, height=300)\n",
      " \n",
      " # create a frame for the text widget\n",
      " text\\_frame = ttk.Frame(master, padding=10)\n",
      " text\\_frame.pack(expand=True, fill='both')\n",
      " \n",
      " # create the text widget and disable editing\n",
      " self.text\\_widget = tk.Text(text\\_frame, wrap='word', state='disabled')\n",
      " self.text\\_widget.pack(expand=True, fill='both')\n",
      " \n",
      " # create a frame for the buttons\n",
      " button\\_frame = ttk.Frame(master, padding=10)\n",
      " button\\_frame.pack(side='bottom', fill='x')\n",
      " \n",
      " # create the buttons\n",
      " load\\_data\\_button = ttk.Button(button\\_frame, text=\"Load Data\", command=self.load\\_data)\n",
      " data\\_analysis\\_button = ttk.Button(button\\_frame, text=\"Data Analysis\", command=self.data\\_analysis)\n",
      " generate\\_models\\_button = ttk.Button(button\\_frame, text=\"Generate Models\", command=self.generate\\_models)\n",
      " predict\\_data\\_button = ttk.Button(button\\_frame, text=\"Predict Data\", command=self.predict\\_data)\n",
      " model\\_validation\\_button = ttk.Button(button\\_frame, text=\"Model Validation\", command=self.model\\_validation)\n",
      " \n",
      " # grid the buttons\n",
      " load\\_data\\_button.grid(row=0, column=0, padx=5, pady=5, sticky='ew')\n",
      " data\\_analysis\\_button.grid(row=0, column=1, padx=5, pady=5, sticky='ew')\n",
      " generate\\_models\\_button.grid(row=0, column=2, padx=5, pady=5, sticky='ew')\n",
      " predict\\_data\\_button.grid(row=1, column=0, padx=5, pady=5, sticky='ew')\n",
      " model\\_validation\\_button.grid(row=1, column=1, padx=5, pady=5, sticky='ew')\n",
      " \n",
      " def load\\_data(self):\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", \"Load Data button clicked\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      " def data\\_analysis(self):\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", \"Data Analysis button clicked\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      " def generate\\_models(self):\n",
      " # create a new window\n",
      " new\\_window = tk.Toplevel(self.master)\n",
      " new\\_window.title(\"Generate Models\")\n",
      " new\\_window.geometry('250x150')\n",
      " \n",
      " # create a frame for the widgets\n",
      " frame = ttk.Frame(new\\_window, padding=10)\n",
      " frame.pack(expand=True, fill='both')\n",
      " \n",
      " # create the preprocess data checkbutton\n",
      " self.preprocess\\_data\\_var = tk.BooleanVar()\n",
      " preprocess\\_data\\_checkbutton = ttk.Checkbutton(frame, text=\"Preprocess Data\", variable=self.preprocess\\_data\\_var)\n",
      " preprocess\\_data\\_checkbutton.grid(row=0, column=0, padx=5, pady=5, sticky='w')\n",
      " \n",
      " # create the save test data checkbutton\n",
      " self.save\\_test\\_data\\_var = tk.BooleanVar()\n",
      " save\\_test\\_data\\_checkbutton = ttk.Checkbutton(frame, text=\"Split & Save Test Data\", variable=self.save\\_test\\_data\\_var, command=self.show\\_test\\_data\\_ratio)\n",
      " save\\_test\\_data\\_checkbutton.grid(row=1, column=0, padx=5, pady=5, sticky='w')\n",
      " \n",
      " # create the test data ratio label and entry field\n",
      " self.test\\_data\\_ratio\\_label = ttk.Label(frame, text=\"Test DataRatio (0-1):\")\n",
      " self.test\\_data\\_ratio\\_label.grid(row=2, column=0, padx=5, pady=5, sticky='w')\n",
      " self.test\\_data\\_ratio\\_entry = ttk.Entry(frame, state=\"disabled\")\n",
      " self.test\\_data\\_ratio\\_entry.grid(row=2, column=1, padx=5, pady=5, sticky='w')\n",
      " \n",
      " # create the generate models button\n",
      " generate\\_models\\_button = ttk.Button(frame, text=\"Generate Models\", command=self.generate\\_models\\_action)\n",
      " generate\\_models\\_button.grid(row=3, column=0, padx=5, pady=5, sticky='ew')\n",
      " \n",
      " # hide the test data ratio widgets by default\n",
      " self.hide\\_test\\_data\\_ratio()\n",
      " self.show\\_test\\_data\\_ratio()\n",
      " \n",
      " def show\\_test\\_data\\_ratio(self):\n",
      " if self.save\\_test\\_data\\_var.get():\n",
      " self.test\\_data\\_ratio\\_entry.config(state=\"normal\")\n",
      " else:\n",
      " self.test\\_data\\_ratio\\_entry.delete(0, \"end\")\n",
      " self.test\\_data\\_ratio\\_entry.config(state=\"disabled\")\n",
      " \n",
      " def hide\\_test\\_data\\_ratio(self):\n",
      " self.test\\_data\\_ratio\\_label.grid\\_remove()\n",
      " self.test\\_data\\_ratio\\_entry.grid\\_remove()\n",
      " \n",
      " def generate\\_models\\_action(self):\n",
      " # get the user input\n",
      " preprocess\\_data = self.preprocess\\_data\\_var.get()\n",
      " save\\_test\\_data = self.save\\_test\\_data\\_var.get()\n",
      " test\\_data\\_ratio = self.test\\_data\\_ratio\\_entry.get()\n",
      " \n",
      " # do something with the input\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", f\"Generate Models button clicked with preprocess\\_data={preprocess\\_data}, save\\_test\\_data={save\\_test\\_data}, test\\_data\\_ratio={test\\_data\\_ratio}\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      " # close the new window\n",
      " new\\_window.destroy()\n",
      " \n",
      " def predict\\_data(self):\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", \"Predict Data button clicked\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      " def model\\_validation(self):\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", \"Model Validation button clicked\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      "root = tk.Tk()\n",
      "app = App(root)\n",
      "root.mainloop()\n",
      "find problems in the code not working as expected</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Chosen sample 563 of the raw training set:\n",
      "<|assistant|>\n",
      "There are a few issues in the provided code that need to be addressed:\n",
      "1. In the `load_data` method, you are trying to insert the text \"Load Data button clicked\" into the text widget's insertion layer. However, the insertion layer is a readonly attribute, so you cannot modify it directly. To fix this, you can create a new text widget and insert the text into it instead.\n",
      "2. In the `data_analysis` method, you are trying to insert the text \"Data Analysis button clicked\" into the text widget's insertion layer. Similarly, you cannot modify the insertion layer directly.\n",
      "3. In the `generate_models` method, you are trying to create a new window and grid its contents. However, the `grid` method is not idempotent, meaning that it can only be called once per grid. To fix this, you can use the `pack` method instead of `grid` to add the widgets to the new window.\n",
      "4. In the `generate_models_action` method, you are trying to get the user input from the `preprocess_data`, `save_test_data`, and `test_data_ratio` variables. However, these variables are not defined in the code provided. You will need to define them somewhere else in your code and pass their values to the `generate_models_action` method.\n",
      "5. In the `predict_data` method, you are trying to insert the text \"Predict Data button clicked\" into the text widget's insertion layer.\n",
      "6. In the `model_validation` method, you are trying to insert the text \"Model Validation button clicked\" into the text widget's insertion layer.\n",
      "7. In the `show_test_data_ratio` method, you are trying to delete the contents of the `test_data_ratio_entry` widget, but you are not adding anything back after deleting. You will need to add the original contents back to the widget after deleting.\n",
      "8. In the `hide_test_data_ratio` method, you are trying to grid_remove the `test_data_ratio_label` and `test_data_ratio_entry` widgets, but they are not currently grided. You will need to grid them in the first place before removing them.\n",
      "9. In the `generate_models` method, you are trying to create a new button with the text \"Generate Models\" and command=self.generate_models_action. However, the `command` parameter should be a reference to a method, not the method itself. To fix this, you can define a new method with the same name as the method you are trying to call, and pass it as the `command` parameter to the button.\n",
      "10. In the `App` class, you have a `master` attribute that is not being used. You can remove it to avoid any potential errors.</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Rejected sample 563 of the raw training set:\n",
      "<|assistant|>\n",
      "The code seems correct and provides appropriate responses to user input based on the buttons being clicked. However, you may want to add more buttons to the GUI and create corresponding functions in the code to further improve the interactivity and functionality of the application.</s>\n",
      "2024-05-31 16:26:24 - INFO - __main__ - Loading SFT adapter for model_args.model_name_or_path='/opt/ml/input/data/sft_model'\n",
      "[INFO|configuration_utils.py:731] 2024-05-31 16:26:24,597 >> loading configuration file /opt/ml/input/data/model/config.json\n",
      "[INFO|configuration_utils.py:731] 2024-05-31 16:26:24,597 >> loading configuration file /opt/ml/input/data/model/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-05-31 16:26:24,598 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "[INFO|configuration_utils.py:796] 2024-05-31 16:26:24,598 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "[INFO|modeling_utils.py:3471] 2024-05-31 16:26:24,603 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:3471] 2024-05-31 16:26:24,603 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:3570] 2024-05-31 16:26:24,603 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:3570] 2024-05-31 16:26:24,603 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:1519] 2024-05-31 16:26:24,603 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|modeling_utils.py:1519] 2024-05-31 16:26:24,603 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-31 16:26:24,604 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "[WARNING|logging.py:329] 2024-05-31 16:26:24,604 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "[INFO|configuration_utils.py:962] 2024-05-31 16:26:24,607 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "[INFO|configuration_utils.py:962] 2024-05-31 16:26:24,607 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "Decontaminating HumanEval samples: 100%|██████████| 1080/1080 [00:01<00:00, 788.54 examples/s]\n",
      "Decontaminating HumanEval samples: 100%|██████████| 1080/1080 [00:01<00:00, 787.81 examples/s]\n",
      "Decontaminating HumanEval samples:   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Decontaminating HumanEval samples: 100%|██████████| 120/120 [00:00<00:00, 751.23 examples/s]\n",
      "Decontaminating HumanEval samples: 100%|██████████| 120/120 [00:00<00:00, 745.46 examples/s]\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Decontaminated 0 (0.00%) samples from the training set.\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Prompt sample 228 of the raw training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "This pure white bouquet of Casa Blanca lilies makes an elegant statement as a perfect floral tribute, centerpiece or decorative arrangement. These lovely lilies also leave a soft fragrance in the air. Show loved ones that you care with fresh flowers from Texis Flower Shop.\n",
      "Vibrant tulips spring forth from a uniquely innovative garden of daisies, mini carnations and solidago arranged in a playful bird's nest basket. Based on the passage above, Can you provide more information about Casa Blanca lilies, such as their origin and symbolism?</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Chosen sample 228 of the raw training set:\n",
      "<|assistant|>\n",
      "Casa Blanca lilies, also known as Lilium 'Casa Blanca', are a popular variety of white lilies that originated in the United States. The hybrid was first developed by Dr. Cecil P. Henson in 1968 and introduced to the market by the Armstrong Nursery in California. The plant's exceptional qualities, such as its large, fragrant white flowers and strong stems, quickly made it a favorite among gardeners and florists.\n",
      "Casa Blanca lilies are known for their large, pure white, trumpet-shaped flowers, which can grow up to 8 inches in diameter. The flowers are often described as having a creamy white or ivory color and sometimes feature subtle yellow or pinkish streaks near the base of the petals. The lily's stamens are a rich yellow, adding a warm contrast to the pure white petals.\n",
      "In terms of symbolism, white lilies in general are often associated with purity, innocence, and sympathy. They are commonly used in funeral arrangements and memorial services to represent the departed's spiritual journey or to express sympathy to the bereaved. However, the specific symbolism of Casa Blanca lilies may vary depending on the culture or personal interpretation.</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Rejected sample 228 of the raw training set:\n",
      "<|assistant|>\n",
      "Casa Blanca lilies, also known as Lilium 'Casa Blanca' or 'White Lily', are a hybrid variety of the Oriental Lily group. They originate from the cold regions of Japan, where they were first cultivated in the late 20th century.\n",
      "Casa Blanca lilies are highly prized for their large, trumpet-shaped flowers, which are pure white, showy, and emit a sweet, captivating fragrance. They have a strong presence in wedding ceremonies, as well as other arrangements, for their elegance and simplicity.\n",
      "In terms of symbolism, white lilies, including Casa Blanca lilies, have a rich and deep meaning in various cultures. Traditionally, they are associated with purity, innocence, and spiritual enlightenment. In Christianity, the white lily is often considered a symbol of the Virgin Mary. It signifies her purity and chastity. In Greek mythology, the lily is associated with the goddess Hera, who created the lily when drops of her breast milk fell to the ground.\n",
      "Additionally, lilies are often included in funeral flower arrangements to symbolize the rebirth and renewal of the deceased's soul and to represent restored innocence after death. Casa Blanca lilies, with their pure white color and intense fragrance, evoke a sense of peace, love, and fond reminiscence, making them an ideal choice for various occasions and expressions of hope, sympathy, and adoration.</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Prompt sample 51 of the raw training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "Rearrange the list of words to form a coherent sentence.\n",
      "film the interested viewer oldest</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Chosen sample 51 of the raw training set:\n",
      "<|assistant|>\n",
      "The oldest viewer is interested in film.</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Rejected sample 51 of the raw training set:\n",
      "<|assistant|>\n",
      "Oldest film the interested viewer.</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Prompt sample 563 of the raw training set:\n",
      "<|system|>\n",
      "</s>\n",
      "<|user|>\n",
      "import tkinter as tk\n",
      "from tkinter import ttk\n",
      "class App:\n",
      " def \\_\\_init\\_\\_(self, master):\n",
      " self.master = master\n",
      " master.title(\"GUI Application\")\n",
      " \n",
      " # set the minimum window size\n",
      " master.minsize(width=400, height=300)\n",
      " \n",
      " # create a frame for the text widget\n",
      " text\\_frame = ttk.Frame(master, padding=10)\n",
      " text\\_frame.pack(expand=True, fill='both')\n",
      " \n",
      " # create the text widget and disable editing\n",
      " self.text\\_widget = tk.Text(text\\_frame, wrap='word', state='disabled')\n",
      " self.text\\_widget.pack(expand=True, fill='both')\n",
      " \n",
      " # create a frame for the buttons\n",
      " button\\_frame = ttk.Frame(master, padding=10)\n",
      " button\\_frame.pack(side='bottom', fill='x')\n",
      " \n",
      " # create the buttons\n",
      " load\\_data\\_button = ttk.Button(button\\_frame, text=\"Load Data\", command=self.load\\_data)\n",
      " data\\_analysis\\_button = ttk.Button(button\\_frame, text=\"Data Analysis\", command=self.data\\_analysis)\n",
      " generate\\_models\\_button = ttk.Button(button\\_frame, text=\"Generate Models\", command=self.generate\\_models)\n",
      " predict\\_data\\_button = ttk.Button(button\\_frame, text=\"Predict Data\", command=self.predict\\_data)\n",
      " model\\_validation\\_button = ttk.Button(button\\_frame, text=\"Model Validation\", command=self.model\\_validation)\n",
      " \n",
      " # grid the buttons\n",
      " load\\_data\\_button.grid(row=0, column=0, padx=5, pady=5, sticky='ew')\n",
      " data\\_analysis\\_button.grid(row=0, column=1, padx=5, pady=5, sticky='ew')\n",
      " generate\\_models\\_button.grid(row=0, column=2, padx=5, pady=5, sticky='ew')\n",
      " predict\\_data\\_button.grid(row=1, column=0, padx=5, pady=5, sticky='ew')\n",
      " model\\_validation\\_button.grid(row=1, column=1, padx=5, pady=5, sticky='ew')\n",
      " \n",
      " def load\\_data(self):\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", \"Load Data button clicked\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      " def data\\_analysis(self):\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", \"Data Analysis button clicked\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      " def generate\\_models(self):\n",
      " # create a new window\n",
      " new\\_window = tk.Toplevel(self.master)\n",
      " new\\_window.title(\"Generate Models\")\n",
      " new\\_window.geometry('250x150')\n",
      " \n",
      " # create a frame for the widgets\n",
      " frame = ttk.Frame(new\\_window, padding=10)\n",
      " frame.pack(expand=True, fill='both')\n",
      " \n",
      " # create the preprocess data checkbutton\n",
      " self.preprocess\\_data\\_var = tk.BooleanVar()\n",
      " preprocess\\_data\\_checkbutton = ttk.Checkbutton(frame, text=\"Preprocess Data\", variable=self.preprocess\\_data\\_var)\n",
      " preprocess\\_data\\_checkbutton.grid(row=0, column=0, padx=5, pady=5, sticky='w')\n",
      " \n",
      " # create the save test data checkbutton\n",
      " self.save\\_test\\_data\\_var = tk.BooleanVar()\n",
      " save\\_test\\_data\\_checkbutton = ttk.Checkbutton(frame, text=\"Split & Save Test Data\", variable=self.save\\_test\\_data\\_var, command=self.show\\_test\\_data\\_ratio)\n",
      " save\\_test\\_data\\_checkbutton.grid(row=1, column=0, padx=5, pady=5, sticky='w')\n",
      " \n",
      " # create the test data ratio label and entry field\n",
      " self.test\\_data\\_ratio\\_label = ttk.Label(frame, text=\"Test DataRatio (0-1):\")\n",
      " self.test\\_data\\_ratio\\_label.grid(row=2, column=0, padx=5, pady=5, sticky='w')\n",
      " self.test\\_data\\_ratio\\_entry = ttk.Entry(frame, state=\"disabled\")\n",
      " self.test\\_data\\_ratio\\_entry.grid(row=2, column=1, padx=5, pady=5, sticky='w')\n",
      " \n",
      " # create the generate models button\n",
      " generate\\_models\\_button = ttk.Button(frame, text=\"Generate Models\", command=self.generate\\_models\\_action)\n",
      " generate\\_models\\_button.grid(row=3, column=0, padx=5, pady=5, sticky='ew')\n",
      " \n",
      " # hide the test data ratio widgets by default\n",
      " self.hide\\_test\\_data\\_ratio()\n",
      " self.show\\_test\\_data\\_ratio()\n",
      " \n",
      " def show\\_test\\_data\\_ratio(self):\n",
      " if self.save\\_test\\_data\\_var.get():\n",
      " self.test\\_data\\_ratio\\_entry.config(state=\"normal\")\n",
      " else:\n",
      " self.test\\_data\\_ratio\\_entry.delete(0, \"end\")\n",
      " self.test\\_data\\_ratio\\_entry.config(state=\"disabled\")\n",
      " \n",
      " def hide\\_test\\_data\\_ratio(self):\n",
      " self.test\\_data\\_ratio\\_label.grid\\_remove()\n",
      " self.test\\_data\\_ratio\\_entry.grid\\_remove()\n",
      " \n",
      " def generate\\_models\\_action(self):\n",
      " # get the user input\n",
      " preprocess\\_data = self.preprocess\\_data\\_var.get()\n",
      " save\\_test\\_data = self.save\\_test\\_data\\_var.get()\n",
      " test\\_data\\_ratio = self.test\\_data\\_ratio\\_entry.get()\n",
      " \n",
      " # do something with the input\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", f\"Generate Models button clicked with preprocess\\_data={preprocess\\_data}, save\\_test\\_data={save\\_test\\_data}, test\\_data\\_ratio={test\\_data\\_ratio}\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      " # close the new window\n",
      " new\\_window.destroy()\n",
      " \n",
      " def predict\\_data(self):\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", \"Predict Data button clicked\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      " def model\\_validation(self):\n",
      " self.text\\_widget.config(state=\"normal\")\n",
      " self.text\\_widget.insert(\"end\", \"Model Validation button clicked\\n\")\n",
      " self.text\\_widget.config(state=\"disabled\")\n",
      " \n",
      "root = tk.Tk()\n",
      "app = App(root)\n",
      "root.mainloop()\n",
      "find problems in the code not working as expected</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Chosen sample 563 of the raw training set:\n",
      "<|assistant|>\n",
      "There are a few issues in the provided code that need to be addressed:\n",
      "1. In the `load_data` method, you are trying to insert the text \"Load Data button clicked\" into the text widget's insertion layer. However, the insertion layer is a readonly attribute, so you cannot modify it directly. To fix this, you can create a new text widget and insert the text into it instead.\n",
      "2. In the `data_analysis` method, you are trying to insert the text \"Data Analysis button clicked\" into the text widget's insertion layer. Similarly, you cannot modify the insertion layer directly.\n",
      "3. In the `generate_models` method, you are trying to create a new window and grid its contents. However, the `grid` method is not idempotent, meaning that it can only be called once per grid. To fix this, you can use the `pack` method instead of `grid` to add the widgets to the new window.\n",
      "4. In the `generate_models_action` method, you are trying to get the user input from the `preprocess_data`, `save_test_data`, and `test_data_ratio` variables. However, these variables are not defined in the code provided. You will need to define them somewhere else in your code and pass their values to the `generate_models_action` method.\n",
      "5. In the `predict_data` method, you are trying to insert the text \"Predict Data button clicked\" into the text widget's insertion layer.\n",
      "6. In the `model_validation` method, you are trying to insert the text \"Model Validation button clicked\" into the text widget's insertion layer.\n",
      "7. In the `show_test_data_ratio` method, you are trying to delete the contents of the `test_data_ratio_entry` widget, but you are not adding anything back after deleting. You will need to add the original contents back to the widget after deleting.\n",
      "8. In the `hide_test_data_ratio` method, you are trying to grid_remove the `test_data_ratio_label` and `test_data_ratio_entry` widgets, but they are not currently grided. You will need to grid them in the first place before removing them.\n",
      "9. In the `generate_models` method, you are trying to create a new button with the text \"Generate Models\" and command=self.generate_models_action. However, the `command` parameter should be a reference to a method, not the method itself. To fix this, you can define a new method with the same name as the method you are trying to call, and pass it as the `command` parameter to the button.\n",
      "10. In the `App` class, you have a `master` attribute that is not being used. You can remove it to avoid any potential errors.</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Rejected sample 563 of the raw training set:\n",
      "<|assistant|>\n",
      "The code seems correct and provides appropriate responses to user input based on the buttons being clicked. However, you may want to add more buttons to the GUI and create corresponding functions in the code to further improve the interactivity and functionality of the application.</s>\n",
      "2024-05-31 16:26:25 - INFO - __main__ - Loading SFT adapter for model_args.model_name_or_path='/opt/ml/input/data/sft_model'\n",
      "[INFO|configuration_utils.py:731] 2024-05-31 16:26:25,176 >> loading configuration file /opt/ml/input/data/model/config.json\n",
      "[INFO|configuration_utils.py:731] 2024-05-31 16:26:25,176 >> loading configuration file /opt/ml/input/data/model/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-05-31 16:26:25,177 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "[INFO|configuration_utils.py:796] 2024-05-31 16:26:25,177 >> Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/opt/ml/input/data/model\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "[INFO|modeling_utils.py:3471] 2024-05-31 16:26:25,182 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:3471] 2024-05-31 16:26:25,182 >> loading weights file /opt/ml/input/data/model/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:3570] 2024-05-31 16:26:25,183 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:3570] 2024-05-31 16:26:25,183 >> Will use torch_dtype=torch.bfloat16 as defined in model's config object\n",
      "[INFO|modeling_utils.py:1519] 2024-05-31 16:26:25,183 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|modeling_utils.py:1519] 2024-05-31 16:26:25,183 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-31 16:26:25,183 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "[WARNING|logging.py:329] 2024-05-31 16:26:25,183 >> The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "[INFO|configuration_utils.py:962] 2024-05-31 16:26:25,186 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "[INFO|configuration_utils.py:962] 2024-05-31 16:26:25,186 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.32s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.88s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.64s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.59s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-05-31 16:26:49,363 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "[INFO|modeling_utils.py:4280] 2024-05-31 16:26:49,363 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "[INFO|modeling_utils.py:4288] 2024-05-31 16:26:49,363 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|modeling_utils.py:4288] 2024-05-31 16:26:49,363 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:915] 2024-05-31 16:26:49,366 >> loading configuration file /opt/ml/input/data/model/generation_config.json\n",
      "[INFO|configuration_utils.py:915] 2024-05-31 16:26:49,366 >> loading configuration file /opt/ml/input/data/model/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-05-31 16:26:49,367 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "[INFO|configuration_utils.py:962] 2024-05-31 16:26:49,367 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.22s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.17s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-05-31 16:26:50,109 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "[INFO|modeling_utils.py:4280] 2024-05-31 16:26:50,109 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "[INFO|modeling_utils.py:4288] 2024-05-31 16:26:50,109 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|modeling_utils.py:4288] 2024-05-31 16:26:50,109 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at /opt/ml/input/data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:915] 2024-05-31 16:26:50,113 >> loading configuration file /opt/ml/input/data/model/generation_config.json\n",
      "[INFO|configuration_utils.py:915] 2024-05-31 16:26:50,113 >> loading configuration file /opt/ml/input/data/model/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-05-31 16:26:50,113 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "[INFO|configuration_utils.py:962] 2024-05-31 16:26:50,113 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Map:   4%|▍         | 41/1080 [00:00<00:02, 401.29 examples/s]\n",
      "[WARNING|tokenization_utils_base.py:3921] 2024-05-31 16:26:52,204 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2318 > 2048). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3921] 2024-05-31 16:26:52,204 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2318 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map:   9%|▉         | 99/1080 [00:00<00:02, 379.49 examples/s]\n",
      "Map:  13%|█▎        | 144/1080 [00:00<00:02, 400.26 examples/s]\n",
      "Map:  19%|█▊        | 200/1080 [00:00<00:02, 380.94 examples/s]\n",
      "Map:  22%|██▏       | 239/1080 [00:00<00:02, 380.28 examples/s]\n",
      "Map:  26%|██▌       | 278/1080 [00:00<00:02, 375.66 examples/s]\n",
      "Map:   0%|          | 0/1080 [00:00<?, ? examples/s]\n",
      "Map:   4%|▍         | 42/1080 [00:00<00:02, 403.19 examples/s]\n",
      "[WARNING|tokenization_utils_base.py:3921] 2024-05-31 16:26:52,935 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2318 > 2048). Running this sequence through the model will result in indexing errors\n",
      "[WARNING|tokenization_utils_base.py:3921] 2024-05-31 16:26:52,935 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2318 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map:   9%|▉         | 99/1080 [00:00<00:02, 379.60 examples/s]\n",
      "Map:  13%|█▎        | 144/1080 [00:00<00:02, 399.23 examples/s]\n",
      "Map:  19%|█▊        | 201/1080 [00:00<00:02, 381.11 examples/s]\n",
      "Map:  22%|██▏       | 240/1080 [00:00<00:02, 381.11 examples/s]\n",
      "Map:  29%|██▉       | 316/1080 [00:00<00:02, 372.94 examples/s]\n",
      "Map:  35%|███▍      | 375/1080 [00:00<00:01, 379.08 examples/s]\n",
      "Map:  39%|███▉      | 424/1080 [00:01<00:01, 359.50 examples/s]\n",
      "Map:  43%|████▎     | 466/1080 [00:01<00:01, 369.22 examples/s]\n",
      "Map:  47%|████▋     | 505/1080 [00:01<00:01, 373.68 examples/s]\n",
      "Map:  51%|█████     | 549/1080 [00:01<00:01, 386.37 examples/s]\n",
      "Map:  55%|█████▌    | 598/1080 [00:01<00:01, 361.59 examples/s]\n",
      "Map:  60%|█████▉    | 644/1080 [00:01<00:01, 385.67 examples/s]\n",
      "Map:  27%|██▋       | 294/1080 [00:00<00:02, 368.47 examples/s]\n",
      "Map:  33%|███▎      | 353/1080 [00:00<00:01, 372.07 examples/s]\n",
      "Map:  36%|███▋      | 393/1080 [00:01<00:01, 376.45 examples/s]\n",
      "Map:  41%|████▏     | 446/1080 [00:01<00:01, 365.17 examples/s]\n",
      "Map:  45%|████▌     | 486/1080 [00:01<00:01, 367.25 examples/s]\n",
      "Map:  49%|████▊     | 525/1080 [00:01<00:01, 370.20 examples/s]\n",
      "Map:  52%|█████▏    | 566/1080 [00:01<00:01, 377.38 examples/s]\n",
      "Map:  65%|██████▌   | 704/1080 [00:01<00:00, 386.22 examples/s]\n",
      "Map:  70%|███████   | 760/1080 [00:02<00:00, 376.21 examples/s]\n",
      "Map:  74%|███████▍  | 799/1080 [00:02<00:00, 373.82 examples/s]\n",
      "Map:  79%|███████▉  | 853/1080 [00:02<00:00, 365.10 examples/s]\n",
      "Map:  83%|████████▎ | 895/1080 [00:02<00:00, 373.43 examples/s]\n",
      "Map:  88%|████████▊ | 947/1080 [00:02<00:00, 354.93 examples/s]\n",
      "Map:  92%|█████████▎| 999/1080 [00:02<00:00, 344.04 examples/s]\n",
      "Map:  57%|█████▋    | 616/1080 [00:01<00:01, 359.35 examples/s]\n",
      "Map:  61%|██████    | 661/1080 [00:01<00:01, 381.80 examples/s]\n",
      "Map:  65%|██████▌   | 702/1080 [00:01<00:00, 383.76 examples/s]\n",
      "Map:  69%|██████▉   | 743/1080 [00:01<00:00, 385.46 examples/s]\n",
      "Map:  74%|███████▍  | 797/1080 [00:02<00:00, 368.25 examples/s]\n",
      "Map:  79%|███████▊  | 849/1080 [00:02<00:00, 358.31 examples/s]\n",
      "Map:  82%|████████▏ | 890/1080 [00:02<00:00, 370.58 examples/s]\n",
      "Map:  87%|████████▋ | 940/1080 [00:02<00:00, 353.33 examples/s]\n",
      "Map:  96%|█████████▌| 1036/1080 [00:03<00:00, 179.12 examples/s]\n",
      "Map:  99%|█████████▉| 1072/1080 [00:03<00:00, 204.42 examples/s]\n",
      "Map: 100%|██████████| 1080/1080 [00:03<00:00, 317.67 examples/s]\n",
      "Map:   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Map:  30%|███       | 36/120 [00:00<00:00, 345.58 examples/s]\n",
      "Map:  60%|██████    | 72/120 [00:00<00:00, 336.21 examples/s]\n",
      "Map:  92%|█████████▏| 110/120 [00:00<00:00, 348.25 examples/s]\n",
      "Map:  92%|█████████▏| 989/1080 [00:02<00:00, 341.74 examples/s]\n",
      "Map:  96%|█████████▌| 1036/1080 [00:03<00:00, 179.44 examples/s]\n",
      "Map:  99%|█████████▉| 1072/1080 [00:03<00:00, 203.72 examples/s]\n",
      "Map: 100%|██████████| 1080/1080 [00:03<00:00, 314.87 examples/s]\n",
      "Map:   0%|          | 0/120 [00:00<?, ? examples/s]\n",
      "Map:  30%|███       | 36/120 [00:00<00:00, 344.94 examples/s]\n",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 306.82 examples/s]\n",
      "Map:  60%|██████    | 72/120 [00:00<00:00, 334.52 examples/s]\n",
      "Map:  91%|█████████ | 109/120 [00:00<00:00, 344.89 examples/s]\n",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 305.88 examples/s]\n",
      "NCCL version 2.18.5+cuda12.1\n",
      "algo-1:197:293 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-1:197:293 [0] nccl_net_ofi_init:1239 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "algo-2:201:295 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\n",
      "algo-2:201:295 [0] nccl_net_ofi_init:1239 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "[INFO|trainer.py:641] 2024-05-31 16:26:58,006 >> Using auto half precision backend\n",
      "[INFO|trainer.py:641] 2024-05-31 16:26:58,006 >> Using auto half precision backend\n",
      "[INFO|trainer.py:641] 2024-05-31 16:26:58,006 >> Using auto half precision backend\n",
      "[INFO|trainer.py:641] 2024-05-31 16:26:58,006 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2078] 2024-05-31 16:26:58,605 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-31 16:26:58,605 >>   Num examples = 1,080\n",
      "[INFO|trainer.py:2078] 2024-05-31 16:26:58,605 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-31 16:26:58,605 >>   Num examples = 1,080\n",
      "[INFO|trainer.py:2080] 2024-05-31 16:26:58,605 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2081] 2024-05-31 16:26:58,605 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2080] 2024-05-31 16:26:58,605 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2081] 2024-05-31 16:26:58,605 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-05-31 16:26:58,605 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-05-31 16:26:58,606 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2084] 2024-05-31 16:26:58,605 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-05-31 16:26:58,606 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-05-31 16:26:58,606 >>   Total optimization steps = 270\n",
      "[INFO|trainer.py:2086] 2024-05-31 16:26:58,606 >>   Total optimization steps = 270\n",
      "[INFO|trainer.py:2087] 2024-05-31 16:26:58,611 >>   Number of trainable parameters = 27,262,976\n",
      "[INFO|trainer.py:2087] 2024-05-31 16:26:58,611 >>   Number of trainable parameters = 27,262,976\n",
      "[INFO|trainer.py:2078] 2024-05-31 16:26:58,576 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-31 16:26:58,576 >>   Num examples = 1,080\n",
      "[INFO|trainer.py:2080] 2024-05-31 16:26:58,576 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2078] 2024-05-31 16:26:58,576 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-31 16:26:58,576 >>   Num examples = 1,080\n",
      "[INFO|trainer.py:2080] 2024-05-31 16:26:58,576 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2081] 2024-05-31 16:26:58,576 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-05-31 16:26:58,576 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-05-31 16:26:58,576 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-05-31 16:26:58,576 >>   Total optimization steps = 270\n",
      "[INFO|trainer.py:2081] 2024-05-31 16:26:58,576 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-05-31 16:26:58,576 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-05-31 16:26:58,576 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-05-31 16:26:58,576 >>   Total optimization steps = 270\n",
      "[INFO|trainer.py:2087] 2024-05-31 16:26:58,582 >>   Number of trainable parameters = 27,262,976\n",
      "[INFO|trainer.py:2087] 2024-05-31 16:26:58,582 >>   Number of trainable parameters = 27,262,976\n",
      "0%|          | 0/270 [00:00<?, ?it/s]\n",
      "[WARNING|logging.py:329] 2024-05-31 16:27:04,869 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-31 16:27:04,869 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-31 16:27:04,899 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-05-31 16:27:04,899 >> The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[WARNING|modeling_utils.py:1190] 2024-05-31 16:27:06,582 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "[WARNING|modeling_utils.py:1190] 2024-05-31 16:27:06,582 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "[WARNING|modeling_utils.py:1190] 2024-05-31 16:27:06,515 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "[WARNING|modeling_utils.py:1190] 2024-05-31 16:27:06,515 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "0%|          | 1/270 [00:09<43:27,  9.69s/it]\n",
      "{'loss': 0.6931, 'grad_norm': 5.625, 'learning_rate': 1.8518518518518518e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -121.4501953125, 'logps/chosen': -105.63301849365234, 'logits/rejected': -2.626948595046997, 'logits/chosen': -2.5184803009033203, 'epoch': 0.01}\n",
      "0%|          | 1/270 [00:09<43:27,  9.69s/it]\n",
      "1%|          | 2/270 [00:19<43:08,  9.66s/it]\n",
      "1%|          | 3/270 [00:29<43:01,  9.67s/it]\n",
      "1%|▏         | 4/270 [00:40<45:34, 10.28s/it]\n",
      "2%|▏         | 5/270 [00:50<44:48, 10.15s/it]\n",
      "2%|▏         | 6/270 [01:01<45:53, 10.43s/it]\n",
      "3%|▎         | 7/270 [01:09<42:41,  9.74s/it]\n",
      "3%|▎         | 8/270 [01:21<46:11, 10.58s/it]\n",
      "3%|▎         | 9/270 [01:33<46:57, 10.80s/it]\n",
      "4%|▎         | 10/270 [01:45<48:51, 11.27s/it]\n",
      "{'loss': 0.6902, 'grad_norm': 6.84375, 'learning_rate': 1.8518518518518516e-07, 'rewards/chosen': 0.001972193131223321, 'rewards/rejected': -0.005475341808050871, 'rewards/accuracies': 0.4722222089767456, 'rewards/margins': 0.0074475351721048355, 'logps/rejected': -278.8580017089844, 'logps/chosen': -286.0933837890625, 'logits/rejected': -2.5930144786834717, 'logits/chosen': -2.6535184383392334, 'epoch': 0.07}\n",
      "4%|▎         | 10/270 [01:45<48:51, 11.27s/it]\n",
      "4%|▍         | 11/270 [01:57<49:45, 11.53s/it]\n",
      "4%|▍         | 12/270 [02:10<52:01, 12.10s/it]\n",
      "5%|▍         | 13/270 [02:21<50:00, 11.67s/it]\n",
      "5%|▌         | 14/270 [02:32<49:03, 11.50s/it]\n",
      "6%|▌         | 15/270 [02:43<47:21, 11.14s/it]\n",
      "6%|▌         | 16/270 [02:51<43:52, 10.37s/it]\n",
      "6%|▋         | 17/270 [03:02<44:51, 10.64s/it]\n",
      "7%|▋         | 18/270 [03:14<45:56, 10.94s/it]\n",
      "7%|▋         | 19/270 [03:27<48:00, 11.48s/it]\n",
      "7%|▋         | 20/270 [03:38<47:51, 11.49s/it]\n",
      "{'loss': 0.691, 'grad_norm': 5.65625, 'learning_rate': 3.703703703703703e-07, 'rewards/chosen': -0.006523690186440945, 'rewards/rejected': -0.005163183435797691, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': -0.001360508264042437, 'logps/rejected': -223.64501953125, 'logps/chosen': -204.2972412109375, 'logits/rejected': -2.632880449295044, 'logits/chosen': -2.6665854454040527, 'epoch': 0.15}\n",
      "7%|▋         | 20/270 [03:38<47:51, 11.49s/it]\n",
      "8%|▊         | 21/270 [03:50<48:21, 11.65s/it]\n",
      "8%|▊         | 22/270 [04:01<47:27, 11.48s/it]\n",
      "9%|▊         | 23/270 [04:12<45:42, 11.10s/it]\n",
      "9%|▉         | 24/270 [04:21<43:40, 10.65s/it]\n",
      "9%|▉         | 25/270 [04:29<40:32,  9.93s/it]\n",
      "10%|▉         | 26/270 [04:42<43:03, 10.59s/it]\n",
      "10%|█         | 27/270 [04:52<43:15, 10.68s/it]\n",
      "10%|█         | 28/270 [05:04<43:43, 10.84s/it]\n",
      "11%|█         | 29/270 [05:15<43:31, 10.84s/it]\n",
      "11%|█         | 30/270 [05:25<42:38, 10.66s/it]\n",
      "{'loss': 0.696, 'grad_norm': 6.6875, 'learning_rate': 4.938271604938271e-07, 'rewards/chosen': -0.007080030627548695, 'rewards/rejected': 0.002290301490575075, 'rewards/accuracies': 0.3499999940395355, 'rewards/margins': -0.009370332583785057, 'logps/rejected': -254.0491943359375, 'logps/chosen': -283.61517333984375, 'logits/rejected': -2.544062614440918, 'logits/chosen': -2.841949939727783, 'epoch': 0.22}\n",
      "11%|█         | 30/270 [05:25<42:38, 10.66s/it]\n",
      "11%|█▏        | 31/270 [05:36<42:35, 10.69s/it]\n",
      "12%|█▏        | 32/270 [05:44<40:02, 10.10s/it]\n",
      "12%|█▏        | 33/270 [05:54<39:02,  9.89s/it]\n",
      "13%|█▎        | 34/270 [06:04<38:55,  9.90s/it]\n",
      "13%|█▎        | 35/270 [06:15<40:28, 10.34s/it]\n",
      "13%|█▎        | 36/270 [06:26<41:24, 10.62s/it]\n",
      "14%|█▎        | 37/270 [06:39<44:02, 11.34s/it]\n",
      "14%|█▍        | 38/270 [06:50<43:08, 11.16s/it]\n",
      "14%|█▍        | 39/270 [07:04<46:33, 12.10s/it]\n",
      "15%|█▍        | 40/270 [07:14<43:18, 11.30s/it]\n",
      "{'loss': 0.6907, 'grad_norm': 5.84375, 'learning_rate': 4.732510288065844e-07, 'rewards/chosen': 0.002040862338617444, 'rewards/rejected': 0.0013634013012051582, 'rewards/accuracies': 0.42500001192092896, 'rewards/margins': 0.000677461561281234, 'logps/rejected': -263.30499267578125, 'logps/chosen': -328.3222351074219, 'logits/rejected': -2.605181932449341, 'logits/chosen': -2.757256031036377, 'epoch': 0.3}\n",
      "15%|█▍        | 40/270 [07:14<43:18, 11.30s/it]\n",
      "15%|█▌        | 41/270 [07:24<42:19, 11.09s/it]\n",
      "16%|█▌        | 42/270 [07:36<42:53, 11.29s/it]\n",
      "16%|█▌        | 43/270 [07:48<43:13, 11.43s/it]\n",
      "16%|█▋        | 44/270 [07:58<41:45, 11.09s/it]\n",
      "17%|█▋        | 45/270 [08:10<42:07, 11.23s/it]\n",
      "17%|█▋        | 46/270 [08:20<41:17, 11.06s/it]\n",
      "17%|█▋        | 47/270 [08:32<41:59, 11.30s/it]\n",
      "18%|█▊        | 48/270 [08:46<44:19, 11.98s/it]\n",
      "18%|█▊        | 49/270 [08:59<45:08, 12.26s/it]\n",
      "19%|█▊        | 50/270 [09:09<43:00, 11.73s/it]\n",
      "{'loss': 0.6937, 'grad_norm': 6.8125, 'learning_rate': 4.5267489711934156e-07, 'rewards/chosen': -0.006109938956797123, 'rewards/rejected': -0.009284386411309242, 'rewards/accuracies': 0.625, 'rewards/margins': 0.003174448385834694, 'logps/rejected': -263.4366760253906, 'logps/chosen': -355.26605224609375, 'logits/rejected': -2.513270378112793, 'logits/chosen': -2.6672348976135254, 'epoch': 0.37}\n",
      "19%|█▊        | 50/270 [09:09<43:00, 11.73s/it]\n",
      "19%|█▉        | 51/270 [09:17<39:04, 10.71s/it]\n",
      "19%|█▉        | 52/270 [09:31<41:40, 11.47s/it]\n",
      "20%|█▉        | 53/270 [09:41<40:40, 11.25s/it]\n",
      "20%|██        | 54/270 [09:54<41:34, 11.55s/it]\n",
      "20%|██        | 55/270 [10:04<40:16, 11.24s/it]\n",
      "21%|██        | 56/270 [10:14<38:41, 10.85s/it]\n",
      "21%|██        | 57/270 [10:26<39:13, 11.05s/it]\n",
      "21%|██▏       | 58/270 [10:36<38:39, 10.94s/it]\n",
      "22%|██▏       | 59/270 [10:48<39:09, 11.13s/it]\n",
      "22%|██▏       | 60/270 [10:59<38:33, 11.02s/it]\n",
      "{'loss': 0.6907, 'grad_norm': 9.1875, 'learning_rate': 4.320987654320987e-07, 'rewards/chosen': 0.008497238159179688, 'rewards/rejected': -0.0004912572912871838, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.008988494984805584, 'logps/rejected': -266.6556091308594, 'logps/chosen': -281.5782775878906, 'logits/rejected': -2.7656843662261963, 'logits/chosen': -2.754098415374756, 'epoch': 0.44}\n",
      "22%|██▏       | 60/270 [10:59<38:33, 11.02s/it]\n",
      "23%|██▎       | 61/270 [11:07<35:42, 10.25s/it]\n",
      "23%|██▎       | 62/270 [11:18<36:01, 10.39s/it]\n",
      "23%|██▎       | 63/270 [11:29<36:56, 10.71s/it]\n",
      "24%|██▎       | 64/270 [11:43<39:55, 11.63s/it]\n",
      "24%|██▍       | 65/270 [11:52<37:11, 10.88s/it]\n",
      "24%|██▍       | 66/270 [12:03<37:22, 10.99s/it]\n",
      "25%|██▍       | 67/270 [12:18<40:39, 12.02s/it]\n",
      "25%|██▌       | 68/270 [12:28<39:00, 11.58s/it]\n",
      "26%|██▌       | 69/270 [12:38<37:12, 11.10s/it]\n",
      "26%|██▌       | 70/270 [12:50<37:11, 11.16s/it]\n",
      "{'loss': 0.6935, 'grad_norm': 6.875, 'learning_rate': 4.11522633744856e-07, 'rewards/chosen': 3.129495235043578e-05, 'rewards/rejected': -0.011049918830394745, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.011081215925514698, 'logps/rejected': -307.21533203125, 'logps/chosen': -335.71331787109375, 'logits/rejected': -2.647487163543701, 'logits/chosen': -2.727731227874756, 'epoch': 0.52}\n",
      "26%|██▌       | 70/270 [12:50<37:11, 11.16s/it]\n",
      "26%|██▋       | 71/270 [13:01<36:58, 11.15s/it]\n",
      "27%|██▋       | 72/270 [13:13<37:44, 11.43s/it]\n",
      "27%|██▋       | 73/270 [13:26<39:13, 11.95s/it]\n",
      "27%|██▋       | 74/270 [13:38<38:46, 11.87s/it]\n",
      "28%|██▊       | 75/270 [13:51<40:06, 12.34s/it]\n",
      "28%|██▊       | 76/270 [13:59<35:32, 10.99s/it]\n",
      "29%|██▊       | 77/270 [14:10<35:16, 10.96s/it]\n",
      "29%|██▉       | 78/270 [14:25<38:38, 12.07s/it]\n",
      "29%|██▉       | 79/270 [14:36<37:29, 11.78s/it]\n",
      "30%|██▉       | 80/270 [14:49<38:41, 12.22s/it]\n",
      "{'loss': 0.694, 'grad_norm': 7.0, 'learning_rate': 3.909465020576131e-07, 'rewards/chosen': 0.006753310561180115, 'rewards/rejected': 0.0069519998505711555, 'rewards/accuracies': 0.5249999761581421, 'rewards/margins': -0.00019868797971867025, 'logps/rejected': -305.8546142578125, 'logps/chosen': -321.2491149902344, 'logits/rejected': -2.6639909744262695, 'logits/chosen': -2.7102508544921875, 'epoch': 0.59}\n",
      "30%|██▉       | 80/270 [14:49<38:41, 12.22s/it]\n",
      "30%|███       | 81/270 [14:58<35:14, 11.19s/it]\n",
      "30%|███       | 82/270 [15:10<36:25, 11.62s/it]\n",
      "31%|███       | 83/270 [15:21<35:08, 11.28s/it]\n",
      "31%|███       | 84/270 [15:31<33:54, 10.94s/it]\n",
      "31%|███▏      | 85/270 [15:41<32:42, 10.61s/it]\n",
      "32%|███▏      | 86/270 [15:50<31:18, 10.21s/it]\n",
      "32%|███▏      | 87/270 [16:02<33:04, 10.84s/it]\n",
      "33%|███▎      | 88/270 [16:13<32:24, 10.68s/it]\n",
      "33%|███▎      | 89/270 [16:24<33:06, 10.97s/it]\n",
      "33%|███▎      | 90/270 [16:35<33:02, 11.02s/it]\n",
      "{'loss': 0.6931, 'grad_norm': 7.40625, 'learning_rate': 3.703703703703703e-07, 'rewards/chosen': 0.0022492692805826664, 'rewards/rejected': 0.002939986763522029, 'rewards/accuracies': 0.5249999761581421, 'rewards/margins': -0.0006907180068083107, 'logps/rejected': -219.61752319335938, 'logps/chosen': -295.9939270019531, 'logits/rejected': -2.6392452716827393, 'logits/chosen': -2.7359070777893066, 'epoch': 0.67}\n",
      "33%|███▎      | 90/270 [16:35<33:02, 11.02s/it]\n",
      "34%|███▎      | 91/270 [16:46<32:26, 10.87s/it]\n",
      "34%|███▍      | 92/270 [16:57<32:44, 11.04s/it]\n",
      "34%|███▍      | 93/270 [17:09<33:07, 11.23s/it]\n",
      "35%|███▍      | 94/270 [17:19<31:38, 10.79s/it]\n",
      "35%|███▌      | 95/270 [17:31<32:14, 11.06s/it]\n",
      "36%|███▌      | 96/270 [17:43<32:58, 11.37s/it]\n",
      "36%|███▌      | 97/270 [17:52<31:15, 10.84s/it]\n",
      "36%|███▋      | 98/270 [18:06<33:46, 11.78s/it]\n",
      "37%|███▋      | 99/270 [18:19<34:18, 12.04s/it]\n",
      "37%|███▋      | 100/270 [18:32<35:02, 12.37s/it]\n",
      "{'loss': 0.6887, 'grad_norm': 6.71875, 'learning_rate': 3.4979423868312755e-07, 'rewards/chosen': 0.005843495950102806, 'rewards/rejected': -0.006984358187764883, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': 0.012827855534851551, 'logps/rejected': -282.949462890625, 'logps/chosen': -288.04119873046875, 'logits/rejected': -2.7510628700256348, 'logits/chosen': -2.730745553970337, 'epoch': 0.74}\n",
      "37%|███▋      | 100/270 [18:32<35:02, 12.37s/it]\n",
      "37%|███▋      | 101/270 [18:44<34:37, 12.29s/it]\n",
      "38%|███▊      | 102/270 [18:59<36:21, 12.98s/it]\n",
      "38%|███▊      | 103/270 [19:08<33:14, 11.95s/it]\n",
      "39%|███▊      | 104/270 [19:18<31:23, 11.34s/it]\n",
      "39%|███▉      | 105/270 [19:32<32:50, 11.94s/it]\n",
      "39%|███▉      | 106/270 [19:45<33:54, 12.40s/it]\n",
      "40%|███▉      | 107/270 [19:56<32:52, 12.10s/it]\n",
      "40%|████      | 108/270 [20:08<32:25, 12.01s/it]\n",
      "40%|████      | 109/270 [20:18<30:08, 11.24s/it]\n",
      "41%|████      | 110/270 [20:27<28:43, 10.77s/it]\n",
      "{'loss': 0.6938, 'grad_norm': 5.9375, 'learning_rate': 3.2921810699588474e-07, 'rewards/chosen': 0.004261932801455259, 'rewards/rejected': 0.002388753928244114, 'rewards/accuracies': 0.574999988079071, 'rewards/margins': 0.001873179106041789, 'logps/rejected': -303.9623718261719, 'logps/chosen': -291.6398010253906, 'logits/rejected': -2.733307361602783, 'logits/chosen': -2.7551848888397217, 'epoch': 0.81}\n",
      "41%|████      | 110/270 [20:27<28:43, 10.77s/it]\n",
      "41%|████      | 111/270 [20:40<29:40, 11.20s/it]\n",
      "41%|████▏     | 112/270 [20:49<27:55, 10.60s/it]\n",
      "42%|████▏     | 113/270 [21:00<28:23, 10.85s/it]\n",
      "42%|████▏     | 114/270 [21:10<27:34, 10.61s/it]\n",
      "43%|████▎     | 115/270 [21:19<26:17, 10.17s/it]\n",
      "43%|████▎     | 116/270 [21:29<25:57, 10.11s/it]\n",
      "43%|████▎     | 117/270 [21:41<26:47, 10.51s/it]\n",
      "44%|████▎     | 118/270 [21:52<27:23, 10.81s/it]\n",
      "44%|████▍     | 119/270 [22:06<29:01, 11.54s/it]\n",
      "44%|████▍     | 120/270 [22:17<29:08, 11.66s/it]\n",
      "{'loss': 0.6949, 'grad_norm': 6.96875, 'learning_rate': 3.086419753086419e-07, 'rewards/chosen': 0.004513311665505171, 'rewards/rejected': -0.00019999034702777863, 'rewards/accuracies': 0.5, 'rewards/margins': 0.004713301546871662, 'logps/rejected': -242.08706665039062, 'logps/chosen': -293.9195251464844, 'logits/rejected': -2.578150510787964, 'logits/chosen': -2.7993946075439453, 'epoch': 0.89}\n",
      "44%|████▍     | 120/270 [22:17<29:08, 11.66s/it]\n",
      "45%|████▍     | 121/270 [22:27<27:45, 11.18s/it]\n",
      "45%|████▌     | 122/270 [22:42<30:21, 12.31s/it]\n",
      "46%|████▌     | 123/270 [22:55<30:01, 12.25s/it]\n",
      "46%|████▌     | 124/270 [23:07<29:42, 12.21s/it]\n",
      "46%|████▋     | 125/270 [23:16<27:46, 11.49s/it]\n",
      "47%|████▋     | 126/270 [23:27<26:48, 11.17s/it]\n",
      "47%|████▋     | 127/270 [23:37<26:00, 10.92s/it]\n",
      "47%|████▋     | 128/270 [23:47<25:01, 10.57s/it]\n",
      "48%|████▊     | 129/270 [23:58<25:04, 10.67s/it]\n",
      "48%|████▊     | 130/270 [24:07<23:33, 10.10s/it]\n",
      "48%|████▊     | 130/270 [24:07<23:33, 10.10s/it]\n",
      "{'loss': 0.6924, 'grad_norm': 5.9375, 'learning_rate': 2.8806584362139917e-07, 'rewards/chosen': 0.0019807147327810526, 'rewards/rejected': -0.005572075489908457, 'rewards/accuracies': 0.6000000238418579, 'rewards/margins': 0.007552790455520153, 'logps/rejected': -225.6296844482422, 'logps/chosen': -308.0544738769531, 'logits/rejected': -2.5872340202331543, 'logits/chosen': -2.8129496574401855, 'epoch': 0.96}\n",
      "49%|████▊     | 131/270 [24:17<23:31, 10.16s/it]\n",
      "49%|████▉     | 132/270 [24:30<25:18, 11.00s/it]\n",
      "49%|████▉     | 133/270 [24:43<26:39, 11.67s/it]\n",
      "50%|████▉     | 134/270 [24:52<24:33, 10.84s/it]\n",
      "50%|█████     | 135/270 [25:02<23:59, 10.66s/it]\n",
      "[INFO|trainer.py:3719] 2024-05-31 16:52:06,432 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3719] 2024-05-31 16:52:06,432 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-31 16:52:06,433 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 16:52:06,433 >>   Batch size = 1\n",
      "[INFO|trainer.py:3721] 2024-05-31 16:52:06,433 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 16:52:06,433 >>   Batch size = 1\n",
      "[INFO|trainer.py:3719] 2024-05-31 16:52:06,432 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-31 16:52:06,432 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 16:52:06,432 >>   Batch size = 1\n",
      "[INFO|trainer.py:3719] 2024-05-31 16:52:06,432 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-31 16:52:06,432 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 16:52:06,432 >>   Batch size = 1\n",
      "0%|          | 0/60 [00:00<?, ?it/s]#033[A\n",
      "3%|▎         | 2/60 [00:01<00:31,  1.87it/s]#033[A\n",
      "5%|▌         | 3/60 [00:04<01:40,  1.76s/it]#033[A\n",
      "7%|▋         | 4/60 [00:05<01:27,  1.56s/it]#033[A\n",
      "8%|▊         | 5/60 [00:07<01:32,  1.68s/it]#033[A\n",
      "10%|█         | 6/60 [00:08<01:19,  1.47s/it]#033[A\n",
      "12%|█▏        | 7/60 [00:09<01:13,  1.39s/it]#033[A\n",
      "13%|█▎        | 8/60 [00:10<01:01,  1.19s/it]#033[A\n",
      "15%|█▌        | 9/60 [00:11<01:01,  1.20s/it]#033[A\n",
      "17%|█▋        | 10/60 [00:13<01:03,  1.27s/it]#033[A\n",
      "18%|█▊        | 11/60 [00:15<01:17,  1.59s/it]#033[A\n",
      "20%|██        | 12/60 [00:16<01:07,  1.40s/it]#033[A\n",
      "22%|██▏       | 13/60 [00:18<01:18,  1.67s/it]#033[A\n",
      "23%|██▎       | 14/60 [00:19<01:04,  1.41s/it]#033[A\n",
      "25%|██▌       | 15/60 [00:21<01:12,  1.60s/it]#033[A\n",
      "27%|██▋       | 16/60 [00:23<01:15,  1.71s/it]#033[A\n",
      "28%|██▊       | 17/60 [00:25<01:10,  1.64s/it]#033[A\n",
      "30%|███       | 18/60 [00:26<00:58,  1.39s/it]#033[A\n",
      "32%|███▏      | 19/60 [00:27<00:58,  1.44s/it]#033[A\n",
      "33%|███▎      | 20/60 [00:29<00:58,  1.45s/it]#033[A\n",
      "35%|███▌      | 21/60 [00:30<00:54,  1.39s/it]#033[A\n",
      "37%|███▋      | 22/60 [00:32<01:02,  1.65s/it]#033[A\n",
      "38%|███▊      | 23/60 [00:33<00:56,  1.53s/it]#033[A\n",
      "40%|████      | 24/60 [00:34<00:49,  1.37s/it]#033[A\n",
      "42%|████▏     | 25/60 [00:36<00:46,  1.32s/it]#033[A\n",
      "43%|████▎     | 26/60 [00:38<00:58,  1.72s/it]#033[A\n",
      "45%|████▌     | 27/60 [00:39<00:47,  1.45s/it]#033[A\n",
      "47%|████▋     | 28/60 [00:41<00:53,  1.67s/it]#033[A\n",
      "48%|████▊     | 29/60 [00:42<00:47,  1.54s/it]#033[A\n",
      "50%|█████     | 30/60 [00:44<00:50,  1.69s/it]#033[A\n",
      "52%|█████▏    | 31/60 [00:46<00:48,  1.67s/it]#033[A\n",
      "53%|█████▎    | 32/60 [00:47<00:44,  1.59s/it]#033[A\n",
      "55%|█████▌    | 33/60 [00:49<00:43,  1.60s/it]#033[A\n",
      "57%|█████▋    | 34/60 [00:50<00:35,  1.35s/it]#033[A\n",
      "58%|█████▊    | 35/60 [00:51<00:32,  1.29s/it]#033[A\n",
      "60%|██████    | 36/60 [00:52<00:28,  1.19s/it]#033[A\n",
      "62%|██████▏   | 37/60 [00:55<00:39,  1.71s/it]#033[A\n",
      "63%|██████▎   | 38/60 [00:56<00:34,  1.58s/it]#033[A\n",
      "65%|██████▌   | 39/60 [00:58<00:35,  1.71s/it]#033[A\n",
      "67%|██████▋   | 40/60 [01:00<00:32,  1.63s/it]#033[A\n",
      "68%|██████▊   | 41/60 [01:02<00:33,  1.76s/it]#033[A\n",
      "70%|███████   | 42/60 [01:04<00:34,  1.90s/it]#033[A\n",
      "72%|███████▏  | 43/60 [01:05<00:30,  1.77s/it]#033[A\n",
      "73%|███████▎  | 44/60 [01:07<00:25,  1.58s/it]#033[A\n",
      "75%|███████▌  | 45/60 [01:08<00:23,  1.58s/it]#033[A\n",
      "77%|███████▋  | 46/60 [01:10<00:21,  1.55s/it]#033[A\n",
      "78%|███████▊  | 47/60 [01:11<00:18,  1.39s/it]#033[A\n",
      "80%|████████  | 48/60 [01:12<00:18,  1.51s/it]#033[A\n",
      "82%|████████▏ | 49/60 [01:14<00:15,  1.44s/it]#033[A\n",
      "83%|████████▎ | 50/60 [01:15<00:13,  1.38s/it]#033[A\n",
      "85%|████████▌ | 51/60 [01:17<00:14,  1.62s/it]#033[A\n",
      "87%|████████▋ | 52/60 [01:18<00:12,  1.55s/it]#033[A\n",
      "88%|████████▊ | 53/60 [01:19<00:09,  1.40s/it]#033[A\n",
      "90%|█████████ | 54/60 [01:21<00:08,  1.35s/it]#033[A\n",
      "92%|█████████▏| 55/60 [01:22<00:06,  1.31s/it]#033[A\n",
      "93%|█████████▎| 56/60 [01:23<00:04,  1.16s/it]#033[A\n",
      "95%|█████████▌| 57/60 [01:24<00:03,  1.19s/it]#033[A\n",
      "97%|█████████▋| 58/60 [01:25<00:02,  1.18s/it]#033[A\n",
      "98%|█████████▊| 59/60 [01:27<00:01,  1.30s/it]#033[A\n",
      "100%|██████████| 60/60 [01:28<00:00,  1.33s/it]#033[A\n",
      "#033[A\n",
      "{'eval_loss': 0.6897696256637573, 'eval_runtime': 90.294, 'eval_samples_per_second': 1.329, 'eval_steps_per_second': 0.664, 'eval_rewards/chosen': 0.00958202313631773, 'eval_rewards/rejected': 0.00143112160731107, 'eval_rewards/accuracies': 0.6000000238418579, 'eval_rewards/margins': 0.008150902576744556, 'eval_logps/rejected': -293.67791748046875, 'eval_logps/chosen': -272.3252258300781, 'eval_logits/rejected': -2.69402813911438, 'eval_logits/chosen': -2.763434648513794, 'epoch': 1.0}\n",
      "50%|█████     | 135/270 [26:33<23:59, 10.66s/it]\n",
      "100%|██████████| 60/60 [01:28<00:00,  1.33s/it]#033[A\n",
      "#033[A\n",
      "50%|█████     | 136/270 [26:44<1:24:54, 38.02s/it]\n",
      "51%|█████     | 137/270 [26:54<1:05:40, 29.63s/it]\n",
      "51%|█████     | 138/270 [27:05<52:53, 24.04s/it]\n",
      "51%|█████▏    | 139/270 [27:16<43:33, 19.95s/it]\n",
      "52%|█████▏    | 140/270 [27:27<37:50, 17.47s/it]\n",
      "{'loss': 0.6925, 'grad_norm': 7.5, 'learning_rate': 2.6748971193415635e-07, 'rewards/chosen': 0.0033057834953069687, 'rewards/rejected': 0.004720435012131929, 'rewards/accuracies': 0.4000000059604645, 'rewards/margins': -0.0014146522153168917, 'logps/rejected': -181.20791625976562, 'logps/chosen': -364.7003479003906, 'logits/rejected': -2.4567456245422363, 'logits/chosen': -2.850132465362549, 'epoch': 1.04}\n",
      "52%|█████▏    | 140/270 [27:27<37:50, 17.47s/it]\n",
      "52%|█████▏    | 141/270 [27:39<34:07, 15.87s/it]\n",
      "53%|█████▎    | 142/270 [27:49<29:41, 13.92s/it]\n",
      "53%|█████▎    | 143/270 [28:01<28:17, 13.36s/it]\n",
      "53%|█████▎    | 144/270 [28:12<26:33, 12.65s/it]\n",
      "54%|█████▎    | 145/270 [28:22<24:28, 11.75s/it]\n",
      "54%|█████▍    | 146/270 [28:32<23:23, 11.31s/it]\n",
      "54%|█████▍    | 147/270 [28:42<22:43, 11.09s/it]\n",
      "55%|█████▍    | 148/270 [28:52<21:54, 10.77s/it]\n",
      "55%|█████▌    | 149/270 [29:03<21:33, 10.69s/it]\n",
      "56%|█████▌    | 150/270 [29:13<21:05, 10.55s/it]\n",
      "{'loss': 0.6911, 'grad_norm': 5.71875, 'learning_rate': 2.4691358024691354e-07, 'rewards/chosen': 0.006814341060817242, 'rewards/rejected': 0.0034389453940093517, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.0033753965981304646, 'logps/rejected': -300.68377685546875, 'logps/chosen': -300.6199645996094, 'logits/rejected': -2.6789045333862305, 'logits/chosen': -2.6955089569091797, 'epoch': 1.11}\n",
      "56%|█████▌    | 150/270 [29:13<21:05, 10.55s/it]\n",
      "56%|█████▌    | 151/270 [29:24<21:20, 10.76s/it]\n",
      "56%|█████▋    | 152/270 [29:35<21:18, 10.83s/it]\n",
      "57%|█████▋    | 153/270 [29:47<21:45, 11.16s/it]\n",
      "57%|█████▋    | 154/270 [29:59<21:47, 11.27s/it]\n",
      "57%|█████▋    | 155/270 [30:09<21:13, 11.07s/it]\n",
      "58%|█████▊    | 156/270 [30:19<20:22, 10.73s/it]\n",
      "58%|█████▊    | 157/270 [30:30<20:18, 10.79s/it]\n",
      "59%|█████▊    | 158/270 [30:42<20:26, 10.95s/it]\n",
      "59%|█████▉    | 159/270 [30:54<20:58, 11.34s/it]\n",
      "59%|█████▉    | 160/270 [31:06<21:03, 11.49s/it]\n",
      "{'loss': 0.685, 'grad_norm': 6.75, 'learning_rate': 2.2633744855967078e-07, 'rewards/chosen': 0.0186627060174942, 'rewards/rejected': -0.012060337699949741, 'rewards/accuracies': 0.699999988079071, 'rewards/margins': 0.030723044648766518, 'logps/rejected': -301.38006591796875, 'logps/chosen': -351.0682678222656, 'logits/rejected': -2.573406219482422, 'logits/chosen': -2.6019949913024902, 'epoch': 1.19}\n",
      "59%|█████▉    | 160/270 [31:06<21:03, 11.49s/it]\n",
      "60%|█████▉    | 161/270 [31:19<21:35, 11.89s/it]\n",
      "60%|██████    | 162/270 [31:29<20:35, 11.44s/it]\n",
      "60%|██████    | 163/270 [31:41<20:56, 11.74s/it]\n",
      "61%|██████    | 164/270 [31:50<19:00, 10.76s/it]\n",
      "61%|██████    | 165/270 [32:02<19:49, 11.33s/it]\n",
      "61%|██████▏   | 166/270 [32:13<19:10, 11.06s/it]\n",
      "62%|██████▏   | 167/270 [32:23<18:21, 10.69s/it]\n",
      "62%|██████▏   | 168/270 [32:33<18:01, 10.60s/it]\n",
      "63%|██████▎   | 169/270 [32:46<18:46, 11.16s/it]\n",
      "63%|██████▎   | 170/270 [32:57<18:57, 11.38s/it]\n",
      "{'loss': 0.6959, 'grad_norm': 8.1875, 'learning_rate': 2.05761316872428e-07, 'rewards/chosen': 0.0022046994417905807, 'rewards/rejected': 0.007593025919049978, 'rewards/accuracies': 0.375, 'rewards/margins': -0.00538832601159811, 'logps/rejected': -213.2236328125, 'logps/chosen': -212.08474731445312, 'logits/rejected': -2.7086830139160156, 'logits/chosen': -2.7075371742248535, 'epoch': 1.26}\n",
      "63%|██████▎   | 170/270 [32:57<18:57, 11.38s/it]\n",
      "63%|██████▎   | 171/270 [33:11<19:47, 11.99s/it]\n",
      "64%|██████▎   | 172/270 [33:20<18:02, 11.05s/it]\n",
      "64%|██████▍   | 173/270 [33:29<16:59, 10.51s/it]\n",
      "64%|██████▍   | 174/270 [33:38<15:54,  9.94s/it]\n",
      "65%|██████▍   | 175/270 [33:50<17:05, 10.80s/it]\n",
      "65%|██████▌   | 176/270 [34:03<17:35, 11.22s/it]\n",
      "66%|██████▌   | 177/270 [34:12<16:41, 10.77s/it]\n",
      "66%|██████▌   | 178/270 [34:24<17:03, 11.13s/it]\n",
      "66%|██████▋   | 179/270 [34:32<15:28, 10.20s/it]\n",
      "67%|██████▋   | 180/270 [34:44<16:08, 10.76s/it]\n",
      "{'loss': 0.6946, 'grad_norm': 7.21875, 'learning_rate': 1.8518518518518516e-07, 'rewards/chosen': 0.0027343849651515484, 'rewards/rejected': 0.005112596787512302, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': -0.0023782099597156048, 'logps/rejected': -244.73526000976562, 'logps/chosen': -216.75204467773438, 'logits/rejected': -2.7062366008758545, 'logits/chosen': -2.7295897006988525, 'epoch': 1.33}\n",
      "67%|██████▋   | 180/270 [34:44<16:08, 10.76s/it]\n",
      "67%|██████▋   | 181/270 [34:56<16:06, 10.86s/it]\n",
      "67%|██████▋   | 182/270 [35:08<16:29, 11.24s/it]\n",
      "68%|██████▊   | 183/270 [35:19<16:25, 11.33s/it]\n",
      "68%|██████▊   | 184/270 [35:29<15:28, 10.80s/it]\n",
      "69%|██████▊   | 185/270 [35:40<15:39, 11.05s/it]\n",
      "69%|██████▉   | 186/270 [35:52<15:37, 11.16s/it]\n",
      "69%|██████▉   | 187/270 [36:05<16:06, 11.65s/it]\n",
      "70%|██████▉   | 188/270 [36:16<15:58, 11.68s/it]\n",
      "70%|███████   | 189/270 [36:27<15:19, 11.36s/it]\n",
      "70%|███████   | 190/270 [36:36<14:11, 10.65s/it]\n",
      "{'loss': 0.6935, 'grad_norm': 5.4375, 'learning_rate': 1.6460905349794237e-07, 'rewards/chosen': 0.0003129005490336567, 'rewards/rejected': 0.005531339906156063, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': -0.005218438804149628, 'logps/rejected': -252.24459838867188, 'logps/chosen': -309.71697998046875, 'logits/rejected': -2.5727591514587402, 'logits/chosen': -2.7906482219696045, 'epoch': 1.41}\n",
      "70%|███████   | 190/270 [36:36<14:11, 10.65s/it]\n",
      "71%|███████   | 191/270 [36:47<14:20, 10.89s/it]\n",
      "71%|███████   | 192/270 [36:58<14:10, 10.91s/it]\n",
      "71%|███████▏  | 193/270 [37:11<14:37, 11.39s/it]\n",
      "72%|███████▏  | 194/270 [37:21<13:57, 11.02s/it]\n",
      "72%|███████▏  | 195/270 [37:36<15:16, 12.22s/it]\n",
      "73%|███████▎  | 196/270 [37:44<13:22, 10.85s/it]\n",
      "73%|███████▎  | 197/270 [37:52<12:17, 10.10s/it]\n",
      "73%|███████▎  | 198/270 [38:03<12:29, 10.41s/it]\n",
      "74%|███████▎  | 199/270 [38:15<12:56, 10.94s/it]\n",
      "74%|███████▍  | 200/270 [38:26<12:33, 10.77s/it]\n",
      "{'loss': 0.6895, 'grad_norm': 6.03125, 'learning_rate': 1.4403292181069958e-07, 'rewards/chosen': 0.012092581950128078, 'rewards/rejected': 0.003955535590648651, 'rewards/accuracies': 0.5249999761581421, 'rewards/margins': 0.008137045428156853, 'logps/rejected': -290.2796630859375, 'logps/chosen': -347.66259765625, 'logits/rejected': -2.6356277465820312, 'logits/chosen': -2.8149969577789307, 'epoch': 1.48}\n",
      "74%|███████▍  | 200/270 [38:26<12:33, 10.77s/it]\n",
      "74%|███████▍  | 201/270 [38:37<12:41, 11.04s/it]\n",
      "75%|███████▍  | 202/270 [38:47<12:10, 10.74s/it]\n",
      "75%|███████▌  | 203/270 [38:59<12:26, 11.15s/it]\n",
      "76%|███████▌  | 204/270 [39:13<13:05, 11.90s/it]\n",
      "76%|███████▌  | 205/270 [39:23<12:09, 11.22s/it]\n",
      "76%|███████▋  | 206/270 [39:32<11:23, 10.67s/it]\n",
      "77%|███████▋  | 207/270 [39:42<10:48, 10.29s/it]\n",
      "77%|███████▋  | 208/270 [39:54<11:09, 10.80s/it]\n",
      "77%|███████▋  | 209/270 [40:06<11:24, 11.23s/it]\n",
      "78%|███████▊  | 210/270 [40:14<10:15, 10.25s/it]\n",
      "{'loss': 0.6944, 'grad_norm': 5.4375, 'learning_rate': 1.2345679012345677e-07, 'rewards/chosen': -0.0009426262113265693, 'rewards/rejected': 0.002183990553021431, 'rewards/accuracies': 0.44999998807907104, 'rewards/margins': -0.0031266161240637302, 'logps/rejected': -297.54437255859375, 'logps/chosen': -294.716552734375, 'logits/rejected': -2.637568473815918, 'logits/chosen': -2.6939549446105957, 'epoch': 1.56}\n",
      "78%|███████▊  | 210/270 [40:14<10:15, 10.25s/it]\n",
      "78%|███████▊  | 211/270 [40:27<10:53, 11.08s/it]\n",
      "79%|███████▊  | 212/270 [40:38<10:38, 11.02s/it]\n",
      "79%|███████▉  | 213/270 [40:50<10:59, 11.57s/it]\n",
      "79%|███████▉  | 214/270 [41:01<10:36, 11.36s/it]\n",
      "80%|███████▉  | 215/270 [41:15<10:58, 11.96s/it]\n",
      "80%|████████  | 216/270 [41:26<10:26, 11.60s/it]\n",
      "80%|████████  | 217/270 [41:37<10:20, 11.71s/it]\n",
      "81%|████████  | 218/270 [41:47<09:42, 11.20s/it]\n",
      "81%|████████  | 219/270 [41:58<09:18, 10.96s/it]\n",
      "81%|████████▏ | 220/270 [42:08<09:01, 10.83s/it]\n",
      "{'loss': 0.684, 'grad_norm': 7.96875, 'learning_rate': 1.02880658436214e-07, 'rewards/chosen': 0.024735039100050926, 'rewards/rejected': -0.003234162461012602, 'rewards/accuracies': 0.75, 'rewards/margins': 0.027969202026724815, 'logps/rejected': -311.0480041503906, 'logps/chosen': -332.8739013671875, 'logits/rejected': -2.6282384395599365, 'logits/chosen': -2.6832661628723145, 'epoch': 1.63}\n",
      "81%|████████▏ | 220/270 [42:08<09:01, 10.83s/it]\n",
      "82%|████████▏ | 221/270 [42:18<08:39, 10.59s/it]\n",
      "82%|████████▏ | 222/270 [42:28<08:07, 10.15s/it]\n",
      "83%|████████▎ | 223/270 [42:39<08:18, 10.60s/it]\n",
      "83%|████████▎ | 224/270 [42:52<08:44, 11.40s/it]\n",
      "83%|████████▎ | 225/270 [43:02<08:10, 10.90s/it]\n",
      "84%|████████▎ | 226/270 [43:14<08:14, 11.23s/it]\n",
      "84%|████████▍ | 227/270 [43:26<08:08, 11.35s/it]\n",
      "84%|████████▍ | 228/270 [43:37<07:54, 11.29s/it]\n",
      "85%|████████▍ | 229/270 [43:49<07:49, 11.44s/it]\n",
      "85%|████████▌ | 230/270 [43:59<07:19, 10.98s/it]\n",
      "{'loss': 0.6895, 'grad_norm': 6.71875, 'learning_rate': 8.230452674897118e-08, 'rewards/chosen': 0.01544759888201952, 'rewards/rejected': 0.002372145187109709, 'rewards/accuracies': 0.625, 'rewards/margins': 0.013075453229248524, 'logps/rejected': -284.52655029296875, 'logps/chosen': -281.92974853515625, 'logits/rejected': -2.6310782432556152, 'logits/chosen': -2.793661117553711, 'epoch': 1.7}\n",
      "85%|████████▌ | 230/270 [43:59<07:19, 10.98s/it]\n",
      "86%|████████▌ | 231/270 [44:09<07:03, 10.86s/it]\n",
      "86%|████████▌ | 232/270 [44:24<07:42, 12.17s/it]\n",
      "86%|████████▋ | 233/270 [44:35<07:14, 11.73s/it]\n",
      "87%|████████▋ | 234/270 [44:45<06:46, 11.30s/it]\n",
      "87%|████████▋ | 235/270 [44:58<06:43, 11.54s/it]\n",
      "87%|████████▋ | 236/270 [45:06<06:02, 10.65s/it]\n",
      "88%|████████▊ | 237/270 [45:17<05:49, 10.60s/it]\n",
      "88%|████████▊ | 238/270 [45:27<05:33, 10.41s/it]\n",
      "89%|████████▊ | 239/270 [45:38<05:31, 10.70s/it]\n",
      "89%|████████▉ | 240/270 [45:49<05:20, 10.68s/it]\n",
      "{'loss': 0.6922, 'grad_norm': 6.25, 'learning_rate': 6.172839506172839e-08, 'rewards/chosen': -0.002105365740135312, 'rewards/rejected': -0.006882362999022007, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.004776997026056051, 'logps/rejected': -316.3293151855469, 'logps/chosen': -318.2934875488281, 'logits/rejected': -2.550994396209717, 'logits/chosen': -2.6808829307556152, 'epoch': 1.78}\n",
      "89%|████████▉ | 240/270 [45:49<05:20, 10.68s/it]\n",
      "89%|████████▉ | 241/270 [45:58<05:02, 10.42s/it]\n",
      "90%|████████▉ | 242/270 [46:10<05:04, 10.86s/it]\n",
      "90%|█████████ | 243/270 [46:23<05:08, 11.43s/it]\n",
      "90%|█████████ | 244/270 [46:33<04:44, 10.94s/it]\n",
      "91%|█████████ | 245/270 [46:46<04:47, 11.48s/it]\n",
      "91%|█████████ | 246/270 [47:01<05:03, 12.64s/it]\n",
      "91%|█████████▏| 247/270 [47:11<04:30, 11.75s/it]\n",
      "92%|█████████▏| 248/270 [47:22<04:14, 11.59s/it]\n",
      "92%|█████████▏| 249/270 [47:34<04:04, 11.65s/it]\n",
      "93%|█████████▎| 250/270 [47:46<03:58, 11.91s/it]\n",
      "{'loss': 0.6905, 'grad_norm': 6.90625, 'learning_rate': 4.115226337448559e-08, 'rewards/chosen': 0.013742655515670776, 'rewards/rejected': -0.006491709500551224, 'rewards/accuracies': 0.6499999761581421, 'rewards/margins': 0.02023436687886715, 'logps/rejected': -243.4119415283203, 'logps/chosen': -326.36651611328125, 'logits/rejected': -2.4630134105682373, 'logits/chosen': -2.755586624145508, 'epoch': 1.85}\n",
      "93%|█████████▎| 250/270 [47:46<03:58, 11.91s/it]\n",
      "93%|█████████▎| 251/270 [47:57<03:41, 11.64s/it]\n",
      "93%|█████████▎| 252/270 [48:09<03:30, 11.67s/it]\n",
      "94%|█████████▎| 253/270 [48:19<03:12, 11.33s/it]\n",
      "94%|█████████▍| 254/270 [48:31<03:00, 11.30s/it]\n",
      "94%|█████████▍| 255/270 [48:43<02:55, 11.68s/it]\n",
      "95%|█████████▍| 256/270 [48:56<02:46, 11.87s/it]\n",
      "95%|█████████▌| 257/270 [49:07<02:32, 11.71s/it]\n",
      "96%|█████████▌| 258/270 [49:20<02:25, 12.09s/it]\n",
      "96%|█████████▌| 259/270 [49:31<02:09, 11.80s/it]\n",
      "96%|█████████▋| 260/270 [49:43<01:59, 11.99s/it]\n",
      "{'loss': 0.692, 'grad_norm': 7.78125, 'learning_rate': 2.0576131687242796e-08, 'rewards/chosen': 0.005465125199407339, 'rewards/rejected': 0.003721538232639432, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': 0.0017435885965824127, 'logps/rejected': -274.20050048828125, 'logps/chosen': -333.7831726074219, 'logits/rejected': -2.6236019134521484, 'logits/chosen': -2.677520751953125, 'epoch': 1.93}\n",
      "96%|█████████▋| 260/270 [49:43<01:59, 11.99s/it]\n",
      "97%|█████████▋| 261/270 [49:56<01:50, 12.28s/it]\n",
      "97%|█████████▋| 262/270 [50:07<01:33, 11.66s/it]\n",
      "97%|█████████▋| 263/270 [50:19<01:22, 11.74s/it]\n",
      "98%|█████████▊| 264/270 [50:27<01:05, 10.88s/it]\n",
      "98%|█████████▊| 265/270 [50:37<00:52, 10.45s/it]\n",
      "99%|█████████▊| 266/270 [50:47<00:40, 10.23s/it]\n",
      "99%|█████████▉| 267/270 [50:57<00:30, 10.24s/it]\n",
      "99%|█████████▉| 268/270 [51:07<00:20, 10.28s/it]\n",
      "100%|█████████▉| 269/270 [51:21<00:11, 11.34s/it]\n",
      "[INFO|trainer.py:3719] 2024-05-31 17:18:36,357 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-31 17:18:36,357 >>   Num examples = 120\n",
      "[INFO|trainer.py:3719] 2024-05-31 17:18:36,357 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-31 17:18:36,357 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 17:18:36,357 >>   Batch size = 1\n",
      "[INFO|trainer.py:3724] 2024-05-31 17:18:36,357 >>   Batch size = 1\n",
      "100%|██████████| 270/270 [51:32<00:00, 11.30s/it]\n",
      "{'loss': 0.6857, 'grad_norm': 9.0625, 'learning_rate': 0.0, 'rewards/chosen': 0.008963456377387047, 'rewards/rejected': -0.0014040280366316438, 'rewards/accuracies': 0.574999988079071, 'rewards/margins': 0.01036748569458723, 'logps/rejected': -223.3091278076172, 'logps/chosen': -283.56817626953125, 'logits/rejected': -2.6553473472595215, 'logits/chosen': -2.756685972213745, 'epoch': 2.0}\n",
      "100%|██████████| 270/270 [51:32<00:00, 11.30s/it]\n",
      "[INFO|trainer.py:3719] 2024-05-31 17:18:36,359 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3719] 2024-05-31 17:18:36,359 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-31 17:18:36,359 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 17:18:36,359 >>   Batch size = 1\n",
      "[INFO|trainer.py:3721] 2024-05-31 17:18:36,359 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 17:18:36,359 >>   Batch size = 1\n",
      "0%|          | 0/60 [00:00<?, ?it/s]#033[A\n",
      "3%|▎         | 2/60 [00:01<00:31,  1.87it/s]#033[A\n",
      "5%|▌         | 3/60 [00:04<01:40,  1.75s/it]#033[A\n",
      "7%|▋         | 4/60 [00:05<01:27,  1.56s/it]#033[A\n",
      "8%|▊         | 5/60 [00:07<01:32,  1.68s/it]#033[A\n",
      "10%|█         | 6/60 [00:08<01:19,  1.47s/it]#033[A\n",
      "12%|█▏        | 7/60 [00:09<01:13,  1.39s/it]#033[A\n",
      "13%|█▎        | 8/60 [00:10<01:02,  1.19s/it]#033[A\n",
      "15%|█▌        | 9/60 [00:11<01:01,  1.20s/it]#033[A\n",
      "17%|█▋        | 10/60 [00:13<01:03,  1.27s/it]#033[A\n",
      "18%|█▊        | 11/60 [00:15<01:17,  1.59s/it]#033[A\n",
      "20%|██        | 12/60 [00:16<01:07,  1.40s/it]#033[A\n",
      "22%|██▏       | 13/60 [00:18<01:18,  1.67s/it]#033[A\n",
      "23%|██▎       | 14/60 [00:19<01:04,  1.41s/it]#033[A\n",
      "25%|██▌       | 15/60 [00:21<01:12,  1.60s/it]#033[A\n",
      "27%|██▋       | 16/60 [00:23<01:15,  1.71s/it]#033[A\n",
      "28%|██▊       | 17/60 [00:25<01:10,  1.64s/it]#033[A\n",
      "30%|███       | 18/60 [00:26<00:58,  1.40s/it]#033[A\n",
      "32%|███▏      | 19/60 [00:27<00:59,  1.44s/it]#033[A\n",
      "33%|███▎      | 20/60 [00:29<00:58,  1.45s/it]#033[A\n",
      "35%|███▌      | 21/60 [00:30<00:54,  1.39s/it]#033[A\n",
      "37%|███▋      | 22/60 [00:32<01:02,  1.65s/it]#033[A\n",
      "38%|███▊      | 23/60 [00:33<00:56,  1.53s/it]#033[A\n",
      "40%|████      | 24/60 [00:34<00:49,  1.37s/it]#033[A\n",
      "42%|████▏     | 25/60 [00:36<00:46,  1.32s/it]#033[A\n",
      "43%|████▎     | 26/60 [00:38<00:58,  1.72s/it]#033[A\n",
      "45%|████▌     | 27/60 [00:39<00:47,  1.45s/it]#033[A\n",
      "47%|████▋     | 28/60 [00:41<00:53,  1.67s/it]#033[A\n",
      "48%|████▊     | 29/60 [00:42<00:47,  1.54s/it]#033[A\n",
      "50%|█████     | 30/60 [00:44<00:50,  1.70s/it]#033[A\n",
      "52%|█████▏    | 31/60 [00:46<00:48,  1.67s/it]#033[A\n",
      "53%|█████▎    | 32/60 [00:47<00:44,  1.59s/it]#033[A\n",
      "55%|█████▌    | 33/60 [00:49<00:43,  1.60s/it]#033[A\n",
      "57%|█████▋    | 34/60 [00:50<00:35,  1.35s/it]#033[A\n",
      "58%|█████▊    | 35/60 [00:51<00:32,  1.29s/it]#033[A\n",
      "60%|██████    | 36/60 [00:52<00:28,  1.19s/it]#033[A\n",
      "62%|██████▏   | 37/60 [00:55<00:39,  1.71s/it]#033[A\n",
      "63%|██████▎   | 38/60 [00:56<00:34,  1.58s/it]#033[A\n",
      "65%|██████▌   | 39/60 [00:58<00:35,  1.71s/it]#033[A\n",
      "67%|██████▋   | 40/60 [01:00<00:32,  1.63s/it]#033[A\n",
      "68%|██████▊   | 41/60 [01:02<00:33,  1.77s/it]#033[A\n",
      "70%|███████   | 42/60 [01:04<00:34,  1.90s/it]#033[A\n",
      "72%|███████▏  | 43/60 [01:05<00:30,  1.77s/it]#033[A\n",
      "73%|███████▎  | 44/60 [01:07<00:25,  1.58s/it]#033[A\n",
      "75%|███████▌  | 45/60 [01:08<00:23,  1.58s/it]#033[A\n",
      "77%|███████▋  | 46/60 [01:10<00:21,  1.55s/it]#033[A\n",
      "78%|███████▊  | 47/60 [01:11<00:18,  1.39s/it]#033[A\n",
      "80%|████████  | 48/60 [01:12<00:18,  1.51s/it]#033[A\n",
      "82%|████████▏ | 49/60 [01:14<00:15,  1.44s/it]#033[A\n",
      "83%|████████▎ | 50/60 [01:15<00:13,  1.38s/it]#033[A\n",
      "85%|████████▌ | 51/60 [01:17<00:14,  1.62s/it]#033[A\n",
      "87%|████████▋ | 52/60 [01:18<00:12,  1.55s/it]#033[A\n",
      "88%|████████▊ | 53/60 [01:20<00:09,  1.40s/it]#033[A\n",
      "90%|█████████ | 54/60 [01:21<00:08,  1.35s/it]#033[A\n",
      "92%|█████████▏| 55/60 [01:22<00:06,  1.31s/it]#033[A\n",
      "93%|█████████▎| 56/60 [01:23<00:04,  1.16s/it]#033[A\n",
      "95%|█████████▌| 57/60 [01:24<00:03,  1.19s/it]#033[A\n",
      "97%|█████████▋| 58/60 [01:25<00:02,  1.18s/it]#033[A\n",
      "98%|█████████▊| 59/60 [01:27<00:01,  1.30s/it]#033[A\n",
      "100%|██████████| 60/60 [01:28<00:00,  1.33s/it]#033[A\n",
      "#033[A\n",
      "{'eval_loss': 0.6918022036552429, 'eval_runtime': 90.3075, 'eval_samples_per_second': 1.329, 'eval_steps_per_second': 0.664, 'eval_rewards/chosen': 0.0058076027780771255, 'eval_rewards/rejected': 0.005605741869658232, 'eval_rewards/accuracies': 0.46666666865348816, 'eval_rewards/margins': 0.00020186159235890955, 'eval_logps/rejected': -293.6361389160156, 'eval_logps/chosen': -272.3629455566406, 'eval_logits/rejected': -2.6939103603363037, 'eval_logits/chosen': -2.7633819580078125, 'epoch': 2.0}\n",
      "100%|██████████| 270/270 [53:03<00:00, 11.30s/it]\n",
      "100%|██████████| 60/60 [01:28<00:00,  1.33s/it]#033[A\n",
      "#033[A\n",
      "[INFO|trainer.py:2329] 2024-05-31 17:20:06,669 >> \n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "[INFO|trainer.py:2329] 2024-05-31 17:20:06,669 >> \n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "{'train_runtime': 3188.0872, 'train_samples_per_second': 0.678, 'train_steps_per_second': 0.085, 'train_loss': 0.6916016666977494, 'epoch': 2.0}\n",
      "100%|██████████| 270/270 [53:03<00:00, 11.30s/it]\n",
      "100%|██████████| 270/270 [53:03<00:00, 11.79s/it]\n",
      "***** train metrics *****\n",
      "epoch                    =        2.0\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =     0.6916\n",
      "  train_runtime            = 0:53:08.08\n",
      "  train_samples            =       1080\n",
      "  train_samples_per_second =      0.678\n",
      "  train_steps_per_second   =      0.085\n",
      "2024-05-31 17:20:06 - INFO - __main__ - *** Training complete ***\n",
      "2024-05-31 17:20:06 - INFO - __main__ - *** Save model ***\n",
      "[INFO|trainer.py:3410] 2024-05-31 17:20:06,674 >> Saving model checkpoint to /opt/ml/model\n",
      "[INFO|trainer.py:3410] 2024-05-31 17:20:06,674 >> Saving model checkpoint to /opt/ml/model\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /opt/ml/input/data/model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-05-31 17:20:06,812 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-05-31 17:20:06,812 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-05-31 17:20:06,812 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-05-31 17:20:06,812 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\n",
      "2024-05-31 17:20:06 - INFO - __main__ - Model saved to /opt/ml/model\n",
      "[INFO|modelcard.py:450] 2024-05-31 17:20:06,842 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'dataset': {'name': '/opt/ml/input/data/train', 'type': '/opt/ml/input/data/train'}}\n",
      "[INFO|modelcard.py:450] 2024-05-31 17:20:06,842 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'dataset': {'name': '/opt/ml/input/data/train', 'type': '/opt/ml/input/data/train'}}\n",
      "[INFO|configuration_utils.py:472] 2024-05-31 17:20:06,847 >> Configuration saved in /opt/ml/model/config.json\n",
      "[INFO|configuration_utils.py:472] 2024-05-31 17:20:06,847 >> Configuration saved in /opt/ml/model/config.json\n",
      "2024-05-31 17:20:06 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3719] 2024-05-31 17:20:06,849 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3719] 2024-05-31 17:20:06,849 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-31 17:20:06,849 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 17:20:06,849 >>   Batch size = 1\n",
      "[INFO|trainer.py:3721] 2024-05-31 17:20:06,849 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 17:20:06,849 >>   Batch size = 1\n",
      "[INFO|trainer.py:2329] 2024-05-31 17:20:06,666 >> \n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "[INFO|trainer.py:2329] 2024-05-31 17:20:06,666 >> \n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "2024-05-31 17:20:06 - INFO - __main__ - *** Training complete ***\n",
      "2024-05-31 17:20:06 - INFO - __main__ - *** Save model ***\n",
      "2024-05-31 17:20:06 - INFO - __main__ - Model saved to /opt/ml/model\n",
      "2024-05-31 17:20:06 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3719] 2024-05-31 17:20:06,672 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3719] 2024-05-31 17:20:06,672 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-05-31 17:20:06,672 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 17:20:06,672 >>   Batch size = 1\n",
      "[INFO|trainer.py:3721] 2024-05-31 17:20:06,672 >>   Num examples = 120\n",
      "[INFO|trainer.py:3724] 2024-05-31 17:20:06,672 >>   Batch size = 1\n",
      "0%|          | 0/60 [00:00<?, ?it/s]\n",
      "3%|▎         | 2/60 [00:00<00:25,  2.23it/s]\n",
      "5%|▌         | 3/60 [00:04<01:37,  1.70s/it]\n",
      "7%|▋         | 4/60 [00:05<01:25,  1.53s/it]\n",
      "8%|▊         | 5/60 [00:07<01:31,  1.66s/it]\n",
      "10%|█         | 6/60 [00:08<01:18,  1.45s/it]\n",
      "12%|█▏        | 7/60 [00:09<01:13,  1.38s/it]\n",
      "13%|█▎        | 8/60 [00:10<01:01,  1.19s/it]\n",
      "15%|█▌        | 9/60 [00:11<01:00,  1.19s/it]\n",
      "17%|█▋        | 10/60 [00:13<01:03,  1.27s/it]\n",
      "18%|█▊        | 11/60 [00:15<01:17,  1.59s/it]\n",
      "20%|██        | 12/60 [00:16<01:07,  1.40s/it]\n",
      "22%|██▏       | 13/60 [00:18<01:18,  1.67s/it]\n",
      "23%|██▎       | 14/60 [00:19<01:04,  1.41s/it]\n",
      "25%|██▌       | 15/60 [00:21<01:12,  1.60s/it]\n",
      "27%|██▋       | 16/60 [00:23<01:15,  1.71s/it]\n",
      "28%|██▊       | 17/60 [00:25<01:10,  1.64s/it]\n",
      "30%|███       | 18/60 [00:25<00:58,  1.39s/it]\n",
      "32%|███▏      | 19/60 [00:27<00:58,  1.44s/it]\n",
      "33%|███▎      | 20/60 [00:28<00:58,  1.45s/it]\n",
      "35%|███▌      | 21/60 [00:30<00:54,  1.39s/it]\n",
      "37%|███▋      | 22/60 [00:32<01:02,  1.65s/it]\n",
      "38%|███▊      | 23/60 [00:33<00:56,  1.53s/it]\n",
      "40%|████      | 24/60 [00:34<00:49,  1.37s/it]\n",
      "42%|████▏     | 25/60 [00:35<00:46,  1.32s/it]\n",
      "43%|████▎     | 26/60 [00:38<00:58,  1.72s/it]\n",
      "45%|████▌     | 27/60 [00:39<00:47,  1.45s/it]\n",
      "47%|████▋     | 28/60 [00:41<00:53,  1.67s/it]\n",
      "48%|████▊     | 29/60 [00:42<00:47,  1.54s/it]\n",
      "50%|█████     | 30/60 [00:44<00:50,  1.69s/it]\n",
      "52%|█████▏    | 31/60 [00:46<00:48,  1.68s/it]\n",
      "53%|█████▎    | 32/60 [00:47<00:44,  1.59s/it]\n",
      "55%|█████▌    | 33/60 [00:49<00:43,  1.60s/it]\n",
      "57%|█████▋    | 34/60 [00:50<00:35,  1.35s/it]\n",
      "58%|█████▊    | 35/60 [00:51<00:32,  1.29s/it]\n",
      "60%|██████    | 36/60 [00:52<00:28,  1.20s/it]\n",
      "62%|██████▏   | 37/60 [00:55<00:39,  1.71s/it]\n",
      "63%|██████▎   | 38/60 [00:56<00:34,  1.58s/it]\n",
      "65%|██████▌   | 39/60 [00:58<00:36,  1.71s/it]\n",
      "67%|██████▋   | 40/60 [00:59<00:32,  1.63s/it]\n",
      "68%|██████▊   | 41/60 [01:02<00:33,  1.77s/it]\n",
      "70%|███████   | 42/60 [01:04<00:34,  1.90s/it]\n",
      "72%|███████▏  | 43/60 [01:05<00:30,  1.77s/it]\n",
      "73%|███████▎  | 44/60 [01:06<00:25,  1.58s/it]\n",
      "75%|███████▌  | 45/60 [01:08<00:23,  1.58s/it]\n",
      "77%|███████▋  | 46/60 [01:09<00:21,  1.55s/it]\n",
      "78%|███████▊  | 47/60 [01:10<00:18,  1.39s/it]\n",
      "80%|████████  | 48/60 [01:12<00:18,  1.51s/it]\n",
      "82%|████████▏ | 49/60 [01:13<00:15,  1.44s/it]\n",
      "83%|████████▎ | 50/60 [01:15<00:13,  1.38s/it]\n",
      "85%|████████▌ | 51/60 [01:17<00:14,  1.62s/it]\n",
      "87%|████████▋ | 52/60 [01:18<00:12,  1.55s/it]\n",
      "88%|████████▊ | 53/60 [01:19<00:09,  1.40s/it]\n",
      "90%|█████████ | 54/60 [01:21<00:08,  1.35s/it]\n",
      "92%|█████████▏| 55/60 [01:22<00:06,  1.31s/it]\n",
      "93%|█████████▎| 56/60 [01:23<00:04,  1.16s/it]\n",
      "95%|█████████▌| 57/60 [01:24<00:03,  1.19s/it]\n",
      "97%|█████████▋| 58/60 [01:25<00:02,  1.18s/it]\n",
      "98%|█████████▊| 59/60 [01:27<00:01,  1.30s/it]\n",
      "100%|██████████| 60/60 [01:28<00:00,  1.33s/it]\n",
      "100%|██████████| 60/60 [01:28<00:00,  1.47s/it]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_logits/chosen      =    -2.7634\n",
      "  eval_logits/rejected    =    -2.6939\n",
      "  eval_logps/chosen       =  -272.3629\n",
      "  eval_logps/rejected     =  -293.6361\n",
      "  eval_loss               =     0.6918\n",
      "  eval_rewards/accuracies =     0.4667\n",
      "  eval_rewards/chosen     =     0.0058\n",
      "eval_rewards/margins    =     0.0002\n",
      "  eval_rewards/rejected   =     0.0056\n",
      "  eval_runtime            = 0:01:30.12\n",
      "  eval_samples            =        120\n",
      "  eval_samples_per_second =      1.332\n",
      "eval_steps_per_second   =      0.666\n",
      "2024-05-31 17:21:36 - INFO - __main__ - *** Training complete! ***\n",
      "2024-05-31 17:21:36 - INFO - __main__ - *** Training complete! ***\n",
      "2024-05-31 17:21:42,800 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-31 17:21:42,800 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-31 17:21:42,800 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "2024-05-31 17:21:42,796 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-31 17:21:42,796 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-31 17:21:42,796 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2024-05-31 17:22:04 Uploading - Uploading generated training model\n",
      "2024-05-31 17:22:04 Completed - Resource retained for reuse\n",
      "Training seconds: 7548\n",
      "Billable seconds: 7548\n"
     ]
    }
   ],
   "source": [
    "dpo_estimator.fit(\n",
    "    {\n",
    "        \"model\": s3_model_location,       # base Mistral 7B model \n",
    "        \"sft_model\": sft_model_location,  # fine-tuned model from the previous step\n",
    "        \"train\": f\"{s3_data}/dpo_split\",  # preference training data\n",
    "    }\n",
    ")\n",
    "dpo_model_location = dpo_estimator.model_data[\"S3DataSource\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Run these cells to remove the data and model artifacts from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive $s3_model_location \n",
    "!aws s3 rm --recursive $sft_model_location\n",
    "!aws s3 rm --recursive $dpo_model_location\n",
    "!aws s3 rm --recursive $s3_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
