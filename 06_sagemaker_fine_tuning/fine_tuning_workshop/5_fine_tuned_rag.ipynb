{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Retrieval Augment Generation Solution with a Fine Tuned Model\n",
    "Now that we have a fine-tuned model, it's time to revisit our retrieval augment generation solution. In this notebook, we will re-implement the initial pipeline but this time using the fine-tuned model to generate the responses. We will then evaluate the performance of the fine-tuned model and compare it with the initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v2:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    \"mistral.mistral-7b-instruct-v0:2\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from rich import print as rprint\n",
    "import json\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_aws.llms import SagemakerEndpoint\n",
    "from langchain_aws.chat_models import BedrockChat\n",
    "from langchain_aws.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import boto3\n",
    "from typing import Dict\n",
    "\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "data_path = Path(\"data/prepared_data\")\n",
    "train_data = (data_path / \"prepared_data_train.jsonl\").read_text().splitlines()\n",
    "test_data = (data_path / \"prepared_data_test.jsonl\").read_text().splitlines()\n",
    "\n",
    "doc_ids = []\n",
    "documents = []\n",
    "\n",
    "# Create a list of LangChain documents that can then be used to ingest into a vector store\n",
    "\n",
    "for record in chain(train_data, test_data):\n",
    "    json_record = json.loads(record)\n",
    "    if json_record[\"ref_doc_id\"] not in doc_ids:\n",
    "        doc_ids.append(json_record[\"ref_doc_id\"])\n",
    "        doc = Document(page_content=json_record[\"context\"], metadata=json_record[\"section_metadata\"])\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session=boto3.session.Session()\n",
    "\n",
    "bedrock_runtime = boto3_session.client(\"bedrock-runtime\")\n",
    "smr_client = boto3_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "embedding_modelId = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "embed_model = BedrockEmbeddings(\n",
    "    model_id=embedding_modelId,\n",
    "    model_kwargs={\"dimensions\": 1024, \"normalize\": True},\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "query = \"Do I really need to fine-tune the large language models?\"\n",
    "response = embed_model.embed_query(query)\n",
    "rprint(f\"Generated an embedding with {len(response)} dimensions. Sample of first 10 dimensions:\\n\", response[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can resuse the vector db from the initial model since we're keeping embeddings the same\n",
    "vector_store_file = \"baseline_rag_vec_db.pkl\"\n",
    "\n",
    "if not Path(vector_store_file).exists():\n",
    "    rprint(f\"Vector store file {vector_store_file} does not exist. Will create a new vector store.\")\n",
    "    CREATE_NEW = True\n",
    "else:\n",
    "    rprint(f\"Vector store file {vector_store_file} already exists. Delete it or change the file name above to create a new vector store.\")\n",
    "    CREATE_NEW = False \n",
    "\n",
    "if CREATE_NEW:\n",
    "    vec_db = FAISS.from_documents(documents, embed_model)\n",
    "    pickle.dump(vec_db.serialize_to_bytes(), open(vector_store_file, \"wb\"))\n",
    "    \n",
    "else:\n",
    "    if not Path(vector_store_file).exists():\n",
    "        raise FileNotFoundError(f\"Vector store file {vector_store_file} not found. Set CREATE_NEW to True to create a new vector store.\")\n",
    "    \n",
    "    vector_db_buff = BytesIO(pickle.load(open(vector_store_file, \"rb\")))\n",
    "    vec_db = FAISS.deserialize_from_bytes(serialized=vector_db_buff.read(), embeddings=embed_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "k = 3\n",
    "faiss_retriever = vec_db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "bm_25 = BM25Retriever.from_documents(documents)\n",
    "bm_25.k = k\n",
    "\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, bm_25], weights=[0.75, 0.25]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SageMaker endpoints with LangChain\n",
    "To use a SageMaker endpoint with LangChain we need to implement a `ContentHandler` class that will handle the preprocessing of the input data and the postprocessing of the output data. The primary reason for this is unlike Bedrock, a SageMaker API may have different and inconsistent input and output payload formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_endpoint_config_path = Path(\"endpoint_config.json\")\n",
    "if sagemaker_endpoint_config_path.exists():\n",
    "    endpoint_name = json.loads(sagemaker_endpoint_config_path.read_text())[\"endpoint_name\"]\n",
    "else:\n",
    "    rprint(f\"[bold red]Endpoint config file {sagemaker_endpoint_config_path} not found. Please make sure that you ran the prior training and deployment notebooks[/bold red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        \"\"\"formats the request payload for the model endpoint\"\"\"\n",
    "        \n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        \"\"\"extracts the generated answer from the endpoint response\"\"\"\n",
    "        \n",
    "        output = output.read().decode(\"utf-8\")\n",
    "        generated_answer = json.loads(output)[\"generated_text\"]\n",
    "        return generated_answer\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "tuned_llm = SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        client=smr_client,\n",
    "        model_kwargs={\"temperature\": 0, \"max_new_tokens\": 500},\n",
    "        content_handler=content_handler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the configured LLM we can reimplement the retrieval augment generation pipeline using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "tuned_prompt_template = \"[INST] You are a Banking Regulations expert.\\nGiven this context\\nCONTEXT\\n{context}\\n Answer this question\\nQuestion: {question} [/INST]\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(tuned_prompt_template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "generate_tuned_answer = {\"answer\": prompt | tuned_llm | output_parser,\n",
    "                   \"context\": itemgetter(\"context\")}\n",
    "\n",
    "tuned_chain = setup_and_retrieval | generate_tuned_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample response \n",
    "\n",
    "sample_record = json.loads(test_data[150])\n",
    "sample_question = sample_record[\"question\"]\n",
    "sample_answer = sample_record[\"answer\"]\n",
    "rprint(f\"Sample question: {sample_question}\")\n",
    "response = tuned_chain.invoke(sample_question)\n",
    "generated_answer = response[\"answer\"]\n",
    "rprint(f\"\\nGenerated answer: {generated_answer}\")\n",
    "rprint(f\"\\nGround truth answer: {sample_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to speed up the inference and evaluation process\n",
    "\n",
    "async def generate_answer_async(rag_chain, example):\n",
    "    example = json.loads(example)\n",
    "    response = await rag_chain.ainvoke(example[\"question\"])\n",
    "    contexts = [doc.page_content for doc in response[\"context\"]]\n",
    "    row = {\"question\": example[\"question\"], \"answer\": response[\"answer\"], \"contexts\": contexts, \"ground_truth\": example[\"answer\"]}\n",
    "    return row\n",
    "\n",
    "async def evaluate_llm_async(metric, rows):\n",
    "    evals = [metric.acall(row) for row in rows]\n",
    "    # event_loop = asyncio.get_event_loop()\n",
    "    evals = await asyncio.gather(*evals)\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLE_LLM_EVALUATION = 100\n",
    "eval_rows = []\n",
    "for example in test_data[:NUM_SAMPLE_LLM_EVALUATION]:\n",
    "    eval_rows.append(generate_answer_async(tuned_chain, example))\n",
    "event_loop = asyncio.get_event_loop()\n",
    "eval_data= event_loop.run_until_complete(asyncio.gather(*eval_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import faithfulness, answer_similarity, answer_relevancy, answer_correctness\n",
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "import math\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"12345\"\n",
    "eval_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0\n",
    "    },\n",
    "    client=bedrock_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_metric = EvaluatorChain(metric=faithfulness, llm=eval_llm, embeddings=embed_model)\n",
    "answer_relevancy_metric = EvaluatorChain(metric=answer_relevancy, llm=eval_llm, embeddings=embed_model)\n",
    "answer_similarity_metric = EvaluatorChain(metric=answer_similarity, llm=eval_llm, embeddings=embed_model)\n",
    "answer_correctness_metric = EvaluatorChain(metric=answer_correctness, llm=eval_llm, embeddings=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Faithfulness:**](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html) measure the extent to which the claims in the generated answer are supported by the context. It is calculated as the ratio of the number of claims in the generated answer that are supported by the context to the total number of claims in the generated answer. In other words it helps us detect hallucinations as we would expect all claims in the generated answer to be supported by the context.\n",
    "It does not reflect on the accuracy or correctness of the claims, only that they are supported by the context.\n",
    "\n",
    "**NOTE:** If you see a message `Failed to parse output. Returning None.` during the evaluation, it simply means that ragas was unable to parse the output from the model. This can happen if the model generates an output that is not in the expected format. These samples will be ignored when calculating the aggregate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_evals = event_loop.run_until_complete(evaluate_llm_async(faithfulness_metric, eval_data))\n",
    "faithfulness_scores = [eval[\"faithfulness\"] for eval in faithfulness_evals if not math.isnan(eval[\"faithfulness\"])]\n",
    "faithfulness_score = sum(faithfulness_scores) / len(faithfulness_scores)\n",
    "\n",
    "print(\"Faithfulness Score: \", faithfulness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Answer Relevancy:**](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html) attempts to measure how pertinent the generated answer is to the given prompt. It works by having the evaluator LLM generate synthetic questions based on the generated answer and then calculating the average semantic similarity between the given question and the synthetic questions. The idea is that a more complete and pertinent answer should yield synthetic questions that are more similar to the given question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_evals = event_loop.run_until_complete(evaluate_llm_async(answer_relevancy_metric, eval_data))\n",
    "relevancy_scores = [eval[\"answer_relevancy\"] for eval in relevancy_evals if not math.isnan(eval[\"answer_relevancy\"])]\n",
    "relevancy_score = sum(relevancy_scores) / len(relevancy_scores)\n",
    "\n",
    "print(\"Answer Relevancy Score: \", relevancy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Answer semantic similarity:**](https://docs.ragas.io/en/stable/concepts/metrics/semantic_similarity.html) measures the cosine similarity between the ground truth answer and the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_similarity_evals = event_loop.run_until_complete(evaluate_llm_async(answer_similarity_metric, eval_data[:10]))\n",
    "similarity_scores = [eval[\"answer_similarity\"] for eval in answer_similarity_evals if not math.isnan(eval[\"answer_similarity\"])]\n",
    "similarity_score = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "print(\"Answer Similarity Score: \", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Answer Correctness**](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html): Combines factual similarity assessed by the evaluator LLM with the semantic similarity between the generated answer and the ground truth. It is calculated as a weighted average of the factual similarity and the semantic similarity. Factual similarity is calculated similar to Faithfulness but also considers overlapping claims between the generated answer and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_correctness_evals = event_loop.run_until_complete(evaluate_llm_async(answer_correctness_metric, eval_data))\n",
    "correctness_scores = [eval[\"answer_correctness\"] for eval in answer_correctness_evals if not math.isnan(eval[\"answer_correctness\"])]\n",
    "correctness_score = sum(correctness_scores) / len(correctness_scores)\n",
    "\n",
    "print(\"Answer Correctness Score: \", correctness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_evals = {\n",
    "    \"faithfulness\": faithfulness_score,\n",
    "    \"relevancy\": relevancy_score,\n",
    "    \"similarity\": similarity_score,\n",
    "    \"correctness\": correctness_score,\n",
    "}\n",
    "original_evals = json.loads(Path(\"base_evaluation.json\").read_text())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_evals(tuned_evals, original_evals):\n",
    "    labels = list(tuned_evals.keys())\n",
    "    tuned_scores = list(tuned_evals.values())\n",
    "    original_scores = list(original_evals.values())\n",
    "\n",
    "    data = {\"Labels\": labels*2, \n",
    "            \"Scores\": tuned_scores + original_scores, \n",
    "            \"Type\": [\"Tuned\"]*len(tuned_scores) + [\"Original\"]*len(original_scores)}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    barplot = sns.barplot(x=\"Labels\", y=\"Scores\", hue=\"Type\", data=df)\n",
    "\n",
    "    for p in barplot.patches:\n",
    "        barplot.annotate(format(p.get_height(), '.3f'), \n",
    "                         (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                         ha = 'center', va = 'center', \n",
    "                         xytext = (0, 10), \n",
    "                         textcoords = 'offset points')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_evals(tuned_evals, original_evals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "sagemaker_session = sagemaker.Session()\n",
    "pred = Predictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "pred.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
