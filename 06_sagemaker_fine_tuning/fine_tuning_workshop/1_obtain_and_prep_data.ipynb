{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and preparing the data \n",
    "The first step of any data science project is to obtain, understand, and prepare the data. In this notebook, we will walk through the process of obtaining a practical real world dataset and prepare it for downstream purposes including building a Retrieval Augmented Generation (RAG) pipeline and fine tuning a large language model to improve the performance of the pipeline.\n",
    "\n",
    "### Usecase Overview\n",
    "The dataset we'll be using is comprised of [US Federal Banks and Banking Regulations](https://www.ecfr.gov/current/title-12). Our goal is to build a chatbot that will help users get answers to specific questions and understand the regulations and policies of the US Federal Banks. To simulate a real world scenario, we will obtain the raw data directly from the provided API and prepare it for downstream tasks. Once we have the data, we will build and evaluate an initial RAG pipeline and then proceed to improve on the pipeline by fine-tuning our own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and wrangle the data\n",
    "The first step is to download the data from the API and wrangle it into a format that can be used for downstream tasks. We will use the `requests` library to download the data. The data comes in an `XML` format, so we will use the `BeautifulSoup` library to parse the data and extract the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating base environment\n",
      "Base environment validated successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #4cc9f0; text-decoration-color: #4cc9f0; font-weight: bold\">Validating lab environment from requirements.txt</span> ‚ú®\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;76;201;240mValidating lab environment from requirements.txt\u001b[0m ‚ú®\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #e85d04; text-decoration-color: #e85d04; font-weight: bold; text-decoration: underline\">ENVIRONMENT STATUS</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> beautifulsoup4 is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> lxml is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> langchain-</span><span style=\"color: #008000; text-decoration-color: #008000\">aws</span><span style=\"color: #008000; text-decoration-color: #008000\">==</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.2</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">11</span><span style=\"color: #008000; text-decoration-color: #008000\"> is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> faiss-</span><span style=\"color: #008000; text-decoration-color: #008000\">cpu</span><span style=\"color: #008000; text-decoration-color: #008000\">==</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.8</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span><span style=\"color: #008000; text-decoration-color: #008000\"> is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> rank-bm25 is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">ragas</span><span style=\"color: #008000; text-decoration-color: #008000\">==</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.1</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">16</span><span style=\"color: #008000; text-decoration-color: #008000\"> is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> sagemaker&gt;=</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2.239</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span><span style=\"color: #008000; text-decoration-color: #008000\"> is installed</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> seaborn is installed</span>\n",
       "‚ùå <span style=\"color: #ef233c; text-decoration-color: #ef233c\">langchain</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\">==</span><span style=\"color: #ef233c; text-decoration-color: #ef233c; font-weight: bold\">0.3</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\">.</span><span style=\"color: #ef233c; text-decoration-color: #ef233c; font-weight: bold\">5</span><span style=\"color: #ef233c; text-decoration-color: #ef233c\"> is not installed</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;4;38;2;232;93;4mENVIRONMENT STATUS\u001b[0m\n",
       "‚úÖ \u001b[32m beautifulsoup4 is installed\u001b[0m\n",
       "‚úÖ \u001b[32m lxml is installed\u001b[0m\n",
       "‚úÖ \u001b[32m langchain-\u001b[0m\u001b[32maws\u001b[0m\u001b[32m==\u001b[0m\u001b[1;32m0.2\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m11\u001b[0m\u001b[32m is installed\u001b[0m\n",
       "‚úÖ \u001b[32m faiss-\u001b[0m\u001b[32mcpu\u001b[0m\u001b[32m==\u001b[0m\u001b[1;32m1.8\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m0\u001b[0m\u001b[32m is installed\u001b[0m\n",
       "‚úÖ \u001b[32m rank-bm25 is installed\u001b[0m\n",
       "‚úÖ \u001b[32m \u001b[0m\u001b[32mragas\u001b[0m\u001b[32m==\u001b[0m\u001b[1;32m0.1\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m16\u001b[0m\u001b[32m is installed\u001b[0m\n",
       "‚úÖ \u001b[32m sagemaker>=\u001b[0m\u001b[1;32m2.239\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m0\u001b[0m\u001b[32m is installed\u001b[0m\n",
       "‚úÖ \u001b[32m seaborn is installed\u001b[0m\n",
       "‚ùå \u001b[38;2;239;35;60mlangchain\u001b[0m\u001b[38;2;239;35;60m==\u001b[0m\u001b[1;38;2;239;35;60m0.3\u001b[0m\u001b[38;2;239;35;60m.\u001b[0m\u001b[1;38;2;239;35;60m5\u001b[0m\u001b[38;2;239;35;60m is not installed\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Installing missing libraries</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mInstalling missing libraries\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain==0.3.5 has been installed successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #a7c957; text-decoration-color: #a7c957\">All required libraries are installed.üéâ</span>\n",
       "<span style=\"color: #a7c957; text-decoration-color: #a7c957\">You may proceed with the lab! üöÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;167;201;87mAll required libraries are installed.üéâ\u001b[0m\n",
       "\u001b[38;2;167;201;87mYou may proceed with the lab! üöÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #e85d04; text-decoration-color: #e85d04; font-weight: bold; text-decoration: underline\">MODEL ACCESS STATUS</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> amazon.titan-embed-text-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2:0</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> mistral.mixtral-8x7b-instruct-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0:1</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> mistral.mistral-7b-instruct-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0:2</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "‚úÖ <span style=\"color: #008000; text-decoration-color: #008000\"> us.amazon.nova-lite-v</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1:0</span><span style=\"color: #008000; text-decoration-color: #008000\"> is accessible</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;4;38;2;232;93;4mMODEL ACCESS STATUS\u001b[0m\n",
       "‚úÖ \u001b[32m amazon.titan-embed-text-v\u001b[0m\u001b[1;32m2:0\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "‚úÖ \u001b[32m mistral.mixtral-8x7b-instruct-v\u001b[0m\u001b[1;32m0:1\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "‚úÖ \u001b[32m mistral.mistral-7b-instruct-v\u001b[0m\u001b[1;32m0:2\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "‚úÖ \u001b[32m us.amazon.nova-lite-v\u001b[0m\u001b[1;32m1:0\u001b[0m\u001b[32m is accessible\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #a7c957; text-decoration-color: #a7c957\">All required models are accessible.üéâ</span>\n",
       "<span style=\"color: #a7c957; text-decoration-color: #a7c957\">You may proceed with the lab! üöÄ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;167;201;87mAll required models are accessible.üéâ\u001b[0m\n",
       "\u001b[38;2;167;201;87mYou may proceed with the lab! üöÄ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v2:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    \"mistral.mistral-7b-instruct-v0:2\",\n",
    "    \"us.amazon.nova-lite-v1:0\"\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['/opt/conda/bin/python', 'mlflow_utils.py', ...>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# create an mlflow tracking server that can later be used to log experiments\n",
    "from mlflow_utils import create_mlflow_tracking_server\n",
    "subprocess.Popen([sys.executable, \"mlflow_utils.py\", \"--tracking-server-name\", \"workshop-mlflow-1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print as rprint\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "INCLUDED_CHAPTERS = [\"I\", \"II\", \"III\"]\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "# Download the raw data and save it to a file\n",
    "\n",
    "api_url = \"https://www.ecfr.gov\"\n",
    "api_path = \"/api/versioner/v1/full/2024-04-04/title-12.xml?chapter=I\"\n",
    "raw_data_path = data_path / \"raw\"\n",
    "raw_data_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "response = requests.get(api_url + api_path, timeout=300)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Failed to download the data. Status code: {response.status_code}\")\n",
    "else:\n",
    "    rprint(\"[green]Data downloaded successfully[/green]\")\n",
    "    with (raw_data_path / f\"raw_data.xml\").open(\"w\") as f:\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heading for each chapter is captured in a `<div3>` tag. We can use that information to split the xml into chapters. As this is a sizeable dataset, we will filter on only the first 3 chapters that deal with Comptroller of the Currency, Federal Reserve System, and Federal Deposit Insurance Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (raw_data_path / f\"raw_data.xml\").open(\"r\") as f:\n",
    "    soup = BeautifulSoup(f, \"lxml\")\n",
    "\n",
    "chapters = soup.find_all(\"div3\")\n",
    "filtered_chapters = [chapter for chapter in chapters if chapter[\"n\"] in INCLUDED_CHAPTERS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is to iterate through the chapter hierarchy and extract the headings and the text for each section. The image illustrates the hierarchy of the data and the corresponding xml tags. \n",
    "\n",
    "![Hierarchy](images/BankingRegHierarchy.png)\n",
    "\n",
    "For each we'll also capture the metadata such as the title, part, chapter, and section. We will then save the data in a `json` format for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "\n",
    "for chapter in filtered_chapters:\n",
    "    chapter_title = next(chapter.stripped_strings)\n",
    "    volumes = chapter.find_all(\"div5\")\n",
    "    for volume in volumes:\n",
    "        volume_title = next(volume.stripped_strings)\n",
    "        for section in volume.find_all(\"div8\"):\n",
    "            section_title = next(section.stripped_strings)\n",
    "            section_attributes = {\n",
    "                \"metadata\": {\n",
    "                    \"chapter_title\": chapter_title,\n",
    "                    \"chapter_id\": chapter[\"n\"],\n",
    "                    \"volume_title\": volume_title,\n",
    "                    \"volume_id\": volume[\"n\"],\n",
    "                    \"section_id\": section[\"n\"],\n",
    "                    \"section_title\": section_title,\n",
    "                    \"unique_id\": f\"{chapter['n']}.{section['n']}\"\n",
    "                }\n",
    "            }\n",
    "            section_text = section.get_text()\n",
    "            section_attributes[\"text\"] = section_text\n",
    "            sections.append(section_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(f\"Found {len(sections)} sections\")\n",
    "rprint(f\"Sample section: {sections[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation\n",
    "We have obtained some raw data that we can immediately use to build a RAG pipeline and fine-tune a model via [Continued Pre-training](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html). Continued pre-training would help domain adapt the model to specific vocabulary and context of the banking regulations, but not necessarily give it the ability to reason about and answer questions pertaining to the regulations. For that we need an instructions tuning dataset with examples of questions and answers pertaining to the regulations. Obtaining such a dataset can be expensive and time consuming as it requires domain experts to create the data. Instead we will synthetically generate a dataset of question and answer pairs using LLMs available on Amazon Bedrock.\n",
    "\n",
    "Synthetic data generation can be a powerful tool to create large amounts of training data for a variety of tasks. We can further refine and filter out synthetic data that is not relevant by enlisting subject matter experts (SMEs) by leveraging tools such as [SageMaker Ground Truth](https://aws.amazon.com/sagemaker/groundtruth/). In summary, the advantages of synthetic data generation are:\n",
    "- It is cost effective and quick to generate large amounts of data\n",
    "- It can be used to create data for tasks where labeled data is scarce\n",
    "- It can be refined and filtered by SMEs to ensure quality\n",
    "- It can be used to evaluate the performance of Retrieval Augmented Generation (RAG) pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Mistral Mixtral 8x7B model hosted on Amazon Bedrock to generate the data. The model will be invoked using the `langchain` library.\n",
    "<div style=\"color: #415a77; background-color: #ff9f1c; padding: 10px; margin-bottom: 10px;\">\n",
    "    <strong>Important Note:</strong> Review the End User License Agreement (EULA) of the model before using it to generate synthetic data to ensure compliance with the terms of use.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models import ChatBedrockConverse\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "import boto3\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "boto3_session=boto3.session.Session()\n",
    "bedrock_runtime = boto3_session.client(\"bedrock-runtime\")\n",
    "\n",
    "llm_modelId = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model_id=llm_modelId,\n",
    "    temperature=0.3,\n",
    "    max_tokens=500,\n",
    "    top_p=1,\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the prompt template that we will use to generate the synthetic data. The prompt assigns a role to the model (law professor specializing in Banking Regulatory Compliance) and asks the model to generate a question and answer pair based on the provided context. The context is a section of the banking regulations that we obtained from the API.\n",
    "You can get creative with the data generation process including:\n",
    "- using different prompts to generate the questions and answers\n",
    "- using different models for questions and answers\n",
    "- using other models to refine the generated questions, and so on\n",
    "\n",
    "To speed up the process, we will create an async function to generate the data in parallel by making concurrent API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_GENERATION_TEMPLATE = \"\"\"You are a law professor specializing in Banking Regulatory Compliance.\n",
    "You are preparing a exam for your students. You need to generate questions and answers based on the following regulation.\n",
    "---------------------\n",
    "{context}\n",
    "----------------------\n",
    "Here are some guidelines your generated question should adhere to\n",
    "- Question should not be multiple choice\n",
    "- The question should be answerable based only on the information provided in the regulation text above\n",
    "- The question should be pointed and specific and not broad such as asking to summarize or explain an entire regulation section\n",
    "- Question can be about a specific detail or a concept mentioned in the regulation\n",
    "- Question may also ask for implications or consequences of a specific detail or concept mentioned in the regulation\n",
    "- Question may require interpretation or analysis of the regulation text\n",
    "- Question should be stand-alone and not part of a series of questions\n",
    "- The question should not include the clause identifier as students should be able to identify the relevant clause based on the question\n",
    "\n",
    "Below are guidelines for the answer\n",
    "- Answer should include an explanation or reasoning for why the answer is correct\n",
    "- If referencing a specific part of the regulation text, please include the full reference to the paragraph section number and the regulation itself\n",
    "\n",
    "Generate {num_questions} questions and a correct answer only using 'Answer' and 'Question' keys as per the format below:\n",
    "\n",
    "[Question 1]\n",
    "Question: \n",
    "Answer:\n",
    "\n",
    "[Question 2]\n",
    "Question:\n",
    "Answer:\n",
    "\n",
    "Do not include any additional keys or information in the response. \n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def generate_questions(section, num_questions=3):\n",
    "\n",
    "    question_generation_prompt = ChatPromptTemplate.from_template(\n",
    "        QUESTION_GENERATION_TEMPLATE, partial_variables={\"num_questions\": num_questions}\n",
    "    )\n",
    "\n",
    "    question_generation_chain = question_generation_prompt | llm\n",
    "\n",
    "    response = await question_generation_chain.ainvoke({\"context\": section[\"text\"]})\n",
    "\n",
    "    result = {\n",
    "        \"ref_doc_id\": section[\"metadata\"][\"unique_id\"],\n",
    "        \"questions\": response.content,\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_question_generation(sections, num_questions=3):\n",
    "\n",
    "    tasks = [\n",
    "        generate_questions(section, num_questions=num_questions) for section in sections\n",
    "    ]\n",
    "    event_loop = asyncio.get_event_loop()\n",
    "    results = event_loop.run_until_complete(asyncio.gather(*tasks))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can take a considerable amount of time to generate the question and answer pairs for the entire dataset. For demonstration purposes, we will generate the data for the first 10 sections of the regulations. A larger dataset has already been generated and is included in this lab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTIONS_TO_SAMPLE = 10\n",
    "generated_questions = run_question_generation(sections[:SECTIONS_TO_SAMPLE], num_questions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a sample question\n",
    "rprint(generated_questions[0][\"questions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question and answer pairs can be extracted using a regular expression. They can then be combined with the original context data giving us a dataset that is comprised of the context, question, and answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question_re = re.compile(r\"\\[Question \\d+\\]\\nQuestion: (.*)\\nAnswer: (.*)\\n?\")\n",
    "prepared_data = []\n",
    "for qa in generated_questions:\n",
    "    question_answers = question_re.findall(qa[\"questions\"])\n",
    "    relevant_section = next(section for section in sections if section[\"metadata\"][\"unique_id\"] == qa[\"ref_doc_id\"])\n",
    "    \n",
    "    for question in question_answers:\n",
    "        prepared_data.append({\n",
    "            \"example_id\": str(uuid.uuid4()),\n",
    "            \"ref_doc_id\": qa[\"ref_doc_id\"],\n",
    "            \"question\": question[0],\n",
    "            \"answer\": question[1],\n",
    "            \"context\" : relevant_section[\"text\"],\n",
    "            \"section_metadata\": relevant_section[\"metadata\"],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(f\"Generated {len(prepared_data)} questions\")\n",
    "rprint(prepared_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook, we obtained the raw data from the API, wrangled it into a format that can be used for downstream tasks, and generated synthetic data using a large language model. We have prepared the data for building and evaluating a RAG pipeline and fine-tuning a model to improve the performance of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
