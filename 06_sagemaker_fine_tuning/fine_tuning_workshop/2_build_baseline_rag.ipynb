{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline Retrieval Augment Generation Solution\n",
    "In this notebook we will build an initial solution that will utilize a pre-trained model augmented with a contextual data from a vector store retriever. At a high level, the solution will work as follows:\n",
    "- Based on a user's query, we will retrieve the top-k most similar documents from the vector store.\n",
    "- Provide the relevant documents as part of the prompt to the model along with the user's question\n",
    "- Generate the answer using the model\n",
    "\n",
    "![Basic RAG](images/chatbot_lang.png)\n",
    "\n",
    "We'll evaluate several aspects of the solution including:\n",
    "- The accuracy of the retrieved context\n",
    "- The quality of the generated answer\n",
    "\n",
    "These metrics will help determine whether a solution using purely pre-trained models is viable or whether we need to consider more complex strategies or fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_models = [\n",
    "    \"amazon.titan-embed-text-v2:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    \"mistral.mistral-7b-instruct-v0:2\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "]\n",
    "validate_model_access(required_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion\n",
    "The prepared datasets have been split into training and validation sets. We will load documents associated with both sets into a vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from rich import print as rprint\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_aws.chat_models import BedrockChat\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import boto3\n",
    "\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "data_path = Path(\"data/prepared_data\")\n",
    "train_data = (data_path / \"prepared_data_train.jsonl\").read_text().splitlines()\n",
    "test_data = (data_path / \"prepared_data_test.jsonl\").read_text().splitlines()\n",
    "\n",
    "doc_ids = []\n",
    "documents = []\n",
    "\n",
    "# Create a list of LangChain documents that can then be used to ingest into a vector store\n",
    "\n",
    "for record in chain(train_data, test_data):\n",
    "    json_record = json.loads(record)\n",
    "    if json_record[\"ref_doc_id\"] not in doc_ids:\n",
    "        doc_ids.append(json_record[\"ref_doc_id\"])\n",
    "        doc = Document(page_content=json_record[\"context\"], metadata=json_record[\"section_metadata\"])\n",
    "        documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will initialize the embedding model that will be used to vectorize the documents and queries. We will use the `amazon.titan-embed-text-v2:0` model for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_session=boto3.session.Session()\n",
    "\n",
    "bedrock_runtime = boto3_session.client(\"bedrock-runtime\")\n",
    "\n",
    "embedding_modelId = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "embed_model = BedrockEmbeddings(\n",
    "    model_id=embedding_modelId,\n",
    "    model_kwargs={\"dimensions\": 1024, \"normalize\": True},\n",
    "    client=bedrock_runtime,\n",
    ")\n",
    "\n",
    "query = \"Do I really need to fine-tune the large language models?\"\n",
    "response = embed_model.embed_query(query)\n",
    "rprint(f\"Generated an embedding with {len(response)} dimensions. Sample of first 10 dimensions:\\n\", response[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents can now be ingested into a vector store. We will utilize a local vector store backed by the `faiss` library for this purpose. In production scenarios, a more scalable solution like OpenSearch or pgvector should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_file = \"baseline_rag_vec_db.pkl\"\n",
    "\n",
    "if not Path(vector_store_file).exists():\n",
    "    rprint(f\"Vector store file {vector_store_file} does not exist. Will create a new vector store.\")\n",
    "    CREATE_NEW = True\n",
    "else:\n",
    "    rprint(f\"Vector store file {vector_store_file} already exists and will be reused. Delete it or change the file name above to if you wish to create a new vector store.\")\n",
    "    CREATE_NEW = False \n",
    "\n",
    "if CREATE_NEW:\n",
    "    vec_db = FAISS.from_documents(documents, embed_model)\n",
    "    pickle.dump(vec_db.serialize_to_bytes(), open(vector_store_file, \"wb\"))\n",
    "    \n",
    "else:\n",
    "    if not Path(vector_store_file).exists():\n",
    "        raise FileNotFoundError(f\"Vector store file {vector_store_file} not found. Set CREATE_NEW to True to create a new vector store.\")\n",
    "    \n",
    "    vector_db_buff = BytesIO(pickle.load(open(vector_store_file, \"rb\")))\n",
    "    vec_db = FAISS.deserialize_from_bytes(serialized=vector_db_buff.read(), embeddings=embed_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the retrieval performance\n",
    "Before moving on to the generation step, we should validate the performance of the retriever. The large language model will not be able to generate accurate answers if the retrieved context is not relevant. We will evaluate the retriever using the validation set. The prepared validation set contains 400 questions along with relevant contexts. For each question, we have the unique document id of the relevant context. So our evaluation is simple: we will retrieve the top-k documents for each question and check if the relevant context is present in the top-k results. We will then calculate the recall or Hit Rate of the retriever. Additionally we'll compute the MRR (Mean Reciprocal Rank) metric. The MRR is the average of the reciprocal ranks of the first relevant document. For example, if we retrieve 5 documents (k=5) and the relevant document is ranked 2nd, the reciprocal rank would be 1/2. We calculate the reciprocal rank for each question and then take the average to get the MRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = (data_path / \"prepared_data_test.jsonl\").read_text().splitlines()\n",
    "retriever_evaluation_data = []\n",
    "\n",
    "# we only need the ref_doc_id and question from the test data\n",
    "\n",
    "for record in test_data:\n",
    "    json_record = json.loads(record)\n",
    "    retriever_evaluation_data.append({\"ref_doc_id\":json_record[\"ref_doc_id\"], \"question\":json_record[\"question\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 # number of documents to retrieve\n",
    "faiss_retriever = vec_db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "correct = 0\n",
    "reciprocal_rank = 0\n",
    "num_examples = 400 # Number of examples to evaluate\n",
    "for i, eval_data in enumerate(retriever_evaluation_data[:num_examples]):\n",
    "    returned_docs = faiss_retriever.invoke(eval_data[\"question\"])\n",
    "    returned_doc_ids = [doc.metadata[\"unique_id\"] for doc in returned_docs]\n",
    "    if eval_data[\"ref_doc_id\"] in returned_doc_ids:\n",
    "        correct += 1\n",
    "        reciprocal_rank += 1 / (returned_doc_ids.index(eval_data[\"ref_doc_id\"]) + 1)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "hit_rate = correct / num_examples\n",
    "mrr = reciprocal_rank / num_examples\n",
    "\n",
    "print(f\"Hit rate @k={k}: {hit_rate}\")\n",
    "print(f\"MRR @k={k}: {mrr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results above may vary but we should see a hit rate of over 0.92 and an MRR of over 0.85. These results are quite good and indicate that the retriever is able to find the relevant context for most questions. If this was not the case, then using a different embedding model or fine-tuning the retriever would be possible options to consider. A number of libraries exist that can be used to fine-tune or train a custom embedding model for retrieval including:\n",
    "- [sentence-transformers](https://www.sbert.net/docs/sentence_transformer/training_overview.html)\n",
    "- [RAGatouille](https://github.com/bclavie/RAGatouille)\n",
    "\n",
    "There are other ways to improve the retriever performance such as using hybrid search that combines both dense and sparse retrieval methods. \n",
    "\n",
    "For example below, we can improve the performance of the above retriever by ensembling it with a sparse retriever like BM25. This tends to work well with domain specific datasets as it combines the strengths of keyword search with semantic search. We'll use langchain's [EnsembleRetriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/ensemble/) to combine the dense retriever with BM25. However many vector dbs offer hybrid search capabilities out of the box such as  [OpenSearch](https://opensearch.org/docs/latest/search-plugins/hybrid-search/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "bm_25 = BM25Retriever.from_documents(documents)\n",
    "bm_25.k = k\n",
    "\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, bm_25], weights=[0.75, 0.25] # you can fine-tune the weights here\n",
    ")\n",
    "\n",
    "correct = 0\n",
    "average_rank = 0\n",
    "num_examples = 400 # Number of examples to evaluate\n",
    "for i, eval_data in enumerate(retriever_evaluation_data[:num_examples]):\n",
    "    returned_docs = ensemble_retriever.invoke(eval_data[\"question\"])\n",
    "    returned_doc_ids = [doc.metadata[\"unique_id\"] for doc in returned_docs]\n",
    "    if eval_data[\"ref_doc_id\"] in returned_doc_ids:\n",
    "        correct += 1\n",
    "        average_rank += 1 / (returned_doc_ids.index(eval_data[\"ref_doc_id\"]) + 1)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "hit_rate = correct / num_examples\n",
    "mrr = average_rank / num_examples\n",
    "\n",
    "print(f\"Hit rate with Hybrid Search @k={k}: {hit_rate}\")\n",
    "print(f\"MRR with Hybrid Search @k={k}: {mrr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an improvement in the hit rate and MRR after ensembling with BM25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Retrieval Augmented Generation (RAG) pipeline\n",
    "Now that we are satisfied that the retriever is performing reasonably well, we can move on to the generation step. We'll build a basic Chain that given a question will retrieve the relevant context and invoke a Large Language Model to generate the answer. We will use the smaller `mistral.mistral-7b-instruct-v0:2` to generate the responses, this will also be the model that we will fine-tune in the subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models import BedrockChat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm_modelId = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "\n",
    "\n",
    "llm = BedrockChat(\n",
    "    model_id=llm_modelId,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    "    client=bedrock_runtime,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the prompt template that will be used to generate the answer. It's a simple template that will provide basic single-turn functionality and not include any guardrails to constrain the interaction. This is a good starting point but in production scenarios, you would want to add more sophisticated guardrails to ensure the model generates safe and accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"You are a Banking Regulatory Compliance expert. You have been asked to provide guidance on the following question using the referenced regulations below.\n",
    "If the referenced regulations do not provide an answer, indicate to the user that you are unable to provide an answer and suggest they consult with a legal expert.\n",
    "\n",
    "----------------------\n",
    "{context}\n",
    "----------------------\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": ensemble_retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "# produce an output that contains the answer and the context that was passed to the model\n",
    "generate_answer = {\"answer\": prompt | llm | output_parser,\n",
    "                   \"context\": itemgetter(\"context\")}\n",
    "\n",
    "chain = setup_and_retrieval | generate_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invoke the chain with a sample test question and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_record = json.loads(test_data[10])\n",
    "sample_question = sample_record[\"question\"]\n",
    "sample_answer = sample_record[\"answer\"]\n",
    "rprint(f\"[bold green]Sample question:[/bold green] {sample_question}\")\n",
    "response = chain.invoke(sample_question)\n",
    "generated_answer = response[\"answer\"]\n",
    "rprint(f\"[bold green]Generated answer:[/bold green] {generated_answer}\")\n",
    "rprint(f\"[bold green]Ground truth answer:[/bold green] {sample_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Evaluation\n",
    "While a manual examination of the generated answers is one of the more reliable ways to evaluate the model, it is not scalable especially as we iterate on the pipeline. In this section we will leverage an automated evaluation framework [RAGAS](https://arxiv.org/abs/2309.15217) (Retrieval Augmented Generation Assessment) along with its implementation in the [ragas](https://docs.ragas.io/en/stable/index.html) python library. RAGAS proposes a number of metrics to evaluate the quality of the generated answers. We will use the following metrics:\n",
    "- Faithfulness: Measures the factual consistency of the generated answer against the given context\n",
    "- Answer Relevance: Focuses on assessing how pertinent the generated answer is to the given prompt\n",
    "- Answer semantic similarity: pertains to the assessment of the semantic resemblance between the generated answer and the ground truth\n",
    "- Answer Correctness: involves gauging the accuracy of the generated answer when compared to the ground truth\n",
    "\n",
    "RAGAS uses an LLM as a judge for many of the metrics and as such can be very sensitive to the choice of the LLM and the generation parameters such as temperatures. Metrics may vary significantly from one LLM to another and even with the same LLM you may see differences from run to run even with low temperature settings. The metrics however are still useful as we can compare the performance of different models and pipelines as it gives us a relative measure of performance improvement from one iteration to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import faithfulness, answer_similarity, answer_relevancy, answer_correctness\n",
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the smallest model (Haiku) in the Claude 3 family of models as the judge for the RAGAS metrics. We will also use the default prompts within RAGAS for the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"12345\" # Ragas raises exception if this is not set\n",
    "\n",
    "eval_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0\n",
    "    },\n",
    "    client=bedrock_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_answer_async(rag_chain, example):\n",
    "    \"\"\"Helper function to generate an answer asynchronously\"\"\"\n",
    "    example = json.loads(example)\n",
    "    response = await rag_chain.ainvoke(example[\"question\"])\n",
    "    contexts = [doc.page_content for doc in response[\"context\"]]\n",
    "    row = {\"question\": example[\"question\"], \"answer\": response[\"answer\"], \"contexts\": contexts, \"ground_truth\": example[\"answer\"]}\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation can be time consuming, we will therefore only use the first 100 example from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the generated responses for the first 100 examples in the test data\n",
    "\n",
    "NUM_SAMPLE_LLM_EVALUATION = 100\n",
    "eval_rows = []\n",
    "for example in test_data[:NUM_SAMPLE_LLM_EVALUATION]:\n",
    "    eval_rows.append(generate_answer_async(chain, example))\n",
    "event_loop = asyncio.get_event_loop()\n",
    "eval_data= event_loop.run_until_complete(asyncio.gather(*eval_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_llm_async(metric, rows):\n",
    "    \"\"\"Helper function to asynchronously evaluate each example\"\"\"\n",
    "    evals = [metric.acall(row) for row in rows]\n",
    "    # event_loop = asyncio.get_event_loop()\n",
    "    evals = await asyncio.gather(*evals)\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_metric = EvaluatorChain(metric=faithfulness, llm=eval_llm, embeddings=embed_model)\n",
    "answer_relevancy_metric = EvaluatorChain(metric=answer_relevancy, llm=eval_llm, embeddings=embed_model)\n",
    "answer_similarity_metric = EvaluatorChain(metric=answer_similarity, llm=eval_llm, embeddings=embed_model)\n",
    "answer_correctness_metric = EvaluatorChain(metric=answer_correctness, llm=eval_llm, embeddings=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Faithfulness:**](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html) measure the extent to which the claims in the generated answer are supported by the context. It is calculated as the ratio of the number of claims in the generated answer that are supported by the context to the total number of claims in the generated answer. In other words it helps us detect hallucinations as we would expect all claims in the generated answer to be supported by the context.\n",
    "It does not reflect on the accuracy or correctness of the claims, only that they are supported by the context.\n",
    "\n",
    "**NOTE:** If you see a message `Failed to parse output. Returning None.` during the evaluation, it simply means that ragas was unable to parse the output from the model. This can happen if the model generates an output that is not in the expected format. These samples will be ignored when calculating the aggregate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_evals = event_loop.run_until_complete(evaluate_llm_async(faithfulness_metric, eval_data))\n",
    "faithfulness_scores = [eval[\"faithfulness\"] for eval in faithfulness_evals if not math.isnan(eval[\"faithfulness\"])]\n",
    "faithfulness_score = sum(faithfulness_scores) / len(faithfulness_scores)\n",
    "print(\"Faithfulness Score: \", faithfulness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can filter on the low scoring examples for further analysis\n",
    "# [e for e in faithfulness_evals if e[\"faithfulness\"] < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Answer Relevancy:**](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html) attempts to measure how pertinent the generated answer is to the given prompt. It works by having the evaluator LLM generate synthetic questions based on the generated answer and then calculating the average semantic similarity between the given question and the synthetic questions. The idea is that a more complete and pertinent answer should yield synthetic questions that are more similar to the given question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_evals = event_loop.run_until_complete(evaluate_llm_async(answer_relevancy_metric, eval_data))\n",
    "relevancy_scores = [eval[\"answer_relevancy\"] for eval in relevancy_evals if not math.isnan(eval[\"answer_relevancy\"])]\n",
    "relevancy_score = sum(relevancy_scores) / len(relevancy_scores)\n",
    "\n",
    "print(\"Answer Relevancy Score: \", relevancy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Answer semantic similarity:**](https://docs.ragas.io/en/stable/concepts/metrics/semantic_similarity.html) measures the cosine similarity between the ground truth answer and the generated answer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_similarity_evals = event_loop.run_until_complete(evaluate_llm_async(answer_similarity_metric, eval_data[:10]))\n",
    "similarity_scores = [eval[\"answer_similarity\"] for eval in answer_similarity_evals if not math.isnan(eval[\"answer_similarity\"])]\n",
    "similarity_score = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "print(\"Answer Similarity Score: \", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Answer Correctness**](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html): Combines factual similarity assessed by the evaluator LLM with the semantic similarity between the generated answer and the ground truth. It is calculated as a weighted average of the factual similarity and the semantic similarity. Factual similarity is calculated similar to Faithfulness but also considers overlapping claims between the generated answer and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_correctness_evals = event_loop.run_until_complete(evaluate_llm_async(answer_correctness_metric, eval_data))\n",
    "correctness_scores = [eval[\"answer_correctness\"] for eval in answer_correctness_evals if not math.isnan(eval[\"answer_correctness\"])]\n",
    "correctness_score = sum(correctness_scores) / len(correctness_scores)\n",
    "\n",
    "print(\"Answer Correctness Score: \", correctness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the evaluation metrics so we can compare them with the fine-tuned model in the subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base_evaluation.json\", \"w\") as f:\n",
    "    metrics = {\n",
    "        \"faithfulness\": faithfulness_score,\n",
    "        \"relevancy\": relevancy_score,\n",
    "        \"similarity\": similarity_score,\n",
    "        \"correctness\": correctness_score,\n",
    "    }\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook, we have demonstrated how to use LangChain to build a hybrid search system that combines BM25 and FAISS retrievers to retrieve relevant documents for a given question. We have also shown how to use LangChain to generate answers to questions using a language model and evaluate the generated answers using Ragas metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
