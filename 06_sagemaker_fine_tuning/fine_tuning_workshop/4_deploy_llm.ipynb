{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy fine-tuned model\n",
    "Now that we have fine-tuned the model, we can deploy it to a SageMaker endpoint. There are numerous deployment [options](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) in SageMaker including RealTime, Serverless, Asynchronous, and Batch Transform. In this notebook, we will deploy the model as a RealTime endpoint. \n",
    "There are also numerous options for deploying LLMs for RealTime inference including:\n",
    "- Single model or multi-model endpoints\n",
    "- Instance Types (GPU, Inferentia2)\n",
    "- Various inference frameworks such as Large Model Inference, Text Generation Inference, TorchServe, and TensorRT LLM\n",
    "We'll use the Large Model Inference (LMI) container to deploy the LLM. \n",
    "\n",
    "Refer to the blog post [here](https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-mixtral-and-llama-2-models-with-new-amazon-sagemaker-containers/) for detailed recommendations for configuring various model architectures for optimal performance on thr LMI container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.environment_validation import validate_environment, validate_model_access\n",
    "validate_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "bucket = sess.default_bucket()  # default bucket name\n",
    "account_id = sess.account_id() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to upload our merged model to S3\n",
    "local_model_path = \"merged_model\"\n",
    "s3_model_path = f\"s3://{bucket}/banking-regulations-model\"\n",
    "!aws s3 cp {local_model_path} {s3_model_path} --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment configuration for the LMI container is specified via a `serving.properties` file. The file contains various configuration options such as the number of threads, batch size, and the maximum sequence length. In this case we will use the following parameters:\n",
    "- **engine**: The runtime engine of the inference code. For LMI this should either be `MPI` or `Python` depending on which inference backend is used\n",
    "- **option.model_id**: The S3 location of the model artifact or the model name from Hugging Face Hub\n",
    "- **option.trust_remote_code**: Enables trust remote code execution which is required for some models\n",
    "- **option.tensor_parallel_degree**: The number of GPUs across which the model should be parallelized (Our deployment instance has just 1 GPU)\n",
    "- **option.rolling_batch**: Enables continuous batching (iteration level batching) with one of the supported backends. See [here](https://docs.djl.ai/docs/serving/serving/docs/lmi/deployment_guide/configurations.html#inference-library-configuration) for more details\n",
    "- **option.max_rolling_batch_size**: The maximum number of requests/sequences the model can process at a time. This parameter should be tuned to maximize throughput while staying within the available memory limits\n",
    "- **option.dtype**: The data type you plan to cast the model weights to.\n",
    "- **option.max_model_len**: The maximum number of tokens the model can process in a single sequence\n",
    "- **option.gpu_memory_utilization**: The fraction of the GPU memory that the model can use. This parameter should be tuned to maximize throughput while staying within the available memory limits\n",
    "\n",
    "For more details on the available configuration options, refer to the [documentation](https://docs.djl.ai/docs/serving/serving/docs/lmi/deployment_guide/configurations.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"serving.properties\").open(\"w\") as f:\n",
    "    f.write((\n",
    "        \"engine=Python\\n\"\n",
    "        f\"option.model_id={s3_model_path}\\n\"\n",
    "        \"option.trust_remote_code=true\\n\"\n",
    "        \"option.tensor_parallel_degree=1\\n\"\n",
    "        \"option.rolling_batch=vllm\\n\"\n",
    "        \"option.max_rolling_batch_size=4\\n\"\n",
    "        \"option.dtype=fp16\\n\"\n",
    "        \"option.max_model_len=8192\\n\"\n",
    "        \"option.gpu_memory_utilization=0.9\\n\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `serving.properties` file is then placed into a tarball and uploaded to S3. The tarball can contain additional files such as the model weights and custom inference code. In this case though, the weights will be downloaded from the S3 location specified in the `serving.properties` file `option.model_id` setting and we will use the default inference code provided by the LMI container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the image_uris module to look up the container uri for the large model inference container\n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=region,\n",
    "        version=\"0.27.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the model configuration artifact to S3\n",
    "\n",
    "s3_code_prefix = \"banking-regulations-model/code\"\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally deploy the model using the SageMaker SDK\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"banking-regulatory-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             # container_startup_health_check_timeout=3600\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the endpoint with an example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data\n",
    "\n",
    "import json\n",
    "test_data = []\n",
    "with open(\"data/prepared_data/prepared_data_test.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "    \n",
    "\n",
    "inference_template = \"[INST] You are a Banking Regulations expert.\\nGiven this context\\nCONTEXT\\n{context}\\n Answer this question\\nQuestion: {question} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the endpoint with a sample question\n",
    "idx = 125\n",
    "context = test_data[idx][\"context\"]\n",
    "question = test_data[idx][\"question\"]\n",
    "answer = test_data[idx][\"answer\"]\n",
    "\n",
    "prompt = inference_template.format(context=context, question=question)\n",
    "\n",
    "response = predictor.predict(\n",
    "    {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\":256, \"do_sample\":False, \"temperature\":0}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question: \", question)\n",
    "print(\"\\nGenerated Answer: \", response[\"generated_text\"])\n",
    "print(\"\\nGround Truth Answer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the endpoint name to a file so we can use it in the next notebook\n",
    "\n",
    "with open(\"endpoint_config.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"endpoint_name\": endpoint_name}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook, we deployed a fine-tuned model to a SageMaker endpoint using the Large Model Inference container. In the next notebook, we will incorporate the endpoint into our RAG pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
