{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wealth Management AI Advisor Capstone\n",
    "\n",
    "In this capstone project, you will design and develop a sophisticated Wealth Management AI Assistant using the GenAI technology stack on AWS. This AI assistant will leverage state-of-the-art retrieval-augmented generation techniques (RAG) utilizing Amazon OpenSearch Serverless to analyze investment funds comprehensively and provide real-time insights to users. The core objective of the project is to bridge the gap between complex financial data and investment decision-making, offering a user-friendly platform for investors and wealth managers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section: Data Ingestion Workflow\n",
    "\n",
    "\n",
    "![title](images/data-ingestion.png)\n",
    "\n",
    "\n",
    "In this section, we will focus on ingesting fund information data to a Vector DB. We will ingest fund PDF documents using text extraction techniques, get high dimensional vector representation of the text(embedding) using different models and ingest the data to Open Search Serverless Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: \n",
    "\n",
    "Here are some of the packages you will need for this capstone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install sentence-transformers\n",
    "!pip install pypdf\n",
    "!pip install -U opensearch-py\n",
    "!pip install datasets\n",
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "#langchain related imports\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.load.dump import dumps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Bedrock Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "module_path = \"../\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Bedrock Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - create the Anthropic Model\n",
    "llm_claude_v2 = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", client=boto3_bedrock, model_kwargs={\"max_tokens_to_sample\": 200}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 1A - Set up additional inference parameters\n",
    "\n",
    "Set up following in `model_kwargs` parameter\n",
    "\n",
    "1. temperature\n",
    "2. top_p\n",
    "3. top_k\n",
    "\n",
    "Hint: Refer [Claude Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html) for details about each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_claude_v2 = \"<YOUR_CODE_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 1B - Set up Meta LLama Model \n",
    "\n",
    "Hints: \n",
    "1. To get the model Id, use `list_foundation_models` function using the bedrock client\n",
    "2. Refer [Meta LLama Documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html) for details about each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_llama13b = \"<YOUR_CODE_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up an Bedrock Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up HuggingFaceEmbeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO:  1C - Experiment with differnt embedding models (Optional)\n",
    "\n",
    "Hint: Use [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leader board to choose from range of best performing embedding model. Choose an model thats relatively smaller(~1GB) for this exercise. If the kernel crashes, try changing the Studio notebook instance type to larger instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = \"<YOUR CODE HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preparation\n",
    "Let's first download some of the files to build our document store. For this example we will be using the files in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 2A - Visually Inspect Fund documents\n",
    "\n",
    "Hint: PDF files are in data sub-folder in the 07_capstone folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 2B - Load Fund PDFs using LangChain Loader\n",
    "\n",
    "Hint: LangChain offers various [PDF Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf). Examples of PDF loaders include `PyPDFLoader`, `UnstructuredPDFLoader`,`PyPDFDirectoryLoader`, `OnlinePDFLoader` etc. After reviewing options, pick the optimal Loader that fits this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = \"<YOUR_CODE_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-DO: 2C - Text Splitters\n",
    "\n",
    "Hint: LangChain offers many text splitters, based on the Fund document type determine the best text splitter for this use case. Refer [Text Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/) for details\n",
    "\n",
    "Considerations: \n",
    "1. Depending on the embedding dimension, you may want to experiment different chunk size and overlaps\n",
    "2. You can create many Open Search Serverless Index depending on different configurations and evaluate responses from LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = \"<YOUR_CODE_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 2D - Create a Function to load the documents and Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents():\n",
    "    # Load all the PDFs from the data folder\n",
    "    loader = \"<YOUR_CODE_HERE>\"\n",
    "    # Include loader from 2B\n",
    "    documents = \"<YOUR_CODE_HERE>\"\n",
    "    # Include the Text splitter from 2C\n",
    "    text_splitter = \"<YOUR_CODE_HERE>\"\n",
    "    # Function returns text_splitter\n",
    "    split_docs = \"<YOUR_CODE_HERE>\"\n",
    "    \n",
    "    return documents, split_docs\n",
    "\n",
    "# use this variable in the Vector Store creation\n",
    "documents, split_docs = process_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 2E - Compute stats\n",
    "\n",
    "1. Number of documents loader\n",
    "2. Average lenght of each documents after splitting\n",
    "3. Total number of documents after splitting\n",
    "4. Average lenght of each chunk after splitting\n",
    "\n",
    "Hint: You many have to iterate steps 2A-2C if these stats doesnt fit this use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = \"<YOUR_CODE_HERE>\"\n",
    "average_charaters_in_documents = \"<YOUR_CODE_HERE>\"\n",
    "total_chunks = \"<YOUR_CODE_HERE>\"\n",
    "average_charaters_in_chunks = \"<YOUR_CODE_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-DO: 2F - Test Embedding Models\n",
    "\n",
    "In this section you evaluate the embedding models on the documents after loading and splitting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = \"\"\"\n",
    " Anthropic announced Claude 3, a new family of state-of-the-art AI models that allows customers to choose the exact combination of intelligence, speed, and cost that suits their business needs. \n",
    " The three models in the family are Claude 3 Haiku, the fastest and most compact model for near-instant responsiveness, \n",
    " Claude 3 Sonnet, the ideal balanced model between skills and speed, and \n",
    " Claude 3 Opus, the most intelligent offering for the top-level performance on highly complex tasks.\n",
    "\"\"\"\n",
    "\n",
    "# use embed_query and np.array()\n",
    "text_embedding = \"<YOUR_CODE_HERE>\"\n",
    "      \n",
    "text_embedding_dim = \"<YOUR_CODE_HERE>\"\n",
    "\n",
    "print(f\"text embedding dimension: {text_embedding_dim}\")\n",
    "\n",
    "print(f\"text embedding: {text_embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Set Up OpenSearch Serverless\n",
    "\n",
    "![title](images/retrieval.png)\n",
    "\n",
    "We set up opensearch related variables and create a new vector store and index in this section. This is a one time activity and doesnt have to be repeated. There is no TO-DO in this section, execute the below cell to create the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "suffix=\"wealth-ai\"\n",
    "vector_store_name = f\"capstone-rag-{suffix}\"\n",
    "index_name = f\"capstone-rag-index-{suffix}\"\n",
    "encryption_policy_name = f\"capstone-rag-sp-{suffix}\"\n",
    "network_policy_name = f\"capstone-rag-np-{suffix}\"\n",
    "access_policy_name = f\"capstone-rag-ap-{suffix}\"\n",
    "user_identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "user_account = boto3.client('sts').get_caller_identity()['Account']\n",
    "sagemaker_notebook_role = 'arn:aws:iam::' + user_account + ':role/aws-service-role/sagemaker.amazonaws.com/AWSServiceRoleForAmazonSageMakerNotebooks'\n",
    "\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "print(\"Creating a security policy for AOSS collection..\")\n",
    "security_policy = aoss_client.create_security_policy(\n",
    "    name = encryption_policy_name,\n",
    "    policy = json.dumps(\n",
    "        {\n",
    "            'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "            'ResourceType': 'collection'}],\n",
    "            'AWSOwnedKey': True\n",
    "        }),\n",
    "    type = 'encryption'\n",
    ")\n",
    "\n",
    "print(\"Creating a network policy for AOSS collection..\")\n",
    "network_policy = aoss_client.create_security_policy(\n",
    "    name = network_policy_name,\n",
    "    policy = json.dumps(\n",
    "        [\n",
    "            {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "            'ResourceType': 'collection'}],\n",
    "            'AllowFromPublic': True}\n",
    "        ]),\n",
    "    type = 'network'\n",
    ")\n",
    "\n",
    "print(\"Creating an AOSS collection..\")\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "print(\"Waiting for an AOSS collection to be created..\")\n",
    "while True:\n",
    "    status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "    if status in ('ACTIVE', 'FAILED'): break\n",
    "    print(\".\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"Creating an access policy for the AOSS collection..\")\n",
    "access_policy = aoss_client.create_access_policy(\n",
    "    name = access_policy_name,\n",
    "    policy = json.dumps(\n",
    "        [\n",
    "            {\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'Resource': ['collection/' + vector_store_name],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateCollectionItems',\n",
    "                            'aoss:DeleteCollectionItems',\n",
    "                            'aoss:UpdateCollectionItems',\n",
    "                            'aoss:DescribeCollectionItems'],\n",
    "                        'ResourceType': 'collection'\n",
    "                    },\n",
    "                    {\n",
    "                        'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateIndex',\n",
    "                            'aoss:DeleteIndex',\n",
    "                            'aoss:UpdateIndex',\n",
    "                            'aoss:DescribeIndex',\n",
    "                            'aoss:ReadDocument',\n",
    "                            'aoss:WriteDocument'],\n",
    "                        'ResourceType': 'index'\n",
    "                    }],\n",
    "                'Principal': [user_identity, sagemaker_notebook_role],\n",
    "                'Description': 'Easy data policy'}\n",
    "        ]),\n",
    "    type = 'data'\n",
    ")\n",
    "\n",
    "host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com:443'\n",
    "\n",
    "print(\"AOSS host: \" + host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, os.environ.get(\"AWS_DEFAULT_REGION\", None), service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 3A - Create an Open Search vector store\n",
    "\n",
    "Hint: Use [OpenSearchVectorSearch](https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html) to VectorStore initialized from documents and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_1 = OpenSearchVectorSearch.from_documents(\n",
    "    documents=<YOUR-CODE-HERE>, #Hint: split_docs from process_documents() function\n",
    "    embedding=<YOUR-CODE-HERE>, #Hint: Bedrock or HuggingFace Embeddings\n",
    "    index_name=<YOUR-CODE-HERE>, #Hint: unique index name for each vector store\n",
    "    engine=<YOUR-CODE-HERE>, # Hint: “nmslib”, “faiss”, “lucene”; default: “nmslib”\n",
    "    space_type=<YOUR-CODE-HERE>, # Hint: “l2”, “l1”, “cosinesimil”, “linf”, “innerproduct”; default: “l2”\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    m=16,\n",
    "    ef_construction=128,\n",
    "    ef_search=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 3B - Create additional vector stores\n",
    "\n",
    "In this section, you can create additional vector stores so you can evaluate performance with different combinations. Here are some ideas\n",
    "\n",
    "1. Create a vector for each embedding you created Bedrock embedding vs HuggingFace Embeddings\n",
    "2. Create a vector store with different chunk size and overlap\n",
    "3. Create a vecore store with different OpenSearch vector engine\n",
    "3. Create a vecore store with different OpenSearch vector space type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_2 = <YOUR-CODE-HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Optionally, you can create additional vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store_n = <YOUR-CODE-HERE> #Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats: You have successfully completed the Data Ingestion Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Text Generation Workflow\n",
    "\n",
    "![title](images/generation.png)\n",
    "\n",
    "In the generation workflow, we will embed the user query using similar models in the data ingestion workflow to perform semantic search against the Vector DB. We will use various techniuques to get the similar documents and pass it to the LLM to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "\n",
    "\n",
    "#### Similarity Search\n",
    "\n",
    "To test the retrieval from the vector store we created, We can use the similarity search method to make a query and return the chunks of text. Pay close attention to the top k= 3 results returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"What are the ticker symbols for Calvert Global Real Estate Fund\"\n",
    "\n",
    "results = vector_store_1.similarity_search(query, k=3)\n",
    "\n",
    "print(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Search with filters\n",
    "\n",
    "Note that expected chunks should return from `data/Calvert Global Real Estate Fund Fact Sheet.pdf`, however we see other sources in the top k=3 results. Lets refine this search using filter in the similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"What are the ticker symbols for Calvert Global Real Estate Fund\"\n",
    "\n",
    "metadata_filter = {\"term\": {\"metadata.source.keyword\": \"data/Calvert Global Real Estate Fund Fact Sheet.pdf\"}}\n",
    "\n",
    "results = vector_store_1.similarity_search(query, k=3, boolean_filter=metadata_filter)\n",
    "\n",
    "print(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO-DO: 4A - Similarity Search Methods\n",
    "\n",
    "Explore following similarity search methods of [OpenSearchVectorSearch](https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html#langchain_community.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.from_documents). Understand different parameters of these methods\n",
    "\n",
    "1. similarity_search\n",
    "2. similarity_search_with_score\n",
    "3. similarity_search_with_score_by_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: RAG Generate Answers\n",
    "\n",
    "A typical weath management advisor would looks for insights from Fund documents and here is an example of a question that would give insights about a fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_0 = \"What is the Expense Ratio of shares in California Limited-Term Tax-Free Funds. Provide me the response in bullet point format\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each query in the below sections define the following \n",
    "\n",
    "1. Define a Prompt Template by apply prompt engineering best practices for each query\n",
    "2. Use LangChain RetrievalQA, Hint: [as_retriever](https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html#langchain_community.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.as_retriever) examples\n",
    "3. print results\n",
    "4. capture results in capstone_eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m prompt_template_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHuman: Use the following pieces of context to provide a concise answer to the question at the end. \u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mIf you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, just say that you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt try to make up an answer.\u001b[39m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mI want just the answer, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt include any verbose text\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     13\u001b[0m PROMPT_0 \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39mprompt_template_0, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m context_0 \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store_1\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(query_0, search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m})\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(dumps(context_0, pretty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     19\u001b[0m qa_prompt_0 \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m     20\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm_claude_v2,\n\u001b[1;32m     21\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     chain_type_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: PROMPT_0},\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vector_store_1' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_0 = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "I want just the answer, don't include any verbose text\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT_0 = PromptTemplate(template=prompt_template_0, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "context_0 = vector_store_1.similarity_search(query_0, search_kwargs={\"k\": 3})\n",
    "\n",
    "print(dumps(context_0, pretty=True))\n",
    "\n",
    "qa_prompt_0 = RetrievalQA.from_chain_type(\n",
    "    llm=llm_claude_v2,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = vector_store_1.as_retriever(search_kwargs={\"k\": 3, \"filter\" : {}}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT_0},\n",
    ")\n",
    "\n",
    "result_0 = qa_prompt_0({\"query\": query_0})\n",
    "print_ww(result_0[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_0 = \"\"\"Human: Use the following pieces of context to provide a concise answer to the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "I want just the answer, don't include any verbose text\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT_0 = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "context_0 = vector_store_1.similarity_search(query_0, search_kwargs={\"k\": 3})\n",
    "\n",
    "print(dumps(context_0, pretty=True))\n",
    "\n",
    "qa_prompt_0 = RetrievalQA.from_chain_type(\n",
    "    llm=llm_claude_v2,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever = vector_store_2.as_retriever(search_kwargs={\"k\": 3, \"filter\" : {}}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n",
    "\n",
    "result_0 = qa_prompt({\"query\": query_0})\n",
    "print_ww(result_0[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-DO: 5A - Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"What is the 30-day SEC unsubsidized yield of Institutional Shares in California Limited Term Tax Free Funds\"\n",
    "\n",
    "prompt_template_1 = <YOUR_CODE_HERE>\n",
    "\n",
    "PROMPT_1 = <YOUR_CODE_HERE>\n",
    "\n",
    "context_1 = <YOUR_CODE_HERE>\n",
    "\n",
    "print(dumps(context_1, pretty=True))\n",
    "\n",
    "qa_prompt_1 = <YOUR_CODE_HERE>\n",
    "\n",
    "result_1 = <YOUR_CODE_HERE>\n",
    "\n",
    "print_ww(result_1[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-DO: 5B - Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"What is the annual return by year for Vanguard 500 Index Fund\"\n",
    "\n",
    "prompt_template_2 = <YOUR_CODE_HERE>\n",
    "\n",
    "PROMPT_2 = <YOUR_CODE_HERE>\n",
    "\n",
    "context_2 = <YOUR_CODE_HERE>\n",
    "\n",
    "print(dumps(context_2, pretty=True))\n",
    "\n",
    "qa_prompt_2 = <YOUR_CODE_HERE>\n",
    "\n",
    "result_2 = <YOUR_CODE_HERE>\n",
    "\n",
    "print_ww(result_2[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-DO: 5C - Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = \"What is the YTD high-low NAV of Class A Shares in California Limited Term Tax Free Funds\"\n",
    "\n",
    "prompt_template_3 = <YOUR_CODE_HERE>\n",
    "\n",
    "PROMPT_3 = <YOUR_CODE_HERE>\n",
    "\n",
    "context_3 = <YOUR_CODE_HERE>\n",
    "\n",
    "print(dumps(context_3, pretty=True))\n",
    "\n",
    "qa_prompt_3 = <YOUR_CODE_HERE>\n",
    "\n",
    "result_3 = <YOUR_CODE_HERE>\n",
    "\n",
    "print_ww(result_3[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-DO: 5D - Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_4 = \"What is Share class information of California Limited-Term Tax-Free Funds\"\n",
    "\n",
    "prompt_template_4 = <YOUR_CODE_HERE>\n",
    "\n",
    "PROMPT_4 = <YOUR_CODE_HERE>\n",
    "\n",
    "context_4 = <YOUR_CODE_HERE>\n",
    "\n",
    "print(dumps(context_4, pretty=True))\n",
    "\n",
    "qa_prompt_4 = <YOUR_CODE_HERE>\n",
    "\n",
    "result_4 = <YOUR_CODE_HERE>\n",
    "\n",
    "print_ww(result_4[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To-DO: 5E - Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5 = \"What are the Top holdings (%) of California Limited-Term Tax-Free Funds\"\n",
    "\n",
    "prompt_template_5 = <YOUR_CODE_HERE>\n",
    "\n",
    "PROMPT_5 = <YOUR_CODE_HERE>\n",
    "\n",
    "context_5 = <YOUR_CODE_HERE>\n",
    "\n",
    "print(dumps(context_5, pretty=True))\n",
    "\n",
    "qa_prompt_5 = <YOUR_CODE_HERE>\n",
    "\n",
    "result_5 = <YOUR_CODE_HERE>\n",
    "\n",
    "print_ww(result_5[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats, you have successfully completed this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: LLM Eval with Ragas\n",
    "\n",
    "In this section, you will evaluate the results of your RAG application. Refer [RAGAS](https://github.com/explodinggradients/ragas) documentation for more help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client():\n",
    "    \n",
    "    from utils import bedrock, print_ww\n",
    "    boto3_bedrock = bedrock.get_bedrock_client(\n",
    "        assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "        region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    "    )\n",
    "    \n",
    "    return boto3_bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "eval_llm = Bedrock(\n",
    "    client=get_client(),\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    model_kwargs={\"max_tokens_to_sample\": 500, \"temperature\": 0.9},\n",
    "    streaming=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast  # Safe evaluation of strings containing Python literals\n",
    "\n",
    "data_samples = {\n",
    "    'question': [],\n",
    "    'ground_truth': [],\n",
    "    'answer': [],\n",
    "    'contexts': []\n",
    "}\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "csv_file_path = './eval/capstone_eval.csv'\n",
    "\n",
    "with open(csv_file_path, mode='r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        data_samples['question'].append(row['question'])\n",
    "        data_samples['ground_truth'].append(row['ground_truths'])\n",
    "        data_samples['answer'].append(row['answer'])\n",
    "        \n",
    "        # Assuming 'contexts' and 'ground_truths' are stored as strings that look like lists\n",
    "        # Convert them from string representation to actual Python lists\n",
    "        # If this assumption is incorrect, you'll need to adjust the parsing\n",
    "        try:\n",
    "            contexts = ast.literal_eval(row['contexts'])\n",
    "            #ground_truths = ast.literal_eval(row['ground_truths'])\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Fallback in case of parsing error, adjust as necessary\n",
    "            contexts = row['contexts'].split(';')  # Example fallback, adjust based on actual format\n",
    "            #ground_truths = row['ground_truths'].split(';')  # Example fallback, adjust based on actual format\n",
    "        \n",
    "        data_samples['contexts'].append(contexts)\n",
    "        #data_samples['ground_truth'].append(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "\n",
    "eval_dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "import nest_asyncio  # CHECK NOTES\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    answer_correctness,\n",
    "    context_relevancy\n",
    ")\n",
    "\n",
    "# NOTES: Only used when running on a jupyter notebook, otherwise comment or remove this function.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "result = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "        answer_similarity,\n",
    "        #answer_correctness,\n",
    "        context_relevancy\n",
    "    ],\n",
    "    llm=eval_llm,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"eval/ragas_results_capstone.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Congrats you have successfully completed the capstone project !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "You have reached the end of this workshop. Following cell will delete all created resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoss_client.delete_collection(id=collection['createCollectionDetail']['id'])\n",
    "aoss_client.delete_access_policy(name=access_policy_name, type='data')\n",
    "aoss_client.delete_security_policy(name=encryption_policy_name, type='encryption')\n",
    "aoss_client.delete_security_policy(name=network_policy_name, type='network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this moduel on retrieval augmented generation! This is an important technique that combines the power of large language models with the precision of retrieval methods. By augmenting generation with relevant retrieved examples, the responses we recieved become more coherent, consistent and grounded. You should feel proud of learning this innovative approach. I'm sure the knowledge you've gained will be very useful for building creative and engaging language generation systems. Well done!\n",
    "\n",
    "In the above implementation of RAG based Question Answering we have explored the following concepts and how to implement them using Amazon Bedrock and it's LangChain integration.\n",
    "\n",
    "- Loading documents and generating embeddings to create a vector store\n",
    "- Retrieving documents to the question\n",
    "- Preparing a prompt which goes as input to the LLM\n",
    "- Present an answer in a human friendly manner\n",
    "- keep source knowledge up to date, and improve trust in our system by providing citations with every answer.\n",
    "\n",
    "### Take-aways\n",
    "- Experiment with different Vector Stores\n",
    "- Leverage various models available under Amazon Bedrock to see alternate outputs\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Integration with enterprise data stores\n",
    "\n",
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
