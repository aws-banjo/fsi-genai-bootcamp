{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Question & Answering with Amazon Bedrock using LangChain & Amazon OpenSearch\n",
    "\n",
    "> *This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "Previously, we used the Anthropic Claude model in Amazon Bedrock to demonstrate a basic Question Answering (QA) system, and learned the value of grounding a model with additional context before generating a response. In the previous notebook, we had to manually provide the model with relevant data and context ourselves. However, this approach is not fit for enterprise-level QA systems where there could be hundreds of thousands of large documents.\n",
    "\n",
    "## Retrieval Augmented Generation (RAG)\n",
    "\n",
    "We can improve upon this process by implementing an architecture called retrieval augmented generation (RAG). RAG retrieves data from outside the LLM's training data sources and augments the prompts by adding the relevant retrieved data as context. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, without needing to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
    "\n",
    "## Solution\n",
    "\n",
    "In this notebook, we aid LLM responses to user queries by implementing RAG using context from external documents. First, we process documents and store these into a vector store. Next, we search the vector store using the user's question, and return relevant data as external context to the LLM. Finally, the LLM generates an answer to the user's question based on the new context provided.\n",
    "\n",
    "We will walk through implementing the following two patterns: Question Answering (QA) and Conversational AI with conversation memory. \n",
    "\n",
    "Let’s break down the solution a little further. \n",
    "\n",
    "### Prepare documents for search\n",
    "![Documents](./images/embeddings_lang.png)\n",
    "\n",
    "First, the documents must be processed and then indexed in a vector store.\n",
    "- Load the documents from our directory\n",
    "- Process the documents by splitting them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using an embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "\n",
    "### Respond to the user’s question\n",
    "![Question](./images/chatbot_lang.png)\n",
    "\n",
    "Once the vector store is indexed with documents and embeddings, we can search for text relevant to the question being asked. The relevant chunks are sent to the model as additional context, where the model will then generate the answer.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running the rest of this notebook, you'll need to run the cells below to ensure necessary libraries are installed and to connect to Amazon Bedrock.\n",
    "\n",
    "⚠️ For more details on how the setup works and **whether you might need to make any changes**, refer to the [Amazon Bedrock boto3 setup notebook](../../02_prompt_engineering/1_setup.ipynb) notebook.\n",
    "\n",
    "In this notebook, we'll also need some extra dependencies:\n",
    "\n",
    "- [OpenSearch Python Client](https://pypi.org/project/opensearch-py/) to store vector embeddings\n",
    "- [PyPDF](https://pypi.org/project/pypdf/) for handling PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.34.66\" \\\n",
    "    \"awscli>=1.32.66\" \\\n",
    "    \"botocore>=1.34.66\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U opensearch-py==2.3.1 langchain==0.1.12 \"pypdf>=3.8,<4\" \\\n",
    "    apache-beam \\\n",
    "    datasets \\\n",
    "    tiktoken \\\n",
    "    rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from rich import print as rprint\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "module_path = \"../../\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LangChain\n",
    "\n",
    "LangChain provides convenient integrations with Amazon Bedrock and other services like vector stores and retrievers. We begin with instantiating the large language model (LLM) and the embeddings model. We are using Anthropic Claude version 2 for text generation and Amazon Titan Embeddings G1 - Text for text embedding.\n",
    "\n",
    "Note: Amazon Bedrock offers a choice of high-performing foundation models (FMs). You can replace the value for `model_id` with one of the available [model IDs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) as follows. Some models have different requirements for inputs such as prompt format. As of this writing, all models are supported in the US West (Oregon, us-west-2) Region. If you are using another AWS Region, check the latest [model support by AWS Region](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html).\n",
    "\n",
    "```python\n",
    "llm = BedrockChat(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\", ...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain.load.dump import dumps\n",
    "\n",
    "# Instantiate the Anthropic Claude 3 Haiku model\n",
    "llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    model_kwargs={\"max_tokens\": 200}\n",
    ")\n",
    "\n",
    "# Instantiate the Amazon Titan Embeddings G1 - Text embeddings model\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\" # change this model ID to use another embeddings model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [Optional] Explore using different embeddings models\n",
    "Below are optional embeddings models from Jina AI and MPNet that are made available through Hugging Face. You can load these models into the local notebook for quick experimentation. Local embedding models require the [sentence-transformers](https://www.sbert.net/) library, which in turn requires Hugging Face's `transformers` library as well as PyTorch. If you want to experiment with these, it is recommended that you use a larger instance type with and a PyTorch kernel.\n",
    "\n",
    "Note: If you are using the Jina AI embeddings model, you may need an instance type equivalent to `ml.m5.large` or larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set to True to load local models but see the note above first\n",
    "LOAD_LOCAL_MODELS = False\n",
    "\n",
    "if LOAD_LOCAL_MODELS:\n",
    "    %pip install -U sentence-transformers\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    jina_embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-en\") # Jina AI embeddings model\n",
    "    mpnet_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") # MPNet embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We will load the documents with the help of [PyPDF in LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf). For this lab, we will be using the files in the data folder.\n",
    "\n",
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents. For this use-case, we are creating chunks of roughly 2000 characters with an overlap of 200 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter). This text splitter is recommended for generic text.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question, but small enough to fit into the model's context window, including the prompt. The embeddings model we are using also has an input tokens limit of 8k tokens, which roughly translates to ~32000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Load the documents from the data folder/directory\n",
    "loader = PyPDFDirectoryLoader(\"../data/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the documents recursively by character\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the split documents to the original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) // len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "\n",
    "print(f\"Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.\")\n",
    "print(f\"After the split we have {len(docs)} documents compared to the original {len(documents)}.\")\n",
    "print(f\"Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test out our embedding model on a single document to see what an embedding looks like below. These embeddings could be generated for the entire corpus of documents and stored in a vector store for easy retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sample_embedding = np.array(bedrock_embeddings.embed_query(docs[0].page_content))\n",
    "    modelId = bedrock_embeddings.model_id\n",
    "    print(\"Embedding model Id :\", modelId)\n",
    "    print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "    print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "        \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/setting-up.html\\\n",
    "        \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\\n",
    "        \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "              \\x1b[0m\")\n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the vector store\n",
    "In this workshop we will use the ***vector engine for Amazon OpenSearch Serverless.***\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application — without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "vector_store_name = \"bedrock-workshop-rag\"\n",
    "index_name = \"bedrock-workshop-rag-index\"\n",
    "encryption_policy_name = \"bedrock-workshop-rag-sp\"\n",
    "network_policy_name = \"bedrock-workshop-rag-np\"\n",
    "access_policy_name = \"bedrock-workshop-rag-ap\"\n",
    "user_identity = boto3.client(\"sts\").get_caller_identity()[\"Arn\"]\n",
    "user_account = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "sagemaker_notebook_role = (\n",
    "    \"arn:aws:iam::\"\n",
    "    + user_account\n",
    "    + \":role/aws-service-role/sagemaker.amazonaws.com/AWSServiceRoleForAmazonSageMakerNotebooks\"\n",
    ")\n",
    "\n",
    "region = os.environ.get(\"AWS_DEFAULT_REGION\", boto3.session.Session().region_name)\n",
    "\n",
    "aoss_client = boto3.client(\n",
    "    \"opensearchserverless\"\n",
    ")  # Create the Amazon OpenSearch Serverless client\n",
    "\n",
    "print(\"Creating a security policy for AOSS collection..\")\n",
    "security_policy = aoss_client.create_security_policy(\n",
    "    name=encryption_policy_name,\n",
    "    policy=json.dumps(\n",
    "        {\n",
    "            \"Rules\": [\n",
    "                {\n",
    "                    \"Resource\": [\"collection/\" + vector_store_name],\n",
    "                    \"ResourceType\": \"collection\",\n",
    "                }\n",
    "            ],\n",
    "            \"AWSOwnedKey\": True,\n",
    "        }\n",
    "    ),\n",
    "    type=\"encryption\",\n",
    ")\n",
    "\n",
    "print(\"Creating a network policy for AOSS collection..\")\n",
    "network_policy = aoss_client.create_security_policy(\n",
    "    name=network_policy_name,\n",
    "    policy=json.dumps(\n",
    "        [\n",
    "            {\n",
    "                \"Rules\": [\n",
    "                    {\n",
    "                        \"Resource\": [\"collection/\" + vector_store_name],\n",
    "                        \"ResourceType\": \"collection\",\n",
    "                    }\n",
    "                ],\n",
    "                \"AllowFromPublic\": True,\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    type=\"network\",\n",
    ")\n",
    "\n",
    "print(\"Creating an AOSS collection..\")\n",
    "collection = aoss_client.create_collection(name=vector_store_name, type=\"VECTORSEARCH\")\n",
    "\n",
    "print(\"Waiting for an AOSS collection to be created..\")\n",
    "while True:\n",
    "    status = aoss_client.list_collections(\n",
    "        collectionFilters={\"name\": vector_store_name}\n",
    "    )[\"collectionSummaries\"][0][\"status\"]\n",
    "    if status in (\"ACTIVE\", \"FAILED\"):\n",
    "        break\n",
    "    print(\".\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"Creating an access policy for the AOSS collection..\")\n",
    "access_policy = aoss_client.create_access_policy(\n",
    "    name=access_policy_name,\n",
    "    policy=json.dumps(\n",
    "        [\n",
    "            {\n",
    "                \"Rules\": [\n",
    "                    {\n",
    "                        \"Resource\": [\"collection/\" + vector_store_name],\n",
    "                        \"Permission\": [\n",
    "                            \"aoss:CreateCollectionItems\",\n",
    "                            \"aoss:DeleteCollectionItems\",\n",
    "                            \"aoss:UpdateCollectionItems\",\n",
    "                            \"aoss:DescribeCollectionItems\",\n",
    "                        ],\n",
    "                        \"ResourceType\": \"collection\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"Resource\": [\"index/\" + vector_store_name + \"/*\"],\n",
    "                        \"Permission\": [\n",
    "                            \"aoss:CreateIndex\",\n",
    "                            \"aoss:DeleteIndex\",\n",
    "                            \"aoss:UpdateIndex\",\n",
    "                            \"aoss:DescribeIndex\",\n",
    "                            \"aoss:ReadDocument\",\n",
    "                            \"aoss:WriteDocument\",\n",
    "                        ],\n",
    "                        \"ResourceType\": \"index\",\n",
    "                    },\n",
    "                ],\n",
    "                \"Principal\": [user_identity, sagemaker_notebook_role],\n",
    "                \"Description\": \"Easy data policy\",\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    type=\"data\",\n",
    ")\n",
    "\n",
    "host = (\n",
    "    collection[\"createCollectionDetail\"][\"id\"]\n",
    "    + \".\"\n",
    "    + region\n",
    "    + \".aoss.amazonaws.com:443\"\n",
    ")\n",
    "\n",
    "print(\"AOSS host: \" + host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to ingest the documents into the vector store. This can be done easily using the [LangChain OpenSearch integration](https://python.langchain.com/docs/integrations/vectorstores/opensearch) which takes in the embeddings model and the documents to create the entire vector store.\n",
    "\n",
    "⚠️ Note: ⚠️ If you are using the Jina AI embeddings model, be sure you are using an instance type equivalent to `ml.m5.large` or larger before running the code below. If you run into a user permission error below, wait a minute and try running the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearchpy import RequestsHttpConnection, AWSV4SignerAuth\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings, # Use 'jina_embeddings' if using Jina AI or 'mpnet_embeddings' if using MPNet embeddings\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Searching the vector store\n",
    "We can make a query to the vector store and return the relevant chunks of text.\n",
    "\n",
    "It takes a few seconds to make the documents available in the collection. If you will get an empty output in the next cell, just wait a little bit and retry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic search methods\n",
    "[Semantic search](https://opensearch.org/docs/latest/search-plugins/semantic-search/) considers the context and intent of a query. In OpenSearch, semantic search is facilitated by neural search with text embedding models. Semantic search creates a dense vector (a list of floats) and ingests data into a k-NN index.\n",
    "\n",
    "Short for k-nearest neighbors, the k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors.  To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points. We will explore some of the distance functions later in this lab.\n",
    "\n",
    "The k-NN plugin supports three different methods for obtaining the k-nearest neighbors from an index of vectors:\n",
    "- Approximate k-NN\n",
    "- Script Score k-NN\n",
    "- Painless extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate k-NN search\n",
    "Standard k-NN search methods compute similarity using a brute-force approach that measures the nearest distance between a query and a number of points, which produces exact results. This works well in many applications. However, in the case of extremely large datasets with high dimensionality, this creates a scaling problem that reduces the efficiency of the search. [Approximate k-NN search](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/) methods can overcome this by employing tools that restructure indexes more efficiently and reduce the dimensionality of searchable vectors. Using this approach requires a sacrifice in accuracy but increases search processing speeds appreciably.\n",
    "\n",
    "Of the three search methods the k-NN plugin provides, this method offers the best search scalability for large datasets. This approach is the preferred method when a dataset reaches hundreds of thousands of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query\n",
    "query = \"What are the Fidelity's digital asset offerings in 2022?\"\n",
    "\n",
    "# Search for the 3 most relevant documents\n",
    "results = docsearch.similarity_search(query, k=3)\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact k-NN with scoring script\n",
    "The k-NN plugin implements the OpenSearch [score script](https://opensearch.org/docs/latest/search-plugins/knn/knn-score-script/) plugin that you can use to find the exact k-nearest neighbors to a given query point.\n",
    "\n",
    "Because the score script approach executes a brute force search, it doesn’t scale as well as the approximate approach. This approach is preferred for searches over smaller bodies of documents or when a pre-filter is needed. Using this approach on large indexes may lead to high latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = docsearch.similarity_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    is_appx_search=False,\n",
    "    search_type=\"script_scoring\"\n",
    ")\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-NN Painless Scripting extensions (exact)\n",
    "Currently, the vector engine for Amazon OpenSearch serverless supports the approximate k-NN search and scoring script search methods. Below is an example of the [painless scripting](https://opensearch.org/docs/latest/search-plugins/knn/painless-functions/) search method on an Amazon OpenSearch service for your reference.\n",
    "\n",
    "Similar to the k-NN Script Score, you can use this method to perform a brute force, exact k-NN search across an index. This approach has slightly slower query performance compared to the k-NN scoring script. If your use case requires more customization over the final score, you should use this approach over k-NN scoring script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "results = docsearch.similarity_search(\n",
    "    query, \n",
    "    is_appx_search=False,\n",
    "    search_type=\"painless_scripting\"\n",
    ")\n",
    "\n",
    "print(dumps(results, pretty=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact k-NN search with filters\n",
    "You can apply [k-NN search with filters](https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/) with either the  scoring script or painless extension search methods. Filters can greatly reduce the number of vectors to be searched. Using the k-NN score script, you can apply a filter on an index before executing the nearest neighbor search (sometimes referred to as a pre-filter search). This is useful for dynamic search cases where the index body may vary based on other conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the Fidelity's digital asset offerings in 2022?\"\n",
    "filter = {\"bool\": {\"filter\": {\"term\": {\"text\": \"crypto\"}}}}\n",
    "\n",
    "# Pre-filter results\n",
    "results = docsearch.similarity_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    is_appx_search=False,\n",
    "    search_type=\"script_scoring\",\n",
    "    pre_filter=filter    \n",
    ")\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply metadata filters to only include results from a specific document or a specific page of a document as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the Fidelity's digital asset offerings in 2022?\"\n",
    "\n",
    "# Metadata filter to only include results from 2022-Fidelity-Annual-Report.pdf\n",
    "metadata_filter = {\"term\": {\"metadata.source.keyword\": \"../data/2022-Fidelity-Investments-Annual-Report.pdf\"}}\n",
    "\n",
    "# Metadata filter to only include results from page 6 of 2022-Fidelity-Annual-Report.pdf\n",
    "metadata_page_filter = {\"bool\": {\"filter\": [{\"term\": {\"metadata.page\": 6}}, {\"term\": {\"metadata.source.keyword\": \"../data/2022-Fidelity-Investments-Annual-Report.pdf\"}}]}}\n",
    "\n",
    "# Search query with metadata filter\n",
    "results = docsearch.similarity_search(query, k=3, boolean_filter=metadata_filter) \n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaces - similarity or distance measures\n",
    "\n",
    "Approximate Search through OpenSearch supports the following similarity or distance measures:\n",
    "\n",
    "**Euclidean distance** – The straight-line distance between points.\n",
    "\n",
    "**L1 (Manhattan) distance** – The sum of the differences of all of the vector components. L1 distance measures how many orthogonal city blocks you need to traverse from point A to point B.\n",
    "\n",
    "**L-infinity (chessboard) distance** – The number of moves a King would make on an n-dimensional chessboard. It’s different than Euclidean distance on the diagonals—a diagonal step on a 2-dimensional chessboard is 1.41 Euclidean units away, but 2 L-infinity units away.\n",
    "\n",
    "**Inner product** – The product of the magnitudes of two vectors and the cosine of the angle between them. Usually used for natural language processing (NLP) vector similarity.\n",
    "\n",
    "**Cosine similarity** – The cosine of the angle between two vectors in a vector space.\n",
    "\n",
    "We can specify the distance measure in the `space_type` parameter when we load our documents as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"l2\", # Options are: “l2”, “l1”, “cosinesimil”, “linf”, “innerproduct”; default: “l2”\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Engines and algorithms\n",
    "The Approximate k-NN search methods leveraged by OpenSearch use approximate nearest neighbor (ANN) algorithms from the [NMSLIB](https://github.com/nmslib/nmslib), [FAISS](https://github.com/facebookresearch/faiss), and [Lucene](https://lucene.apache.org/) libraries to power k-NN search.\n",
    "\n",
    "The engine details are as follows:\n",
    "\n",
    "- Non-Metric Space Library (NMSLIB) – NMSLIB implements the HNSW ANN algorithm\n",
    "- Facebook AI Similarity Search (FAISS) – FAISS implements both HNSW and IVF ANN algorithms\n",
    "- Lucene – Lucene implements the HNSW algorithm\n",
    "\n",
    "Each of the three engines used for approximate k-NN search has its own attributes that make one more sensible to use than the others in a given situation. In general, NMSLIB and FAISS should be selected for large-scale use cases. Lucene is a good option for smaller deployments, but offers benefits like smart filtering where the optimal filtering strategy—pre-filtering, post-filtering, or exact k-NN—is automatically applied depending on the situation.\n",
    "\n",
    "We can specify the engine as shown below.\n",
    "\n",
    "Note: As of this writing, Amazon OpenSearch Serverless vector search collections don't support the Apache Lucene ANN engine. Vector search collections only support the HNSW algorithm with FAISS and do not support IVF and IVFQ. Please check the updated [limitations](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vector-search.html#serverless-vector-limitations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\", # Options are: “nmslib”, “faiss”, “lucene”; default: “nmslib”\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For HNSW, we can tune the m, ef_construction, and ef_search parameters to achieve our desired trade-off:**\n",
    "\n",
    "**m** – Controls the maximum number of edges a node in a graph can have. Because each node has to store all of its edges, increasing this value will increase the memory footprint, but also increase the connectivity of the graph, which will improve recall.\n",
    "\n",
    "**ef_construction** – Controls the size of the candidate queue for edges when adding a node to the graph. Increasing this value will increase the number of candidates to consider, which will increase the index latency. However, because more candidates will be considered, the quality of the graph will be better, leading to better recall during search.\n",
    "\n",
    "**ef_search** – Similar to ef_construction, it controls the size of the candidate queue for graph traversal during search. Increasing this value will increase the search latency, but will also improve the recall.\n",
    "\n",
    "In general, we chose configurations that gradually increase the parameters, as detailed in the following table.\n",
    "\n",
    "![](./images/hnsw_parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example using the **FAISS** engine and providing a parameter configuration that provides a balance between latency, memory, and recall (see table above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    bedrock_embeddings,\n",
    "    opensearch_url=host,\n",
    "    http_auth=auth,\n",
    "    timeout = 100,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=index_name,\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"l2\",\n",
    "    m=16, # maximum number of edges\n",
    "    ef_construction=128, # size of the candidate queue for edges\n",
    "    ef_search=128 # size of the candidate queue for graph traversal\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum marginal relevance search (MMR)\n",
    "If you’d like to look up for some similar documents, but you’d also like to receive diverse results, MMR is a method you should consider. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = docsearch.max_marginal_relevance_search(\n",
    "    query, \n",
    "    k=2,\n",
    "    fetch_k=10,\n",
    "    lambda_param=0.5\n",
    ")\n",
    "\n",
    "rprint(dumps(results, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrating RAG using LangChain\n",
    "Now that we can query our vector database for documents, we can retrieve data from outside of a large language model's training data sources and augment our prompts by adding the relevant retrieved data in context.\n",
    "\n",
    "We can use LangChain to build applications that read data from stored internal documents and summarize them into conversational responses. We can create a Retrieval Augmented Generation (RAG) workflow that introduces new information to the language model during prompting. Implementing context-aware workflows like RAG reduces model hallucination and improves response accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Question Answering\n",
    "Generative Question Answering (GQA) is a paradigm shift from traditional models that use exact keyword matches. GQA uses large language models (LLMs) to generate human-like, novel responses to user queries. \n",
    "\n",
    "We instruct the model to base the answer to our question on the information returned from our knowledge base - Amazon OpenSearch serverless. We can do this easily using the [RetrievalQA](https://api.python.langchain.com/en/stable/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html#langchain.chains.retrieval_qa.base.RetrievalQA) chain in LangChain. This chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, # this is our foundation model\n",
    "    chain_type=\"stuff\", # uses all of the text from the documents in the prompt\n",
    "    retriever=docsearch.as_retriever() # this is our vector store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try this with our earlier query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are Fidelity's digital asset offerings in 2022?\"\n",
    "\n",
    "response = qa.invoke(query)\n",
    "rprint(response['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re still not entirely protected from convincing yet false information, called hallucinations, by the model. They can happen, and it’s unlikely that we can eliminate the problem completely. However, we can do more to improve our trust in the answers provided.\n",
    "\n",
    "An effective way of doing this is by adding citations to the response, allowing a user to see where the information is coming from. We can do this using a slightly different version of the RetrievalQA chain called [RetrievalQAWithSourcesChain](https://api.python.langchain.com/en/stable/chains/langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain.html#langchain.chains.qa_with_sources.retrieval.RetrievalQAWithSourcesChain). RetrievalQAWithSourcesChain does question answering over retrieved documents as before, but it also cites the sources in the text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "response = qa_with_sources.invoke(query)\n",
    "rprint(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM provided the relevant text directly from the documents. We can see the full result below, and you will notice that you can match the cited text from the result to the text in the source documents provided as context to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(dumps(response, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing the GQA prompt\n",
    "We’ve learned how to ground Large Language Models with source knowledge by using a vector database as our knowledge base. Using this, we can encourage accuracy in our LLM’s responses, keep source knowledge up to date, and improve trust in our system by providing citations with every answer.\n",
    "\n",
    "In the above scenario you explored the quick and easy way to get a context-aware answer to your question. Now let's have a look at a more customizable option with the help of RetrievalQA where you can customize how the documents fetched should be added to prompt using the `chain_type` parameter. \n",
    "\n",
    "If you want to control how many relevant documents should be retrieved then change the `k` parameter in the cell below to see different outputs. In many scenarios you might want to know which were the source documents that the LLM used to generate the answer, you can get those documents in the output using the `return_source_documents` parameter, which returns the documents that are added to the context of the LLM prompt. \n",
    "\n",
    "RetrievalQA also allows you to provide a custom [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/) which can be specific to the model.\n",
    "\n",
    "Note: In this example we are using Anthropic Claude as the LLM under Amazon Bedrock, this particular model performs best if the inputs are provided under `Human:` and the model is requested to generate an output after `Assistant:`. In the cell below you see an example of how to control the prompt such that the LLM stays grounded and doesn't answer outside the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Human: Use the following pieces of context to provide a concise answer in Italian to the question at the end. \\ \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_prompt = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(\n",
    "        search_kwargs={'k': 3}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our query with the new prompt from our prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are Fidelity's best achievements in 2022?\"\n",
    "\n",
    "response = qa_prompt.invoke({\"query\": query})\n",
    "rprint(response[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the full result to see the original query, result or response from the LLM, and the source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(dumps(response, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational AI with retrieval\n",
    "\n",
    "Conversational AI is a type of artificial intelligence (AI) that can simulate human conversation. Using our knowledge base as before, we can add in conversation history to have a conversation based on retrieved documents.\n",
    "\n",
    "We can add conversation history along with our query and context with the assistance of the [ConversationalRetrievalChain](https://api.python.langchain.com/en/stable/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain) LangChain chain. This chain can be used to have conversations with a document. It takes in a question and (optional) previous conversation history.\n",
    "\n",
    "The algorithm for this chain consists of three parts:\n",
    "1. Use the chat history and the user's question to create a “standalone question”. This is done so that this question can be passed into the retrieval step to fetch relevant documents. If only the new question was passed in, then relevant context may be lacking. If the whole conversation was passed into retrieval, there may be unnecessary information there that would distract from retrieval.\n",
    "2. This new rephrased question is passed to the retriever and the relevant documents are returned.\n",
    "3. The retrieved documents are passed to an LLM along with the new question and relevant documents to generate a final response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up our prompt templates\n",
    "First, we will create a prompt template which will reformat the user input to be more compatible for searching the vector database. The prompt provides instructions for the LLM to rephrase the user input and chat history into a new standalone question that only includes the relevant information from the conversation.\n",
    "\n",
    "Note that this time our prompt template includes a `{chat_history}` variable where our chat history will be included in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-message>\n",
    "{question}\n",
    "<follow-up-message>\n",
    "\n",
    "Human: Given the conversation above (between Human and Assistant) and the follow up message from Human, \\\n",
    "rewrite the follow up message to be a standalone question that captures all relevant context \\\n",
    "from the conversation. Answer only with the new question and nothing else.\n",
    "\n",
    "Assistant: Standalone Question:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next prompt will be used to pass the rephrased standalone question and the relevant documents returned from the retriever as context for the LLM to respond to with an answer. In this case, we also provide specific instructions about how to answer the question.\n",
    "\n",
    "Note that this time our prompt template includes a `{context}` variable where context retrieved from the vector database will be included in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respond_prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Human: Given the context above, answer the question inside the <q></q> XML tags.\n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "If the answer is not in the context say \"Sorry, I don't know as the answer was not found in the context\". Do not use any XML tags in the answer.\n",
    "\n",
    "Assistant:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up conversation memory\n",
    "\n",
    "Now that we have our prompts ready, we need to facilitate conversation memory. We will use LangChain's [ConversationBufferMemory](https://api.python.langchain.com/en/stable/memory/langchain.memory.buffer.ConversationBufferMemory.html#langchain.memory.buffer.ConversationBufferMemory) class which provides an easy way to capture conversational memory for LLM chat applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory_chain = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    human_prefix=\"Human\",\n",
    "    ai_prefix=\"Assistant\"\n",
    ")\n",
    "\n",
    "memory_chain.chat_memory.add_user_message(\n",
    "    'Hello, what are you able to do?'\n",
    ")\n",
    "\n",
    "memory_chain.chat_memory.add_ai_message(\n",
    "    \"Hi! I am a helpful chat assistant which can answer questions about annual financial reports for 2022 and 2023.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain for Conversational Retrieval\n",
    "Let's check out an example of Anthropic Claude being able to retrieve context through conversational memory below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "conversational_qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, # this is our foundation model\n",
    "    retriever=docsearch.as_retriever(), # this is our Amazon OpenSearch vector database\n",
    "    memory=memory_chain, # this is the conversational memory storage class\n",
    "    condense_question_prompt=condense_prompt, # this is the prompt for condensing user inputs\n",
    "    verbose=False, # change this to True in order to see the logs working in the background\n",
    ")\n",
    "\n",
    "conversational_qa.combine_docs_chain.llm_chain.prompt = respond_prompt # this is the prompt in order to respond to condensed questions\n",
    "\n",
    "response = conversational_qa.invoke({'question': \"What were Fidelity's digital asset offerings in 2022?\"})\n",
    "rprint(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look to see the full response which contains the question asked, chat history, and the answer from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rprint(dumps(response, pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask follow up questions and the model will answer based on the context from the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_qa.invoke({'question': 'Why did they expand these offerings?'})\n",
    "rprint(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may continue the conversation by asking follow up questions.\n",
    "\n",
    "```python\n",
    "response = conversational_qa.invoke({'question': 'Add additional question here'})\n",
    "rprint(response['answer'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversational Retrieval with source documents\n",
    "\n",
    "To return the source documents, we need to specify the `input_key` and `output_key` parameters in our memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_chain_with_sources = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    input_key=\"question\", # specify the input_key to return source documents\n",
    "    output_key=\"answer\", # specify the output_key to return source documents\n",
    "    human_prefix=\"Human\",\n",
    "    ai_prefix=\"Assistant\"\n",
    ")\n",
    "\n",
    "memory_chain_with_sources.chat_memory.add_user_message(\n",
    "    'Hello, what are you able to do?'\n",
    ")\n",
    "\n",
    "memory_chain_with_sources.chat_memory.add_ai_message(\n",
    "    \"Hi! I am a helpful chat assistant which can answer questions about annual financial reports for 2022 and 2023.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse our previous `condense_prompt` and `respond_prompt` templates, and add the `return_source_documents` parameter to our ConversationalRetrievalChain chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_with_sources = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    memory=memory_chain_with_sources,\n",
    "    condense_question_prompt=condense_prompt,\n",
    "    return_source_documents=True, # this is to return the source documents\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "conversational_qa_with_sources.combine_docs_chain.llm_chain.prompt = respond_prompt # this is the prompt in order to respond to condensed questions\n",
    "\n",
    "result = conversational_qa_with_sources.invoke({'question': \"What were Fidelity's digital asset offerings in 2022?\"})\n",
    "rprint(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the source documents below. You can cite the sources in your application to provide the users confidence in the responses during the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(dumps(result['source_documents'], pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "You have reached the end of this workshop. Following cell will delete all created resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoss_client.delete_collection(id=collection['createCollectionDetail']['id'])\n",
    "aoss_client.delete_access_policy(name=access_policy_name, type='data')\n",
    "aoss_client.delete_security_policy(name=encryption_policy_name, type='encryption')\n",
    "aoss_client.delete_security_policy(name=network_policy_name, type='network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In the above implementation of RAG based Question Answering and Conversational AI, we have explored the following concepts and how to implement them using the LangChain integrations for Amazon Bedrock and Amazon OpenSearch Serverless:\n",
    "\n",
    "- Loading documents and processing them into smaller chunks\n",
    "- Creating a vector store using vector engine for Amazon OpenSearch Serverless\n",
    "- Generating embeddings with an embeddings model\n",
    "- Searching the vector store to retrieve context relevant to the question\n",
    "- Performing Generative Question Answering using foundation models\n",
    "- Improving trust in our system by providing citations with every answer\n",
    "- Preparing prompt templates to use as input to the LLM\n",
    "- Storing conversation memory and providing the history as context to the LLM\n",
    "\n",
    "### Next steps\n",
    "- Experiment with different vector stores\n",
    "- Leverage various text and embedding models available through Amazon Bedrock to see alternate outputs\n",
    "- Explore options such as persistent storage of embeddings and document chunks\n",
    "- Use Amazon Bedrock Knowledge Bases, a fully managed RAG capability with built-in session context management\n",
    "\n",
    "# Thank You"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
